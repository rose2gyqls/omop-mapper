#!/usr/bin/env python3
"""
3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

1ë‹¨ê³„: Elasticsearchì—ì„œ concept_name ì¼ì¹˜ë„ë¡œ 10ê°œ í›„ë³´êµ° ì„ ë³„
2ë‹¨ê³„: 10ê°œ í›„ë³´êµ°ì„ ì˜ë¯¸ì  ìœ ì‚¬ë„ + Jaccard ìœ ì‚¬ë„ë¡œ 5ê°œë¡œ ì¶•ì†Œ + Non-std to Std ë³€í™˜
3ë‹¨ê³„: ëª¨ë“  Standard í›„ë³´êµ°ì— ëŒ€í•´ ì˜ë¯¸ì  ìœ ì‚¬ë„ + Jaccard ìœ ì‚¬ë„ë¡œ ìµœì¢… ë¦¬ë­í‚¹

í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: entity_sample.xlsxì˜ 10, 11, 12 ì‹œíŠ¸
"""

import sys
import os
import json
import logging
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'indexing'))

from omop_mapper.elasticsearch_client import ElasticsearchClient
from omop_mapper.entity_mapping_api import EntityMappingAPI
from sapbert_embedder import SapBERTEmbedder

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NumpyEncoder(json.JSONEncoder):
    """NumPy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


class ThreeStageMapper:
    """3ë‹¨ê³„ ë§¤í•‘ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, index_name: str = "concept", use_small_index: bool = False):
        """
        ì´ˆê¸°í™”
        
        Args:
            index_name: ì‚¬ìš©í•  ì¸ë±ìŠ¤ ì´ë¦„ (concept ë˜ëŠ” concept-small)
            use_small_index: concept-small ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€ (ì†Œë¬¸ì ë³€í™˜)
        """
        self.index_name = index_name
        self.use_small_index = use_small_index
        
        # Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.es_client = ElasticsearchClient()
        
        # EntityMappingAPI ì´ˆê¸°í™” (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        self.mapping_api = EntityMappingAPI()
        
        # SapBERT ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™” (ì„ë² ë”© í™œì„±í™”)
        self.embedder = SapBERTEmbedder(enabled=True, batch_size=32)
        
        logger.info(f"3ë‹¨ê³„ ë§¤í•‘ í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - ì¸ë±ìŠ¤: {index_name}, ì†Œë¬¸ì ë³€í™˜: {use_small_index}")
    
    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """Jaccard ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (SapBERT ì„ë² ë”© ê¸°ë°˜)"""
        try:
            if not self.embedder.enabled:
                return 0.0
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.embedder.encode_texts([text1, text2], show_progress=False)
            
            if len(embeddings) != 2:
                return 0.0
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity(
                embeddings[0:1], 
                embeddings[1:2]
            )[0][0]
            
            # -1~1 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
            return (similarity + 1.0) / 2.0
            
        except Exception as e:
            logger.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def calculate_hybrid_score(self, text1: str, text2: str, 
                             jaccard_weight: float = 0.4, 
                             semantic_weight: float = 0.6) -> Dict[str, float]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (Jaccard + ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
        jaccard_sim = self.calculate_jaccard_similarity(text1, text2)
        semantic_sim = self.calculate_semantic_similarity(text1, text2)
        
        hybrid_score = jaccard_sim * jaccard_weight + semantic_sim * semantic_weight
        
        return {
            'hybrid_score': float(hybrid_score),
            'jaccard_similarity': float(jaccard_sim),
            'semantic_similarity': float(semantic_sim)
        }
    
    def stage1_elasticsearch_search(self, entity_name: str, domain_id: str = "Condition") -> List[Dict[str, Any]]:
        """1ë‹¨ê³„: Elasticsearchì—ì„œ 10ê°œ í›„ë³´êµ° ê²€ìƒ‰"""
        try:
            # concept-small ì¸ë±ìŠ¤ ì‚¬ìš© ì‹œ ì†Œë¬¸ì ë³€í™˜
            search_query = entity_name.lower() if self.use_small_index else entity_name
            
            # Elasticsearch ê²€ìƒ‰ ì‹¤í–‰
            results = self.es_client.search_concepts(
                query=search_query,
                domain_ids=[domain_id] if domain_id else None,
                standard_concept_only=False,  # Standard/Non-standard ëª¨ë‘ í¬í•¨
                limit=10
            )
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            candidates = []
            for result in results:
                candidates.append({
                    '_source': {
                        'concept_id': result.concept_id,
                        'concept_name': result.concept_name,
                        'domain_id': result.domain_id,
                        'vocabulary_id': result.vocabulary_id,
                        'concept_class_id': result.concept_class_id,
                        'standard_concept': result.standard_concept,
                        'concept_code': result.concept_code
                    },
                    '_score': result.score
                })
            
            logger.info(f"1ë‹¨ê³„ ì™„ë£Œ: {len(candidates)}ê°œ í›„ë³´êµ° ê²€ìƒ‰ ('{entity_name}' -> '{search_query}')")
            return candidates
            
        except Exception as e:
            logger.error(f"1ë‹¨ê³„ Elasticsearch ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def stage2_hybrid_filtering_and_nonstd_to_std(self, entity_name: str, candidates: List[Dict[str, Any]], 
                                                 domain_id: str = "Condition") -> List[Dict[str, Any]]:
        """2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ 5ê°œ ì„ ë³„ + Non-std to Std ë³€í™˜"""
        if not candidates:
            return []
        
        # ê° í›„ë³´ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        scored_candidates = []
        for candidate in candidates:
            source = candidate['_source']
            concept_name = source.get('concept_name', '')
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            scores = self.calculate_hybrid_score(entity_name, concept_name)
            
            scored_candidates.append({
                'candidate': candidate,
                'hybrid_score': float(scores['hybrid_score']),
                'jaccard_similarity': float(scores['jaccard_similarity']),
                'semantic_similarity': float(scores['semantic_similarity']),
                'elasticsearch_score': float(candidate['_score'])
            })
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì„ íƒ
        scored_candidates.sort(key=lambda x: x['hybrid_score'], reverse=True)
        top5_candidates = scored_candidates[:5]
        
        # Standard/Non-standard ë¶„ë¥˜ ë° Non-std to Std ë³€í™˜
        all_standard_candidates = []
        
        for scored_candidate in top5_candidates:
            candidate = scored_candidate['candidate']
            source = candidate['_source']
            
            if source.get('standard_concept') == 'S':
                # Standard ê°œë…: ì§ì ‘ ì¶”ê°€
                all_standard_candidates.append({
                    'concept': source,
                    'is_original_standard': True,
                    'original_candidate': candidate,
                    'stage2_hybrid_score': float(scored_candidate['hybrid_score']),
                    'stage2_jaccard_similarity': float(scored_candidate['jaccard_similarity']),
                    'stage2_semantic_similarity': float(scored_candidate['semantic_similarity']),
                    'elasticsearch_score': float(scored_candidate['elasticsearch_score'])
                })
            else:
                # Non-standard ê°œë…: Standard í›„ë³´ë“¤ ì¡°íšŒ
                concept_id = str(source.get('concept_id', ''))
                try:
                    standard_candidates = self.mapping_api._get_standard_candidates(concept_id, domain_id)
                    
                    for std_candidate in standard_candidates:
                        all_standard_candidates.append({
                            'concept': std_candidate,
                            'is_original_standard': False,
                            'original_non_standard': source,
                            'original_candidate': candidate,
                            'stage2_hybrid_score': float(scored_candidate['hybrid_score']),
                            'stage2_jaccard_similarity': float(scored_candidate['jaccard_similarity']),
                            'stage2_semantic_similarity': float(scored_candidate['semantic_similarity']),
                            'elasticsearch_score': 0.0  # Non-std -> StdëŠ” ES ì ìˆ˜ ì—†ìŒ
                        })
                        
                except Exception as e:
                    logger.warning(f"Non-std to Std ë³€í™˜ ì‹¤íŒ¨ (concept_id: {concept_id}): {e}")
        
        logger.info(f"2ë‹¨ê³„ ì™„ë£Œ: {len(top5_candidates)}ê°œ ì„ ë³„ -> {len(all_standard_candidates)}ê°œ Standard í›„ë³´êµ°")
        return all_standard_candidates
    
    def stage3_final_reranking(self, entity_name: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """3ë‹¨ê³„: ëª¨ë“  Standard í›„ë³´êµ°ì— ëŒ€í•´ ìµœì¢… ë¦¬ë­í‚¹"""
        if not candidates:
            return []
        
        # ê° Standard í›„ë³´ì— ëŒ€í•´ ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        final_scored_candidates = []
        
        for candidate in candidates:
            concept = candidate['concept']
            concept_name = concept.get('concept_name', '')
            
            # ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            scores = self.calculate_hybrid_score(entity_name, concept_name)
            
            final_scored_candidates.append({
                **candidate,  # ê¸°ì¡´ ì •ë³´ ìœ ì§€
                'stage3_hybrid_score': float(scores['hybrid_score']),
                'stage3_jaccard_similarity': float(scores['jaccard_similarity']),
                'stage3_semantic_similarity': float(scores['semantic_similarity']),
                'final_score': float(scores['hybrid_score'])  # ìµœì¢… ì ìˆ˜
            })
        
        # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        final_scored_candidates.sort(key=lambda x: x['final_score'], reverse=True)
        
        logger.info(f"3ë‹¨ê³„ ì™„ë£Œ: {len(final_scored_candidates)}ê°œ í›„ë³´êµ° ìµœì¢… ë¦¬ë­í‚¹")
        return final_scored_candidates
    
    def map_entity_3stage(self, entity_name: str, domain_id: str = "Condition") -> Dict[str, Any]:
        """3ë‹¨ê³„ ë§¤í•‘ ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
        result = {
            'entity_name': entity_name,
            'domain_id': domain_id,
            'stage1_candidates': [],
            'stage2_candidates': [],
            'stage3_candidates': [],
            'final_mapping': None
        }
        
        try:
            # 1ë‹¨ê³„: Elasticsearch ê²€ìƒ‰
            stage1_candidates = self.stage1_elasticsearch_search(entity_name, domain_id)
            result['stage1_candidates'] = [
                {
                    'concept_id': c['_source'].get('concept_id'),
                    'concept_name': c['_source'].get('concept_name'),
                    'standard_concept': c['_source'].get('standard_concept'),
                    'elasticsearch_score': float(c['_score'])
                }
                for c in stage1_candidates
            ]
            
            # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ + Non-std to Std
            stage2_candidates = self.stage2_hybrid_filtering_and_nonstd_to_std(
                entity_name, stage1_candidates, domain_id
            )
            result['stage2_candidates'] = [
                {
                    'concept_id': c['concept'].get('concept_id'),
                    'concept_name': c['concept'].get('concept_name'),
                    'is_original_standard': c['is_original_standard'],
                    'stage2_hybrid_score': float(c['stage2_hybrid_score']),
                    'stage2_jaccard_similarity': float(c['stage2_jaccard_similarity']),
                    'stage2_semantic_similarity': float(c['stage2_semantic_similarity']),
                    'elasticsearch_score': float(c['elasticsearch_score'])
                }
                for c in stage2_candidates
            ]
            
            # 3ë‹¨ê³„: ìµœì¢… ë¦¬ë­í‚¹
            stage3_candidates = self.stage3_final_reranking(entity_name, stage2_candidates)
            result['stage3_candidates'] = [
                {
                    'concept_id': c['concept'].get('concept_id'),
                    'concept_name': c['concept'].get('concept_name'),
                    'is_original_standard': c['is_original_standard'],
                    'stage3_hybrid_score': float(c['stage3_hybrid_score']),
                    'stage3_jaccard_similarity': float(c['stage3_jaccard_similarity']),
                    'stage3_semantic_similarity': float(c['stage3_semantic_similarity']),
                    'final_score': float(c['final_score'])
                }
                for c in stage3_candidates
            ]
            
            # ìµœì¢… ë§¤í•‘ ê²°ê³¼
            if stage3_candidates:
                best_candidate = stage3_candidates[0]
                result['final_mapping'] = {
                    'concept_id': best_candidate['concept'].get('concept_id'),
                    'concept_name': best_candidate['concept'].get('concept_name'),
                    'domain_id': best_candidate['concept'].get('domain_id'),
                    'vocabulary_id': best_candidate['concept'].get('vocabulary_id'),
                    'final_score': float(best_candidate['final_score']),
                    'mapping_method': 'direct_standard' if best_candidate['is_original_standard'] else 'non_standard_to_standard'
                }
            
        except Exception as e:
            logger.error(f"3ë‹¨ê³„ ë§¤í•‘ ì‹¤íŒ¨ ({entity_name}): {e}")
            result['error'] = str(e)
        
        return result


def load_test_entities(excel_path: str, sheets: List[str]) -> List[Dict[str, Any]]:
    """ì—‘ì…€ íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì—”í‹°í‹° ë¡œë“œ"""
    entities = []
    
    for sheet_name in sheets:
        try:
            df = pd.read_excel(excel_path, sheet_name=sheet_name)
            
            # NaN ê°’ ì œê±°
            valid_entities = df[df['entity_plain_name'].notna()]['entity_plain_name'].unique()
            
            for entity_name in valid_entities:
                entities.append({
                    'page': sheet_name,
                    'entity_plain_name': str(entity_name).strip()
                })
                
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ '{sheet_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    logger.info(f"í…ŒìŠ¤íŠ¸ ì—”í‹°í‹° ë¡œë“œ ì™„ë£Œ: {len(entities)}ê°œ")
    return entities


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--index', choices=['concept', 'concept-small'], 
                       default='concept', help='ì‚¬ìš©í•  ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: concept)')
    parser.add_argument('--output', default='3stage_mapping_results.json', 
                       help='ê²°ê³¼ ì €ì¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: 3stage_mapping_results.json)')
    parser.add_argument('--limit', type=int, default=None, 
                       help='í…ŒìŠ¤íŠ¸í•  ì—”í‹°í‹° ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: ì „ì²´)')
    
    args = parser.parse_args()
    
    # ì¸ë±ìŠ¤ ì„¤ì •
    use_small_index = (args.index == 'concept-small')
    
    print("=" * 80)
    print("ğŸ”¬ 3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print(f"ğŸ“Š ì¸ë±ìŠ¤: {args.index}")
    print(f"ğŸ“„ ì¶œë ¥ íŒŒì¼: {args.output}")
    print(f"ğŸ”¢ ì œí•œ: {args.limit if args.limit else 'ì—†ìŒ'}")
    print("=" * 80)
    
    # ë§¤í•‘ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    mapper = ThreeStageMapper(index_name=args.index, use_small_index=use_small_index)
    
    # í…ŒìŠ¤íŠ¸ ì—”í‹°í‹° ë¡œë“œ
    excel_path = "/home/work/skku/hyo/omop-mapper/data/entity_sample.xlsx"
    test_sheets = ['10', '11', '12']
    
    entities = load_test_entities(excel_path, test_sheets)
    
    if args.limit:
        entities = entities[:args.limit]
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {len(entities)}ê°œ ì—”í‹°í‹°")
    print()
    
    # ë§¤í•‘ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = []
    
    for i, entity_info in enumerate(entities, 1):
        entity_name = entity_info['entity_plain_name']
        page = entity_info['page']
        
        print(f"[{i}/{len(entities)}] í…ŒìŠ¤íŠ¸ ì¤‘: '{entity_name}' (í˜ì´ì§€: {page})")
        
        # 3ë‹¨ê³„ ë§¤í•‘ ì‹¤í–‰
        mapping_result = mapper.map_entity_3stage(entity_name)
        mapping_result['page'] = page
        
        results.append(mapping_result)
        
        # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
        if mapping_result.get('final_mapping'):
            final = mapping_result['final_mapping']
            print(f"  âœ… ë§¤í•‘ ì„±ê³µ: {final['concept_name']} (ì ìˆ˜: {final['final_score']:.3f})")
        else:
            print(f"  âŒ ë§¤í•‘ ì‹¤íŒ¨")
        print()
    
    # ê²°ê³¼ ì €ì¥ (NumPy íƒ€ì… ë³€í™˜ í¬í•¨)
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
    
    print("=" * 80)
    print("ğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 80)
    print(f"ğŸ“„ ê²°ê³¼ ì €ì¥ë¨: {args.output}")
    print(f"ğŸ“Š ì´ {len(results)}ê°œ ì—”í‹°í‹° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ì„±ê³µë¥  ê³„ì‚°
    success_count = sum(1 for r in results if r.get('final_mapping'))
    success_rate = success_count / len(results) * 100 if results else 0
    print(f"âœ… ë§¤í•‘ ì„±ê³µë¥ : {success_count}/{len(results)} ({success_rate:.1f}%)")


if __name__ == "__main__":
    main()
