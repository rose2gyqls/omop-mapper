from pickle import NONE
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import time
import logging
from enum import Enum

from .elasticsearch_client import ElasticsearchClient

try:
    import torch
    import numpy as np
    from transformers import AutoTokenizer, AutoModel
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SAPBERT = True
except ImportError:
    HAS_SAPBERT = False

logger = logging.getLogger(__name__)


class DomainID(Enum):
    """ë„ë©”ì¸ ID"""
    PROCEDURE = "procedure"
    CONDITION = "condition"
    DRUG = "drug"
    OBSERVATION = "observation"
    MEASUREMENT = "measurement"
    THRESHOLD = "threshold"
    DEMOGRAPHICS = "demographics"
    PERIOD = "period"
    PROVIDER = "provider"


@dataclass
class EntityInput:
    """ì…ë ¥ìš© ì—”í‹°í‹° ë°ì´í„°"""
    entity_name: str
    domain_id: DomainID
    vocabulary_id: Optional[str] = None


@dataclass
class MappingResult:
    """ë§¤í•‘ ê²°ê³¼ ë°ì´í„°"""
    source_entity: EntityInput
    mapped_concept_id: str
    mapped_concept_name: str
    domain_id: str
    vocabulary_id: str
    concept_class_id: str
    standard_concept: str
    concept_code: str
    concept_embedding: List[float]
    valid_start_date: Optional[str] = None
    valid_end_date: Optional[str] = None
    invalid_reason: Optional[str] = None
    mapping_score: float = 0.0
    mapping_confidence: str = "low"
    mapping_method: str = "unknown"
    alternative_concepts: List[Dict[str, Any]] = None
    

class EntityMappingAPI:
    """ì—”í‹°í‹° ë§¤í•‘ API í´ë˜ìŠ¤"""

    def __init__(
        self,
        es_client: Optional[ElasticsearchClient] = None,
        confidence_threshold: float = 0.5
    ):
        """
        ì—”í‹°í‹° ë§¤í•‘ API ì´ˆê¸°í™”
        
        Args:
            es_client: Elasticsearch í´ë¼ì´ì–¸íŠ¸
            confidence_threshold: ë§¤í•‘ ì‹ ë¢°ë„ ì„ê³„ì¹˜
        """
        self.es_client = es_client or ElasticsearchClient.create_default()
        self.confidence_threshold = confidence_threshold
    
    def map_entity(self, entity_input: EntityInput) -> Optional[MappingResult]:
        """
        ë‹¨ì¼ ì—”í‹°í‹°ë¥¼ OMOP CDMì— 3ë‹¨ê³„ ë§¤í•‘
        1ë‹¨ê³„: Elasticsearch ì¿¼ë¦¬ë¡œ top 5ê°œ í›„ë³´êµ° ì¶”ì¶œ
        2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜ ë° ëª¨ë“  Standard í›„ë³´êµ° ìˆ˜ì§‘ í›„ ì¤‘ë³µ ì œê±°
        3ë‹¨ê³„: ìˆ˜ì§‘ëœ í›„ë³´êµ°ë“¤ì— ëŒ€í•´ ëª¨ë‘ hybrid ì ìˆ˜(concept_embedding í•„ë“œ ì‚¬ìš©)ë¡œ ê³„ì‚°
        
        Args:
            entity_input: ë§¤í•‘í•  ì—”í‹°í‹° ì •ë³´
            
        Returns:
            MappingResult: ë§¤í•‘ ê²°ê³¼ ë˜ëŠ” None (ë§¤í•‘ ì‹¤íŒ¨ì‹œ)
        """
        try:
            entity_name = entity_input.entity_name
            domain_id = entity_input.domain_id
            
            logger.info(f"ğŸš€ 3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ ì‹œì‘: {entity_name} (ë„ë©”ì¸: {domain_id})")
            
            # ===== 1ë‹¨ê³„: Elasticsearch ì¿¼ë¦¬ë¡œ top 5ê°œ í›„ë³´êµ° ì¶”ì¶œ =====
            stage1_candidates = self._stage1_elasticsearch_search(entity_input)
            if not stage1_candidates:
                logger.warning(f"âš ï¸ 1ë‹¨ê³„ ì‹¤íŒ¨ - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {entity_name}")
                return None
            
            # ===== 2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜ ë° ëª¨ë“  Standard í›„ë³´êµ° ìˆ˜ì§‘ í›„ ì¤‘ë³µ ì œê±° =====
            stage2_candidates = self._stage2_collect_standard_candidates(stage1_candidates, domain_id)
            if not stage2_candidates:
                logger.warning(f"âš ï¸ 2ë‹¨ê³„ ì‹¤íŒ¨ - Standard í›„ë³´ ì—†ìŒ: {entity_name}")
                return None
            
            # ===== 3ë‹¨ê³„: ìˆ˜ì§‘ëœ í›„ë³´êµ°ë“¤ì— ëŒ€í•´ ëª¨ë‘ hybrid ì ìˆ˜ ê³„ì‚° =====
            stage3_candidates = self._stage3_calculate_hybrid_scores(entity_input, stage2_candidates)
            if not stage3_candidates:
                logger.warning(f"âš ï¸ 3ë‹¨ê³„ ì‹¤íŒ¨ - ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {entity_name}")
                return None
            
            # ìµœì¢… ë§¤í•‘ ê²°ê³¼ ìƒì„±
            mapping_result = self._create_final_mapping_result(entity_input, stage3_candidates)
            
            logger.info(f"âœ… 3ë‹¨ê³„ ë§¤í•‘ ì„±ê³µ: {entity_name} -> {mapping_result.mapped_concept_name}")
            logger.info(f"ğŸ“Š ìµœì¢… ë§¤í•‘ ì ìˆ˜: {mapping_result.mapping_score:.4f} (ì‹ ë¢°ë„: {mapping_result.mapping_confidence})")
            return mapping_result
                
        except Exception as e:
            logger.error(f"âš ï¸ ì—”í‹°í‹° ë§¤í•‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _stage1_elasticsearch_search(self, entity_input: EntityInput) -> List[Dict[str, Any]]:
        """
        1ë‹¨ê³„: Elasticsearch ì¿¼ë¦¬ë¡œ top 5ê°œ í›„ë³´êµ° ì¶”ì¶œ
        ë””ë²„ê¹…ìš©ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰, í…ìŠ¤íŠ¸ ê²€ìƒ‰, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ê°ê° ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ ë¹„êµ
        
        Args:
            entity_input: ì—”í‹°í‹° ì…ë ¥ ì •ë³´
            
        Returns:
            List[ë§¤ì¹­ëœ ì»¨ì…‰ í›„ë³´ë“¤] - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼
        """
        logger.info("=" * 60)
        logger.info("1ë‹¨ê³„: Elasticsearch ì¿¼ë¦¬ë¡œ top 5ê°œ í›„ë³´êµ° ì¶”ì¶œ (ë””ë²„ê¹… ëª¨ë“œ)")
        logger.info("=" * 60)
        
        entity_name = entity_input.entity_name
        domain_id = entity_input.domain_id
        es_index = "concept"
        top_k = 5
        
        logger.info(f"ğŸ” ì—”í‹°í‹°: {entity_name}, ë„ë©”ì¸: {domain_id}")
        
        # ì—”í‹°í‹° ì„ë² ë”© ìƒì„±
        entity_embedding = None
        if HAS_SAPBERT:
            entity_embedding = self._get_simple_embedding(entity_name)
            if entity_embedding is not None:
                logger.info("âœ… ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì„±ê³µ")
            else:
                logger.warning("âš ï¸ ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
        else:
            logger.warning("âš ï¸ SapBERT ë¯¸ì„¤ì¹˜")
        
        # 1. ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (ë””ë²„ê¹…ìš©)
        logger.info("\n" + "=" * 40)
        logger.info("ğŸ§  1-1. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼")
        logger.info("=" * 40)
        vector_results = []
        if entity_embedding is not None:
            vector_results = self._perform_vector_search_silent(entity_embedding, es_index, top_k)
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(vector_results)}ê°œ")
            for i, hit in enumerate(vector_results, 1):
                source = hit['_source']
                logger.info(f"  {i}. {source.get('concept_name', 'N/A')} "
                          f"(ID: {source.get('concept_id', 'N/A')}) "
                          f"- ë²¡í„° ì ìˆ˜: {hit['_score']:.4f}")
        else:
            logger.info("ë²¡í„° ê²€ìƒ‰ ê±´ë„ˆëœ€ (ì„ë² ë”© ì—†ìŒ)")
        
        # 2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (ë””ë²„ê¹…ìš©)
        logger.info("\n" + "=" * 40)
        logger.info("ğŸ“ 1-2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼")
        logger.info("=" * 40)
        text_results = self._perform_text_only_search_silent(entity_name, es_index, top_k)
        logger.info(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼: {len(text_results)}ê°œ")
        for i, hit in enumerate(text_results, 1):
            source = hit['_source']
            logger.info(f"  {i}. {source.get('concept_name', 'N/A')} "
                      f"(ID: {source.get('concept_id', 'N/A')}) "
                      f"- í…ìŠ¤íŠ¸ ì ìˆ˜: {hit['_score']:.4f}")
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ìµœì¢… ê²°ê³¼ìš©)
        logger.info("\n" + "=" * 40)
        logger.info("ğŸ”„ 1-3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ (ìµœì¢…)")
        logger.info("=" * 40)
        
        if entity_embedding is not None:
            # ë²¡í„°+í…ìŠ¤íŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ìˆ˜í–‰
            hybrid_results = self._perform_native_hybrid_search(entity_name, entity_embedding, es_index, top_k)
        else:
            # í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
            hybrid_results = text_results
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {len(hybrid_results)}ê°œ")
        for i, hit in enumerate(hybrid_results, 1):
            source = hit['_source']
            standard_status = "Standard" if source.get('standard_concept') in ['S', 'C'] else "Non-standard"
            concept_name = source.get('concept_name', 'N/A')
            concept_length = len(concept_name) if concept_name != 'N/A' else 0
            length_diff = abs(len(entity_name.strip()) - concept_length)
            logger.info(f"  {i}. {concept_name} "
                      f"(ID: {source.get('concept_id', 'N/A')}) "
                      f"- {standard_status}, í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hit['_score']:.4f}")
        
        logger.info(f"\nğŸ“Š 1ë‹¨ê³„ ìµœì¢… ê²°ê³¼: {len(hybrid_results)}ê°œ í›„ë³´ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)")
        
        # ë””ë²„ê¹…ìš©: stage1 í›„ë³´êµ° ì €ì¥
        self._last_stage1_candidates = [
            {
                'concept_id': str(hit['_source'].get('concept_id', '')),
                'concept_name': hit['_source'].get('concept_name', ''),
                'vocabulary_id': hit['_source'].get('vocabulary_id', ''),
                'standard_concept': hit['_source'].get('standard_concept', ''),
                'elasticsearch_score': hit['_score']
            }
            for hit in hybrid_results
        ]
        
        return hybrid_results
    
    def _stage2_collect_standard_candidates(self, stage1_candidates: List[Dict[str, Any]], domain_id: str) -> List[Dict[str, Any]]:
        """
        2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜ ë° ëª¨ë“  Standard í›„ë³´êµ° ìˆ˜ì§‘ í›„ ì¤‘ë³µ ì œê±°
        
        Args:
            stage1_candidates: 1ë‹¨ê³„ì—ì„œ ê²€ìƒ‰ëœ í›„ë³´ë“¤
            domain_id: ë„ë©”ì¸ ID
            
        Returns:
            List[ì¤‘ë³µ ì œê±°ëœ Standard í›„ë³´ë“¤]
        """
        logger.info("=" * 60)
        logger.info("2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜ ë° ëª¨ë“  Standard í›„ë³´êµ° ìˆ˜ì§‘")
        logger.info("=" * 60)
        
        all_standard_candidates = []
        standard_count = 0
        non_standard_count = 0
        
        for candidate in stage1_candidates:
            source = candidate['_source']
            
            if source.get('standard_concept') == 'S' or source.get('standard_concept') == 'C':
                # Standard ì—”í‹°í‹°: ì§ì ‘ ì¶”ê°€
                standard_count += 1
                all_standard_candidates.append({
                    'concept': source,
                    'is_original_standard': True,
                    'original_candidate': candidate,
                    'elasticsearch_score': candidate['_score']
                })
                logger.info(f"  Standard ì¶”ê°€: {source.get('concept_name', 'N/A')} (concept_id: {source.get('concept_id', 'N/A')})")
            else:
                # Non-standard ì—”í‹°í‹°: Standard í›„ë³´ë“¤ ì¡°íšŒ í›„ ì¶”ê°€
                non_standard_count += 1
                concept_id = str(source.get('concept_id', ''))
                logger.info(f"  Non-standard ì²˜ë¦¬: {source.get('concept_name', 'N/A')} (concept_id: {concept_id})")
                
                standard_candidates_from_non = self._get_standard_candidates(concept_id, domain_id)
                
                for std_candidate in standard_candidates_from_non:
                    all_standard_candidates.append({
                        'concept': std_candidate,
                        'is_original_standard': False,
                        'original_non_standard': source,
                        'original_candidate': candidate,
                        'elasticsearch_score': 0.0  # Non-standard â†’ Standardì˜ ê²½ìš° Elasticsearch ì ìˆ˜ ì—†ìŒ
                    })
                    logger.info(f"    -> Standard ë§¤í•‘: {std_candidate.get('concept_name', 'N/A')} (concept_id: {std_candidate.get('concept_id', 'N/A')})")
        
        logger.info(f"ğŸ“Š 2ë‹¨ê³„ ë¶„ë¥˜ ê²°ê³¼: Standard {standard_count}ê°œ, Non-standard {non_standard_count}ê°œ")
        logger.info(f"ğŸ“Š ìˆ˜ì§‘ëœ ì´ Standard í›„ë³´: {len(all_standard_candidates)}ê°œ")
        
        # ì¤‘ë³µ ì œê±° (ë™ì¼í•œ concept_idì™€ concept_nameì¸ ê²½ìš° ìµœê³  Elasticsearch ì ìˆ˜ë§Œ ìœ ì§€)
        unique_candidates = {}
        for candidate in all_standard_candidates:
            concept = candidate['concept']
            concept_key = (concept.get('concept_id', ''), concept.get('concept_name', ''))
            
            # ë™ì¼í•œ ì»¨ì…‰ì´ ì´ë¯¸ ìˆëŠ” ê²½ìš° ë” ë†’ì€ Elasticsearch ì ìˆ˜ë§Œ ìœ ì§€
            if concept_key not in unique_candidates or candidate['elasticsearch_score'] > unique_candidates[concept_key]['elasticsearch_score']:
                unique_candidates[concept_key] = candidate
        
        # ì¤‘ë³µ ì œê±°ëœ í›„ë³´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        deduplicated_candidates = list(unique_candidates.values())
        
        logger.info(f"ğŸ“Š ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_standard_candidates)}ê°œ â†’ {len(deduplicated_candidates)}ê°œ í›„ë³´")
        
        return deduplicated_candidates
    
    def _stage3_calculate_hybrid_scores(self, entity_input: EntityInput, stage2_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        3ë‹¨ê³„: ìˆ˜ì§‘ëœ í›„ë³´êµ°ë“¤ì— ëŒ€í•´ ëª¨ë‘ hybrid ì ìˆ˜(concept_embedding í•„ë“œ ì‚¬ìš©)ë¡œ ê³„ì‚°
        
        Args:
            entity_input: ì—”í‹°í‹° ì…ë ¥ ì •ë³´
            stage2_candidates: 2ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ Standard í›„ë³´ë“¤
            
        Returns:
            List[hybrid ì ìˆ˜ê°€ ê³„ì‚°ëœ í›„ë³´ë“¤ (ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬)]
        """
        logger.info("=" * 60)
        logger.info("3ë‹¨ê³„: ìˆ˜ì§‘ëœ í›„ë³´êµ°ë“¤ì— ëŒ€í•´ ëª¨ë‘ hybrid ì ìˆ˜ ê³„ì‚°")
        logger.info("=" * 60)
        
        final_candidates = []
        
        for i, candidate in enumerate(stage2_candidates, 1):
            concept = candidate['concept']
            elasticsearch_score = candidate['elasticsearch_score']
            
            logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} (concept_id: {concept.get('concept_id', 'N/A')})")
            logger.info(f"     Elasticsearch ì ìˆ˜: {elasticsearch_score:.4f}")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ + ì˜ë¯¸ì  ìœ ì‚¬ë„, concept_embedding í•„ë“œ ì‚¬ìš©)
            hybrid_score, text_sim, semantic_sim = self._calculate_hybrid_score(
                entity_input.entity_name, 
                concept.get('concept_name', ''),
                elasticsearch_score,
                concept
            )
            
            logger.info(f"     í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_sim:.4f}")
            logger.info(f"     ì˜ë¯¸ì  ìœ ì‚¬ë„: {semantic_sim:.4f}")
            logger.info(f"     í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.4f}")
            
            final_candidates.append({
                'concept': concept,
                'final_score': hybrid_score,
                'is_original_standard': candidate['is_original_standard'],
                'original_candidate': candidate['original_candidate'],
                'elasticsearch_score': elasticsearch_score,
                'text_similarity': text_sim,
                'semantic_similarity': semantic_sim
            })
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_candidates = sorted(final_candidates, key=lambda x: x['final_score'], reverse=True)
        
        logger.info("ğŸ“Š 3ë‹¨ê³„ ê²°ê³¼ - í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ìˆœìœ„:")
        for i, candidate in enumerate(sorted_candidates, 1):
            concept = candidate['concept']
            logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} "
                      f"(concept_id: {concept.get('concept_id', 'N/A')}) "
                      f"- ì ìˆ˜: {candidate['final_score']:.4f} "
                      f"(í…ìŠ¤íŠ¸: {candidate['text_similarity']:.4f}, "
                      f"ì˜ë¯¸ì : {candidate['semantic_similarity']:.4f})")
        
        # ë””ë²„ê¹…ìš©: ë§ˆì§€ë§‰ ë¦¬ë­í‚¹ í›„ë³´ ì €ì¥ (stage3)
        self._last_rerank_candidates = [
            {
                'concept_id': str(c['concept'].get('concept_id', '')),
                'concept_name': c['concept'].get('concept_name', ''),
                'vocabulary_id': c['concept'].get('vocabulary_id', ''),
                'standard_concept': c['concept'].get('standard_concept', ''),
                'elasticsearch_score': c.get('elasticsearch_score', 0.0),
                'text_similarity': c.get('text_similarity', 0.0),
                'semantic_similarity': c.get('semantic_similarity', 0.0),
                'final_score': c.get('final_score', 0.0)
            }
            for c in sorted_candidates
        ]
        
        return sorted_candidates
    
    def _create_final_mapping_result(self, entity_input: EntityInput, sorted_candidates: List[Dict[str, Any]]) -> MappingResult:
        """
        ìµœì¢… ë§¤í•‘ ê²°ê³¼ ìƒì„±
        
        Args:
            entity_input: ì›ë³¸ ì—”í‹°í‹° ì…ë ¥
            sorted_candidates: ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤
            
        Returns:
            MappingResult: ë§¤í•‘ ê²°ê³¼
        """
        best_candidate = sorted_candidates[0]
        alternative_candidates = sorted_candidates[1:4]  # ìƒìœ„ 3ê°œ ëŒ€ì•ˆ
        
        mapping_result = self._create_mapping_result(entity_input, best_candidate, alternative_candidates)
        
        mapping_type = "direct_standard" if best_candidate['is_original_standard'] else "non_standard_to_standard"
        logger.info(f"ğŸ“Š ë§¤í•‘ ìœ í˜•: {mapping_type}")
        
        return mapping_result
    
    
    
    def _perform_vector_search(self, entity_embedding: np.ndarray, es_index: str, top_k: int) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (knn ì¿¼ë¦¬ë§Œ ì‚¬ìš©)
        
        Args:
            entity_embedding: ì—”í‹°í‹° ì„ë² ë”© ë²¡í„°
            es_index: Elasticsearch ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë“¤]
        """
        logger.info(f"ğŸ§  ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰")
        
        # ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embedding_list = entity_embedding.tolist()
        
        # knn ì¿¼ë¦¬ë§Œ ì‚¬ìš©
        vector_query = {
            "knn": {
                "field": "concept_embedding",
                "query_vector": embedding_list,
                "k": top_k,
                "num_candidates": top_k * 3
            },
            "size": top_k,
            "_source": True
        }
        
        try:
            response = self.es_client.es_client.search(
                index=es_index,
                body=vector_query
            )
            
            hits = response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            logger.info(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê°œ ê²°ê³¼")
            
            # ëª¨ë“  ê²°ê³¼ ë¡œê¹…
            for i, hit in enumerate(hits, 1):
                source = hit['_source']
                logger.info(f"  {i}. {source.get('concept_name', 'N/A')} "
                          f"(ID: {source.get('concept_id', 'N/A')}) "
                          f"- ë²¡í„° ì ìˆ˜: {hit['_score']:.4f}")
            
            return hits
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _perform_vector_search_silent(self, entity_embedding: np.ndarray, es_index: str, top_k: int) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ë¡œê¹… ì—†ëŠ” ë²„ì „)
        
        Args:
            entity_embedding: ì—”í‹°í‹° ì„ë² ë”© ë²¡í„°
            es_index: Elasticsearch ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë“¤]
        """
        # ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embedding_list = entity_embedding.tolist()
        
        # knn ì¿¼ë¦¬ë§Œ ì‚¬ìš©
        vector_query = {
            "knn": {
                "field": "concept_embedding",
                "query_vector": embedding_list,
                "k": top_k,
                "num_candidates": top_k * 3
            },
            "size": top_k,
            "_source": True
        }
        
        try:
            response = self.es_client.es_client.search(
                index=es_index,
                body=vector_query
            )
            
            hits = response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            return hits
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    
    def _perform_text_only_search(self, entity_name: str, es_index: str, top_k: int) -> List[Dict[str, Any]]:
        """
        ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (runtime error ë°©ì§€ë¥¼ ìœ„í•´ ë‹¨ìˆœí™”)
        
        Args:
            entity_name: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            es_index: Elasticsearch ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ë“¤]
        """
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰: {entity_name}")
        
        # ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¿¼ë¦¬ (runtime error ë°©ì§€)
        body = {
            "size": top_k,
            "query": {
                "bool": {
                    "should": [
                        # 1) ì™„ì „ ì¼ì¹˜
                        {
                            "term": {
                                "concept_name.keyword": {
                                    "value": entity_name,
                                    "boost": 3.0
                                }
                            }
                        },
                        # 2) ë¶€ë¶„ ì¼ì¹˜
                        {
                            "match": {
                                "concept_name": {
                                    "query": entity_name,
                                    "boost": 2.0
                                }
                            }
                        },
                        # 3) êµ¬ë¬¸ ì¼ì¹˜
                        {
                            "match_phrase": {
                                "concept_name": {
                                    "query": entity_name,
                                    "boost": 2.5
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            }
        }
        
        try:
            response = self.es_client.es_client.search(
                index=es_index,
                body=body
            )
            
            hits = response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê°œ ê²°ê³¼")
            
            return hits
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _perform_text_only_search_silent(self, entity_name: str, es_index: str, top_k: int) -> List[Dict[str, Any]]:
        """
        ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë¡œê¹… ì—†ëŠ” ë²„ì „)
        
        Args:
            entity_name: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            es_index: Elasticsearch ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ë“¤]
        """
        # ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¿¼ë¦¬ (runtime error ë°©ì§€)
        body = {
            "size": top_k,
            "query": {
                "bool": {
                    "should": [
                        # 1) ì™„ì „ ì¼ì¹˜
                        {
                            "term": {
                                "concept_name.keyword": {
                                    "value": entity_name,
                                    "boost": 3.0
                                }
                            }
                        },
                        # 2) ë¶€ë¶„ ì¼ì¹˜
                        {
                            "match": {
                                "concept_name": {
                                    "query": entity_name,
                                    "boost": 2.0
                                }
                            }
                        },
                        # 3) êµ¬ë¬¸ ì¼ì¹˜
                        {
                            "match_phrase": {
                                "concept_name": {
                                    "query": entity_name,
                                    "boost": 2.5
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            }
        }
        
        try:
            response = self.es_client.es_client.search(
                index=es_index,
                body=body
            )
            
            hits = response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            return hits
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _perform_native_hybrid_search(self, entity_name: str, entity_embedding: np.ndarray, es_index: str, top_k: int) -> List[Dict[str, Any]]:
        """
        ë„¤ì´í‹°ë¸Œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¡œ ê²°í•©)
        
        Args:
            entity_name: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            entity_embedding: ì—”í‹°í‹° ì„ë² ë”© ë²¡í„°
            es_index: Elasticsearch ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë“¤]
        """
        logger.info(f"ğŸ”„ ë„¤ì´í‹°ë¸Œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ê¸€ììˆ˜ ìœ ì‚¬ë„ í¬í•¨): {entity_name}")
        
        # ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embedding_list = entity_embedding.tolist()
        
        # ì—”í‹°í‹° ì´ë¦„ ê¸¸ì´ ê³„ì‚°
        entity_length = len(entity_name.strip())
        scale_len = max(8.0, entity_length * 0.8)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ (knn + function_scoreë¡œ ê¸€ììˆ˜ ìœ ì‚¬ë„ ì¶”ê°€)
        body = {
            "size": top_k,
            "knn": {
                "field": "concept_embedding",
                "query_vector": embedding_list,
                "k": top_k * 2,
                "num_candidates": top_k * 5,
                "boost": 0.5  # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸€ììˆ˜ ê³ ë ¤ë¡œ ì¡°ì •)
            },
            "query": {
                "function_score": {
                    "query": {
                        "bool": {
                            "should": [
                                # ì™„ì „ ì¼ì¹˜
                                {
                                    "term": {
                                        "concept_name.keyword": {
                                            "value": entity_name,
                                            "boost": 3.0
                                        }
                                    }
                                },
                                # êµ¬ë¬¸ ì¼ì¹˜
                                {
                                    "match": {
                                        "concept_name": {
                                            "query": entity_name,
                                            "boost": 2.5
                                        }
                                    }
                                }
                            ],
                            "minimum_should_match": 1
                        }
                    },
                    # ê¸€ììˆ˜ ìœ ì‚¬ë„ í•¨ìˆ˜
                    "functions": [
                        {
                            "script_score": {
                                "script": {
                                    "params": {
                                        "origin_len": float(entity_length),
                                        "scale_len": float(scale_len)
                                    },
                                    "source": """
                                        double origin = params.origin_len;
                                        double scale = params.scale_len;
                                        double len = 0.0;
                                        
                                        if (!doc['concept_name.keyword'].isEmpty()) {
                                            len = doc['concept_name.keyword'].value.length();
                                        } else if (!doc['concept_name'].isEmpty()) {
                                            len = doc['concept_name'].value.length();
                                        }
                                        
                                        // ê°€ìš°ì‹œì•ˆ ê°ì‡ : exp(-0.5 * ((len-origin)/scale)^2)
                                        double x = (len - origin) / scale;
                                        double decay = Math.exp(-0.5 * x * x);
                                        
                                        // ê¸¸ì´ ìœ ì‚¬ë„ ë³´ë„ˆìŠ¤ (1.0 ~ 2.0)
                                        return 1.0 + decay;
                                    """
                                }
                            }
                        }
                    ],
                    "score_mode": "multiply",  # ê¸°ë³¸ ì ìˆ˜ì™€ ê¸¸ì´ ìœ ì‚¬ë„ ê³±ì…ˆ
                    "boost_mode": "multiply",
                    "boost": 0.3  # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸€ììˆ˜ ê³ ë ¤ë¡œ ì¡°ì •)
                }
            }
        }
        
        try:
            response = self.es_client.es_client.search(
                index=es_index,
                body=body
            )
            
            hits = response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            
            logger.info(f"âœ… ë„¤ì´í‹°ë¸Œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê°œ ê²°ê³¼")
            return hits
            
        except Exception as e:
            logger.error(f"ë„¤ì´í‹°ë¸Œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨ - í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
            return self._perform_text_only_search(entity_name, es_index, top_k)
    
    # def _apply_length_similarity_scoring(self, entity_name: str, hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    #     """
    #     ê¸€ì ê¸¸ì´ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ì¬ì¡°ì •
        
    #     Args:
    #         entity_name: ì›ë³¸ ì—”í‹°í‹° ì´ë¦„
    #         hits: ê²€ìƒ‰ ê²°ê³¼ë“¤
            
    #     Returns:
    #         List[ê¸¸ì´ ìœ ì‚¬ë„ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼ë“¤]
    #     """
    #     if not hits:
    #         return hits
        
    #     entity_length = len(entity_name.lower().strip())
    #     enhanced_hits = []
        
    #     for hit in hits:
    #         concept_name = hit['_source'].get('concept_name', '')
    #         concept_length = len(concept_name.lower().strip())
            
    #         # ê¸¸ì´ ì°¨ì´ ê³„ì‚°
    #         length_diff = abs(entity_length - concept_length)
    #         max_length = max(entity_length, concept_length)
            
    #         # ê¸¸ì´ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)
    #         if max_length == 0:
    #             length_similarity = 1.0
    #         else:
    #             length_similarity = 1.0 - (length_diff / max_length)
            
    #         # ê¸¸ì´ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì ìš©
    #         # ê¸¸ì´ê°€ ë¹„ìŠ·í• ìˆ˜ë¡ ë” ë†’ì€ ì ìˆ˜
    #         length_weight = 0.15  # 15% ê°€ì¤‘ì¹˜
    #         original_score = hit['_score']
            
    #         # ê¸¸ì´ ìœ ì‚¬ë„ ë³´ë„ˆìŠ¤/í˜ë„í‹° ì ìš©
    #         if length_similarity >= 0.9:  # ë§¤ìš° ìœ ì‚¬í•œ ê¸¸ì´
    #             length_bonus = 1.2
    #         elif length_similarity >= 0.8:  # ìœ ì‚¬í•œ ê¸¸ì´
    #             length_bonus = 1.1
    #         elif length_similarity >= 0.6:  # ë³´í†µ ê¸¸ì´
    #             length_bonus = 1.0
    #         elif length_similarity >= 0.4:  # ë‹¤ì†Œ ë‹¤ë¥¸ ê¸¸ì´
    #             length_bonus = 0.9
    #         else:  # ë§¤ìš° ë‹¤ë¥¸ ê¸¸ì´
    #             length_bonus = 0.8
            
    #         # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    #         adjusted_score = original_score * (1 + length_weight * (length_bonus - 1))
            
    #         # ìƒˆë¡œìš´ hit ê°ì²´ ìƒì„±
    #         enhanced_hit = hit.copy()
    #         enhanced_hit['_score'] = adjusted_score
    #         enhanced_hit['_original_score'] = original_score
    #         enhanced_hit['length_similarity'] = length_similarity
    #         enhanced_hit['length_bonus'] = length_bonus
            
    #         enhanced_hits.append(enhanced_hit)
        
    #     # ì¡°ì •ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
    #     enhanced_hits.sort(key=lambda x: x['_score'], reverse=True)
        
    #     return enhanced_hits
    
    def _get_standard_candidates(self, non_standard_concept_id: str, domain_id: str) -> List[Dict[str, Any]]:
        """
        Non-standard ì»¨ì…‰ì˜ Standard í›„ë³´ë“¤ ì¡°íšŒ
        concept_relationship ì¸ë±ìŠ¤ì—ì„œ "Maps to" ê´€ê³„ë¡œ ì—°ê²°ëœ standard ì»¨ì…‰ë“¤ì„ ì°¾ìŒ
        
        Args:
            non_standard_concept_id: Non-standard ì»¨ì…‰ ID
            domain_id: ë„ë©”ì¸ ID
            
        Returns:
            List[Standard ì»¨ì…‰ í›„ë³´ë“¤]
        """
        try:
            standard_concept_ids = self._get_maps_to_relationships(non_standard_concept_id)
            standard_candidates = self._search_concepts_in_all_indices(standard_concept_ids, domain_id)
            
            logger.info(f"Non-standard {non_standard_concept_id}ì— ëŒ€í•œ {len(standard_candidates)}ê°œ standard í›„ë³´ ì¡°íšŒ ì™„ë£Œ")
            return standard_candidates
            
        except Exception as e:
            logger.error(f"Standard í›„ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
    
    def _get_maps_to_relationships(self, concept_id_1: str) -> List[str]:
        """
        concept-relationship ì¸ë±ìŠ¤ì—ì„œ Maps to ê´€ê³„ ì¡°íšŒ
        
        Args:
            concept_id_1: ì†ŒìŠ¤ ì»¨ì…‰ ID
            
        Returns:
            List[Maps toë¡œ ì—°ê²°ëœ concept_id_2 ë¦¬ìŠ¤íŠ¸]
        """
        try:
            relationship_query = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"concept_id_1": concept_id_1}},
                            {"match": {"relationship_id": "Maps to"}}
                        ]
                    }
                },
                "size": 10
            }
            
            relationship_response = self.es_client.es_client.search(
                index="concept-relationship",
                body=relationship_query
            )
            
            standard_concept_ids = []
            for hit in relationship_response['hits']['hits']:
                concept_id_2 = hit['_source'].get('concept_id_2')
                if concept_id_2:
                    standard_concept_ids.append(str(concept_id_2))
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            logger.info(f"concept-relationship ì¸ë±ìŠ¤ì—ì„œ {concept_id_1}ì— ëŒ€í•œ {len(standard_concept_ids)}ê°œ Maps to ê´€ê³„ ë°œê²¬")
            if standard_concept_ids:
                logger.info(f"Maps to ê´€ê³„ë¡œ ì°¾ì€ concept_ids: {standard_concept_ids}")
            
            return standard_concept_ids
            
        except Exception as e:
            logger.warning(f"concept-relationship ì¸ë±ìŠ¤ Maps to ê´€ê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _search_concepts_in_all_indices(self, concept_ids: List[str], domain_id: str) -> List[Dict[str, Any]]:
        """
        ì—”í‹°í‹° íƒ€ì…ì— ë”°ë¼ ì§€ì •ëœ ë„ë©”ì¸ì˜ concept ì¸ë±ìŠ¤ì—ì„œ concept_idë“¤ ê²€ìƒ‰
        
        Args:
            concept_ids: ê²€ìƒ‰í•  concept_id ë¦¬ìŠ¤íŠ¸
            domain_id: ê²€ìƒ‰í•  ë„ë©”ì¸ ID
            
        Returns:
            List[ì°¾ì€ ì»¨ì…‰ë“¤]
        """
        all_candidates = []
        
        try:
            concepts_query = {
                "query": {
                    "bool": {
                        "must": [
                            {"terms": {"concept_id": concept_ids}},
                            {"terms": {"standard_concept": ["S", "C"]}}
                        ]
                    }
                },
                "size": len(concept_ids)
            }
            
            concepts_response = self.es_client.es_client.search(
                index="concept",
                body=concepts_query
            )
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {concepts_response['hits']['total']['value']}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            for hit in concepts_response['hits']['hits']:
                all_candidates.append(hit['_source'])
                
            if concepts_response['hits']['total']['value'] > 0:
                logger.info(f"{concepts_response['hits']['total']['value']}ê°œ standard concept ë°œê²¬")
            
        except Exception as e:
            logger.warning(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        
        return all_candidates
    
    def _calculate_similarity(self, entity_name: str, concept_name: str) -> float:
        """
        ë‘ ë¬¸ìì—´ ê°„ì˜ Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            entity_name: ì›ë³¸ ì—”í‹°í‹° ì´ë¦„
            concept_name: ë¹„êµí•  ì»¨ì…‰ ì´ë¦„
            
        Returns:
            Jaccard ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not entity_name or not concept_name:
            return 0.0
        
        # ëŒ€ì†Œë¬¸ì ì •ê·œí™”
        entity_name = entity_name.lower()
        concept_name = concept_name.lower()
        
        # n-gram 3ìœ¼ë¡œ ë¶„í• 
        entity_ngrams = self._get_ngrams(entity_name, n=3)
        concept_ngrams = self._get_ngrams(concept_name, n=3)
        
        if not entity_ngrams or not concept_ngrams:
            return 0.0
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        intersection = entity_ngrams.intersection(concept_ngrams)
        union = entity_ngrams.union(concept_ngrams)
        jaccard_similarity = len(intersection) / len(union) if union else 0.0
        
        return jaccard_similarity
    
    def _get_ngrams(self, text: str, n: int = 3) -> set:
        """
        í…ìŠ¤íŠ¸ë¥¼ n-gramìœ¼ë¡œ ë¶„í• 
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            n: n-gram í¬ê¸° (ê¸°ë³¸ê°’: 3)
            
        Returns:
            n-gram ì§‘í•©
        """
        if len(text) < n:
            return {text}
        
        ngrams = set()
        for i in range(len(text) - n + 1):
            ngrams.add(text[i:i + n])
        
        return ngrams
    
    def _create_mapping_result(self, entity_input: EntityInput, best_candidate: Dict[str, Any], alternative_candidates: List[Dict[str, Any]]) -> MappingResult:
        """
        ë§¤í•‘ ê²°ê³¼ ìƒì„±
        
        Args:
            entity_input: ì›ë³¸ ì—”í‹°í‹° ì…ë ¥
            best_candidate: ìµœì  í›„ë³´
            alternative_candidates: ëŒ€ì•ˆ í›„ë³´ë“¤
            
        Returns:
            MappingResult: ë§¤í•‘ ê²°ê³¼
        """
        concept = best_candidate['concept']
        final_score = best_candidate['final_score']
        
        # ëŒ€ì•ˆ ì»¨ì…‰ë“¤ ì¶”ì¶œ
        alternative_concepts = []
        for alt_candidate in alternative_candidates:
            if 'concept' in alt_candidate:
                alt_concept = alt_candidate['concept']
                alternative_concepts.append({
                    'concept_id': str(alt_concept.get('concept_id', '')),
                    'concept_name': alt_concept.get('concept_name', ''),
                    'vocabulary_id': alt_concept.get('vocabulary_id', ''),
                    'score': alt_candidate.get('final_score', 0)
                })
        
        # ë§¤í•‘ ë°©ë²• ê²°ì •
        mapping_method = "direct_standard" if best_candidate['is_original_standard'] else "non_standard_to_standard"
        
        # ë§¤í•‘ ì‹ ë¢°ë„ ê³„ì‚° (final_score ì‚¬ìš©)
        mapping_score = final_score
        mapping_confidence = self._determine_confidence(mapping_score)
        
        return MappingResult(
            source_entity=entity_input,
            mapped_concept_id=str(concept.get('concept_id', '')),
            mapped_concept_name=concept.get('concept_name', ''),
            domain_id=concept.get('domain_id', ''),
            vocabulary_id=concept.get('vocabulary_id', ''),
            concept_class_id=concept.get('concept_class_id', ''),
            standard_concept=concept.get('standard_concept', ''),
            concept_code=concept.get('concept_code', ''),
            valid_start_date=concept.get('valid_start_date'),
            valid_end_date=concept.get('valid_end_date'),
            invalid_reason=concept.get('invalid_reason'),
            concept_embedding=concept.get('concept_embedding'),
            mapping_score=mapping_score,
            mapping_confidence=mapping_confidence,
            mapping_method=mapping_method,
            alternative_concepts=alternative_concepts
        )
    
    def _determine_confidence(self, score: float) -> str:
        """
        ë§¤í•‘ ì‹ ë¢°ë„ ê²°ì • (0.0 ~ 1.0 ì ìˆ˜ ê¸°ì¤€)
        
        ì‹ ë¢°ë„ ê¸°ì¤€:
        - 0.95 ~ 1.00: very_high (ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        - 0.85 ~ 0.94: high (ë†’ì€ ìœ ì‚¬ë„)
        - 0.70 ~ 0.84: medium (ì¤‘ê°„ ìœ ì‚¬ë„)
        - 0.50 ~ 0.69: low (ë‚®ì€ ìœ ì‚¬ë„)
        - 0.00 ~ 0.49: very_low (ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„)
        """
        if score >= 0.95:
            return "very_high"
        elif score >= 0.85:
            return "high"
        elif score >= 0.70:
            return "medium"
        elif score >= 0.50:
            return "low"
        else:
            return "very_low"
    
    def _calculate_hybrid_score(self, entity_name: str, concept_name: str, 
                              elasticsearch_score: float, concept_source: Dict[str, Any], 
                              text_weight: float = 0.4, semantic_weight: float = 0.6) -> tuple:
        """
        í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            concept_name: ì»¨ì…‰ ì´ë¦„
            elasticsearch_score: Elasticsearch ì ìˆ˜
            concept_source: ì»¨ì…‰ ì†ŒìŠ¤ ë°ì´í„°
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.4)
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.6)
            
        Returns:
            tuple: (í•˜ì´ë¸Œë¦¬ë“œ_ì ìˆ˜, í…ìŠ¤íŠ¸_ìœ ì‚¬ë„, ì˜ë¯¸ì _ìœ ì‚¬ë„)
        """
        try:
            # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            text_similarity = self._calculate_similarity(entity_name, concept_name)
            
            # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            concept_embedding = concept_source.get('concept_embedding')
            if concept_embedding and len(concept_embedding) == 768:
                # SapBERT ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                try:
                    # ì—”í‹°í‹° ì„ë² ë”© ìƒì„± (SapBERT ì‚¬ìš©)
                    entity_embedding = self._get_simple_embedding(entity_name) if HAS_SAPBERT else None
                    
                    if entity_embedding is not None:
                        concept_emb_array = np.array(concept_embedding).reshape(1, -1)
                        entity_emb_array = entity_embedding.reshape(1, -1)
                        semantic_similarity = cosine_similarity(entity_emb_array, concept_emb_array)[0][0]
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” -1~1 ë²”ìœ„ì´ë¯€ë¡œ 0~1ë¡œ ì •ê·œí™”
                        semantic_similarity = (semantic_similarity + 1.0) / 2.0
                        logger.debug(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì„±ê³µ: {semantic_similarity:.4f} for {concept_source.get('concept_name', 'N/A')}")
                    else:
                        # ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ì‹œ 0.0ìœ¼ë¡œ ì„¤ì •
                        semantic_similarity = 0.0
                        logger.debug(f"ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©: {concept_source.get('concept_name', 'N/A')}")
                        
                except Exception as e:
                    logger.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    semantic_similarity = 0.0
            else:
                # ì„ë² ë”©ì´ ì—†ëŠ” ê²½ìš° 0.0ìœ¼ë¡œ ì„¤ì • (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ êµ¬ë¶„)
                semantic_similarity = 0.0
                logger.debug(f"ì»¨ì…‰ ì„ë² ë”© ì—†ìŒ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©: {concept_source.get('concept_name', 'N/A')}")
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
            hybrid_score = (text_weight * text_similarity) + (semantic_weight * semantic_similarity)
            
            # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì œí•œ
            hybrid_score = max(0.0, min(1.0, hybrid_score))
            text_similarity = max(0.0, min(1.0, text_similarity))
            semantic_similarity = max(0.0, min(1.0, semantic_similarity))
            
            return hybrid_score, text_similarity, semantic_similarity
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ Python ìœ ì‚¬ë„ ì‚¬ìš©
            fallback_similarity = self._calculate_similarity(entity_name, concept_name)
            return fallback_similarity, fallback_similarity, fallback_similarity
    
    def _get_simple_embedding(self, text: str):
        """
        SapBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        """
        try:
            # SapBERT ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not hasattr(self, '_sapbert_model') or self._sapbert_model is None:
                self._initialize_sapbert_model()
            
            if self._sapbert_model is None:
                return None
            
            # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
            text = text.lower().strip()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self._sapbert_tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=25
            )
            inputs = {k: v.to(self._sapbert_device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self._sapbert_model(**inputs)
                # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _initialize_sapbert_model(self):
        """SapBERT ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        try:
            if not HAS_SAPBERT:
                logger.warning("SapBERT ê´€ë ¨ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                self._sapbert_model = None
                self._sapbert_tokenizer = None
                self._sapbert_device = None
                return
            
            model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
            logger.info(f"ğŸ¤– SapBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            
            self._sapbert_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self._sapbert_model = AutoModel.from_pretrained(model_name)
            # GPUê°€ ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œ ì ìœ ëœ ê²½ìš° CPU ì‚¬ìš©
            self._sapbert_device = torch.device('cpu')  # ì„ì‹œë¡œ CPU ê°•ì œ ì‚¬ìš©
            self._sapbert_model.to(self._sapbert_device)
            self._sapbert_model.eval()
            
            logger.info(f"âœ… SapBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {self._sapbert_device})")
            
        except Exception as e:
            logger.error(f"âŒ SapBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._sapbert_model = None
            self._sapbert_tokenizer = None
            self._sapbert_device = None
    
    def health_check(self) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸"""
        es_health = self.es_client.health_check()
        
        return {
            "api_status": "healthy",
            "elasticsearch_status": es_health,
            "confidence_threshold": self.confidence_threshold
        }

# API í¸ì˜ í•¨ìˆ˜ë“¤
def map_single_entity(
    entity_name: str,
    entity_type: str,
    domain_id: Optional[DomainID] = None,
    vocabulary_id: Optional[str] = None,
    confidence: float = 1.0
) -> Optional[MappingResult]:
    """
    ë‹¨ì¼ ì—”í‹°í‹° ë§¤í•‘ í¸ì˜ í•¨ìˆ˜
    
    Args:
        entity_name: ì—”í‹°í‹° ì´ë¦„
        entity_type: ì—”í‹°í‹° íƒ€ì… ('diagnostic', 'drug', 'test', 'surgery')
        domain_id: OMOP ë„ë©”ì¸ ID (ì„ íƒì‚¬í•­)
        vocabulary_id: OMOP ì–´íœ˜ì²´ê³„ ID (ì„ íƒì‚¬í•­)
        confidence: ì—”í‹°í‹° ì‹ ë¢°ë„
        
    Returns:
        MappingResult: ë§¤í•‘ ê²°ê³¼ ë˜ëŠ” None
    """
    try:
        api = EntityMappingAPI()
        
        entity_input = EntityInput(
            entity_name=entity_name,
            domain_id=domain_id if isinstance(domain_id, DomainID) else (DomainID(domain_id) if domain_id else None),
            vocabulary_id=vocabulary_id,
            confidence=confidence
        )
        
        return api.map_entity(entity_input)
        
    except ValueError:
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”í‹°í‹° íƒ€ì…: {entity_type}")
        return None