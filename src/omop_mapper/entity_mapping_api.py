from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging
from enum import Enum

from .elasticsearch_client import ElasticsearchClient
from .mapping_stages import (
    Stage1CandidateRetrieval,
    Stage2StandardCollection,
    Stage3HybridScoring
)
from .mapping_validation import MappingValidator

try:
    import torch
    import numpy as np
    from transformers import AutoTokenizer, AutoModel
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SAPBERT = True
except ImportError:
    HAS_SAPBERT = False

logger = logging.getLogger(__name__)


class DomainID(Enum):
    """ë„ë©”ì¸ ID (Elasticsearchì— ì €ì¥ëœ í˜•ì‹ê³¼ ì¼ì¹˜)"""
    PROCEDURE = "Procedure"
    CONDITION = "Condition"
    DRUG = "Drug"
    OBSERVATION = "Observation"
    MEASUREMENT = "Measurement"
    THRESHOLD = "Threshold"
    DEMOGRAPHICS = "Demographics"
    PERIOD = "Period"
    PROVIDER = "Provider"
    DEVICE = "Device"


@dataclass
class EntityInput:
    """
    ì…ë ¥ìš© ì—”í‹°í‹° ë°ì´í„°
    
    Args:
        entity_name: ë§¤í•‘í•  ì—”í‹°í‹° ì´ë¦„
        domain_id: ë„ë©”ì¸ ID (Noneì´ë©´ ëª¨ë“  ë„ë©”ì¸ ê²€ìƒ‰, ì§€ì •í•˜ë©´ í•´ë‹¹ ë„ë©”ì¸ë§Œ ê²€ìƒ‰)
        vocabulary_id: ì–´íœ˜ì²´ê³„ ID (ì„ íƒì‚¬í•­)
    """
    entity_name: str
    domain_id: Optional[DomainID] = None
    vocabulary_id: Optional[str] = None


@dataclass
class MappingResult:
    """ë§¤í•‘ ê²°ê³¼ ë°ì´í„°"""
    source_entity: EntityInput
    mapped_concept_id: str
    mapped_concept_name: str
    domain_id: str
    vocabulary_id: str
    concept_class_id: str
    standard_concept: str
    concept_code: str
    concept_embedding: List[float]
    valid_start_date: Optional[str] = None
    valid_end_date: Optional[str] = None
    invalid_reason: Optional[str] = None
    mapping_score: float = 0.0
    mapping_confidence: str = "low"
    mapping_method: str = "unknown"
    alternative_concepts: List[Dict[str, Any]] = None
    

class EntityMappingAPI:
    """ì—”í‹°í‹° ë§¤í•‘ API í´ë˜ìŠ¤ (3ë‹¨ê³„ ë§¤í•‘ íŒŒì´í”„ë¼ì¸)"""

    def __init__(
        self,
        es_client: Optional[ElasticsearchClient] = None,
        confidence_threshold: float = 0.5
    ):
        """
        ì—”í‹°í‹° ë§¤í•‘ API ì´ˆê¸°í™”
        
        Args:
            es_client: Elasticsearch í´ë¼ì´ì–¸íŠ¸
            confidence_threshold: ë§¤í•‘ ì‹ ë¢°ë„ ì„ê³„ì¹˜
        """
        self.es_client = es_client or ElasticsearchClient.create_default()
        self.confidence_threshold = confidence_threshold
        
        # SapBERT ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self._sapbert_model = None
        self._sapbert_tokenizer = None
        self._sapbert_device = None
        
        # Stage ëª¨ë“ˆ ì´ˆê¸°í™”
        self.stage1 = Stage1CandidateRetrieval(
            es_client=self.es_client,
            has_sapbert=HAS_SAPBERT
        )
        
        self.stage2 = Stage2StandardCollection(
            es_client=self.es_client
        )
        
        self.stage3 = None  # SapBERT ëª¨ë¸ ë¡œë”© í›„ ì´ˆê¸°í™”
        
        # ê²€ì¦ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.validator = MappingValidator(
            es_client=self.es_client,
            openai_api_key=None,  # .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´
            openai_model="gpt-4o-mini"
        )
        
        # ë””ë²„ê¹…ìš© ë³€ìˆ˜
        self._last_stage1_candidates = []
        self._last_stage2_candidates = []
        self._last_rerank_candidates = []
    
    def map_entity(self, entity_input: EntityInput) -> Optional[List[MappingResult]]:
        """
        ë‹¨ì¼ ì—”í‹°í‹°ë¥¼ OMOP CDMì— 3ë‹¨ê³„ ë§¤í•‘
        
        **ë„ë©”ì¸ ê²€ìƒ‰ ì „ëµ**:
        - entity_input.domain_idê°€ Noneì´ë©´: 6ê°œ ì£¼ìš” ë„ë©”ì¸ ëª¨ë‘ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ìµœì  ë§¤í•‘ ì°¾ê¸°
        - entity_input.domain_idê°€ ì§€ì •ë˜ë©´: í•´ë‹¹ ë„ë©”ì¸ì—ì„œë§Œ ê²€ìƒ‰ (íŠ¹ì • ë„ë©”ì¸ ë§¤í•‘ì´ í•„ìš”í•œ ê²½ìš°)
        
        **3ë‹¨ê³„ ë§¤í•‘ íŒŒì´í”„ë¼ì¸** (ê° ë„ë©”ì¸ë³„ë¡œ ìˆ˜í–‰):
        - Stage 1: Elasticsearchì—ì„œ í›„ë³´êµ° 9ê°œ ì¶”ì¶œ (Lexical 3 + Semantic 3 + Combined 3)
        - Stage 2: Non-standard to Standard ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
        - Stage 3: LLM ê¸°ë°˜ í‰ê°€ ë° ìµœì¢… ë­í‚¹
        
        Args:
            entity_input: ë§¤í•‘í•  ì—”í‹°í‹° ì •ë³´
            
        Returns:
            List[MappingResult]: ê° ë„ë©”ì¸ë³„ ë§¤í•‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìµœê³  ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬)
        """
        try:
            entity_name = entity_input.entity_name
            input_domain = entity_input.domain_id
            
            # ===== ê²€ìƒ‰ ëŒ€ìƒ ë„ë©”ì¸ ê²°ì • =====
            if input_domain is None:
                # ì¼€ì´ìŠ¤ 1: ì—”í‹°í‹°ë§Œ ì œê³µëœ ê²½ìš° â†’ ëª¨ë“  ì£¼ìš” ë„ë©”ì¸ ê²€ìƒ‰
                target_domains = [
                    DomainID.DRUG,
                    DomainID.OBSERVATION,
                    DomainID.PROCEDURE,
                    DomainID.CONDITION,
                    DomainID.MEASUREMENT,
                    DomainID.DEVICE
                ]
                logger.info("=" * 100)
                logger.info(f"ğŸš€ ì „ì²´ ë„ë©”ì¸ 3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ ì‹œì‘")
                logger.info(f"   ì—”í‹°í‹°: {entity_name}")
                logger.info(f"   ê²€ìƒ‰ ì „ëµ: ëª¨ë“  ë„ë©”ì¸ ê²€ìƒ‰ í›„ ìµœì  ë§¤í•‘ ì„ íƒ")
                logger.info(f"   ëŒ€ìƒ ë„ë©”ì¸: Drug, Observation, Procedure, Condition, Measurement, Device (6ê°œ)")
                logger.info("=" * 100)
            else:
                # ì¼€ì´ìŠ¤ 2: ì—”í‹°í‹° + ë„ë©”ì¸ ì œê³µëœ ê²½ìš° â†’ í•´ë‹¹ ë„ë©”ì¸ë§Œ ê²€ìƒ‰
                target_domains = [input_domain]
                logger.info("=" * 100)
                logger.info(f"ğŸš€ ë‹¨ì¼ ë„ë©”ì¸ 3ë‹¨ê³„ ì—”í‹°í‹° ë§¤í•‘ ì‹œì‘")
                logger.info(f"   ì—”í‹°í‹°: {entity_name}")
                logger.info(f"   ê²€ìƒ‰ ì „ëµ: ì§€ì •ëœ ë„ë©”ì¸ì—ì„œë§Œ ê²€ìƒ‰")
                logger.info(f"   ëŒ€ìƒ ë„ë©”ì¸: {input_domain.value} (1ê°œ)")
                logger.info("=" * 100)
            
            # SapBERT ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            if HAS_SAPBERT and self._sapbert_model is None:
                self._initialize_sapbert_model()
            
            # Stage 3 ì´ˆê¸°í™” (SapBERT ëª¨ë¸ ë¡œë”© í›„)
            if self.stage3 is None:
                self.stage3 = Stage3HybridScoring(
                    sapbert_model=self._sapbert_model,
                    sapbert_tokenizer=self._sapbert_tokenizer,
                    sapbert_device=self._sapbert_device,
                    text_weight=0.4,
                    semantic_weight=0.6,
                    es_client=self.es_client,
                    openai_api_key=None,  # .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´
                    openai_model="gpt-4o-mini"
                )
            
            # ì—”í‹°í‹° ì„ë² ë”© ìƒì„±
            entity_embedding = None
            if HAS_SAPBERT and self._sapbert_model is not None:
                entity_embedding = self._get_simple_embedding(entity_name)
                if entity_embedding is not None:
                    logger.info("âœ… ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì„±ê³µ")
                else:
                    logger.warning("âš ï¸ ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # ê° ë„ë©”ì¸ë³„ ë§¤í•‘ ê²°ê³¼ ì €ì¥
            all_mapping_results = []
            self._last_domain_results = {}  # ë„ë©”ì¸ë³„ ê²°ê³¼ ì €ì¥
            self._all_domain_stage_results = {}  # ë„ë©”ì¸ë³„ Stage ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
            domain_candidates = {}  # ê²€ìƒ‰ ë„ë©”ì¸ë³„ í›„ë³´êµ° ì €ì¥ (Best Domain ì„ íƒìš©)
            result_to_search_domain = {}  # ê²°ê³¼ ê°ì²´ -> ê²€ìƒ‰ ë„ë©”ì¸ ë§¤í•‘
            
            # ê° ë„ë©”ì¸ë³„ë¡œ Stage 1, 2, 3 ìˆ˜í–‰
            for domain in target_domains:
                domain_result, domain_stages = self._map_entity_for_domain(
                    entity_name=entity_name,
                    domain_id=domain,
                    entity_embedding=entity_embedding,
                    entity_input=entity_input
                )
                
                if domain_result:
                    all_mapping_results.append(domain_result)
                    search_domain_str = str(domain.value)
                    
                    self._last_domain_results[search_domain_str] = domain_result
                    self._all_domain_stage_results[search_domain_str] = domain_stages
                    
                    # ë„ë©”ì¸ë³„ í›„ë³´êµ° ì €ì¥ (ê²€ìƒ‰ ë„ë©”ì¸ì„ í‚¤ë¡œ ì‚¬ìš©)
                    if 'candidates' in domain_stages:
                        domain_candidates[search_domain_str] = domain_stages['candidates']
                    
                    # ê²°ê³¼ ê°ì²´ -> ê²€ìƒ‰ ë„ë©”ì¸ ë§¤í•‘ ì €ì¥ (ë‚˜ì¤‘ì— Best ê²°ê³¼ì˜ ê²€ìƒ‰ ë„ë©”ì¸ì„ ì°¾ê¸° ìœ„í•¨)
                    result_to_search_domain[id(domain_result)] = search_domain_str
            
            logger.info("\n" + "=" * 100)
            logger.info(f"âœ… ë„ë©”ì¸ë³„ ë§¤í•‘ ì™„ë£Œ: {len(all_mapping_results)}ê°œ ë„ë©”ì¸ì—ì„œ ê²°ê³¼ ë°œê²¬")
            logger.info("=" * 100)
            
            # ë„ë©”ì¸ë³„ ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° Best Domainì˜ í›„ë³´êµ° ì„¤ì •
            if all_mapping_results:
                logger.info("\nğŸ“Š ì „ì²´ ë„ë©”ì¸ ìµœì¢… ê²°ê³¼:")
                for idx, result in enumerate(all_mapping_results, 1):
                    logger.info(f"  {idx}. [{result.domain_id}] {result.mapped_concept_name} - ì ìˆ˜: {result.mapping_score:.4f}")
                
                best = max(all_mapping_results, key=lambda x: x.mapping_score)
                logger.info(f"\nğŸ† ìµœê³  ì ìˆ˜: [{best.domain_id}] {best.mapped_concept_name} ({best.mapping_score:.4f})")
                
                # Best resultê°€ ì–´ëŠ ê²€ìƒ‰ ë„ë©”ì¸ì—ì„œ ë‚˜ì™”ëŠ”ì§€ ì°¾ê¸°
                best_search_domain = result_to_search_domain.get(id(best))
                
                if best_search_domain and best_search_domain in domain_candidates:
                    best_candidates = domain_candidates[best_search_domain]
                    self._last_stage1_candidates = best_candidates.get('stage1', [])
                    self._last_stage2_candidates = best_candidates.get('stage2', [])
                    self._last_rerank_candidates = best_candidates.get('stage3', [])
                    logger.info(f"âœ… Best resultì˜ ê²€ìƒ‰ ë„ë©”ì¸ [{best_search_domain}]ì˜ í›„ë³´êµ°ì„ ë””ë²„ê¹… ë³€ìˆ˜ì— ì €ì¥")
                    logger.info(f"   (ê²°ê³¼ ë„ë©”ì¸: [{best.domain_id}])")
                else:
                    logger.warning(f"âš ï¸ Best resultì˜ ê²€ìƒ‰ ë„ë©”ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {best.domain_id}")
            
            return all_mapping_results if all_mapping_results else None
                
        except Exception as e:
            logger.error(f"âš ï¸ ì—”í‹°í‹° ë§¤í•‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return None
    
    def _map_entity_for_domain(
        self,
        entity_name: str,
        domain_id,
        entity_embedding,
        entity_input: EntityInput
    ) -> tuple[Optional[MappingResult], Dict[str, Any]]:
        """
        íŠ¹ì • ë„ë©”ì¸ì— ëŒ€í•´ 3ë‹¨ê³„ ë§¤í•‘ ìˆ˜í–‰
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            domain_id: ë„ë©”ì¸ ID
            entity_embedding: ì—”í‹°í‹° ì„ë² ë”©
            entity_input: ì›ë³¸ ì—”í‹°í‹° ì…ë ¥
            
        Returns:
            tuple: (MappingResult, Stage ê²°ê³¼ ë”•ì…”ë„ˆë¦¬) ë˜ëŠ” (None, {})
        """
        try:
            domain_str = str(domain_id.value) if hasattr(domain_id, 'value') else str(domain_id)
            
            logger.info("\n" + "=" * 100)
            logger.info(f"ğŸ“ ë„ë©”ì¸: {domain_str.upper()}")
            logger.info("=" * 100)
            
            # Stageë³„ ê²°ê³¼ ì €ì¥ìš©
            stage_results = {
                'search_domain': domain_str,  # ê²€ìƒ‰í•œ ë„ë©”ì¸
                'result_domain': None,  # ì‹¤ì œ ê²°ê³¼ ë„ë©”ì¸ (ë‚˜ì¤‘ì— ì„¤ì •)
                'stage1_count': 0,
                'stage2_count': 0,
                'stage3_count': 0,
                'candidates': {}  # í›„ë³´êµ° ì •ë³´ ì €ì¥
            }
            
            # ===== Stage 1: í›„ë³´êµ° 9ê°œ ì¶”ì¶œ =====
            es_index = getattr(self.es_client, 'concept_index', 'concept')
            stage1_candidates = self.stage1.retrieve_candidates(
                entity_name=entity_name,
                domain_id=domain_str,
                entity_embedding=entity_embedding,
                es_index=es_index
            )
            
            if not stage1_candidates:
                logger.info(f"âš ï¸ [{domain_str}] Stage 1 - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return None, {}
            
            stage_results['stage1_count'] = len(stage1_candidates)
            
            # ===== Stage 2: Standard í›„ë³´ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±° =====
            stage2_candidates = self.stage2.collect_standard_candidates(
                stage1_candidates=stage1_candidates,
                domain_id=domain_str
            )
            
            if not stage2_candidates:
                logger.info(f"âš ï¸ [{domain_str}] Stage 2 - Standard í›„ë³´ ì—†ìŒ")
                return None, {}
            
            stage_results['stage2_count'] = len(stage2_candidates)
            
            # ===== Stage 3: LLM ê¸°ë°˜ í‰ê°€ =====
            stage3_candidates = self.stage3.calculate_hybrid_scores(
                entity_name=entity_name,
                stage2_candidates=stage2_candidates,
                stage1_candidates=stage1_candidates
            )
            
            if not stage3_candidates:
                logger.info(f"âš ï¸ [{domain_str}] Stage 3 - LLM í‰ê°€ ì‹¤íŒ¨")
                return None, {}
            
            stage_results['stage3_count'] = len(stage3_candidates)
            
            # ===== ìµœì¢… ë§¤í•‘ ê²°ê³¼ ìƒì„± =====
            # entity_inputì˜ domain_idë¥¼ í˜„ì¬ ë„ë©”ì¸ìœ¼ë¡œ ì„¤ì •
            domain_entity_input = EntityInput(
                entity_name=entity_input.entity_name,
                domain_id=domain_id if isinstance(domain_id, DomainID) else None,
                vocabulary_id=entity_input.vocabulary_id
            )
            
            # LLM ë°©ì‹ ê²°ê³¼ ì‚¬ìš©
            mapping_result = self._create_final_mapping_result(domain_entity_input, stage3_candidates)
            
            # ===== ê²€ì¦ ë‹¨ê³„ =====
            logger.info("\n" + "=" * 100)
            logger.info("ğŸ” ë§¤í•‘ ê²€ì¦ ì‹œì‘")
            logger.info("=" * 100)
            
            # ìµœì¢… ë§¤í•‘ ê²°ê³¼ ê²€ì¦
            is_valid = self.validator.validate_mapping(
                entity_name=entity_name,
                concept_id=mapping_result.mapped_concept_id,
                concept_name=mapping_result.mapped_concept_name,
                synonyms=None  # Elasticsearchì—ì„œ ì¡°íšŒ
            )
            
            if not is_valid:
                logger.warning(f"âš ï¸ [{domain_str}] ìµœì¢… ë§¤í•‘ ê²€ì¦ ì‹¤íŒ¨: {mapping_result.mapped_concept_name}")
                logger.info("ğŸ” í›„ë³´êµ° ìˆœì°¨ ê²€ì¦ ì‹œì‘...")
                
                # ì›ë˜ í›„ë³´ ì •ë³´ ì €ì¥
                original_candidate_id = mapping_result.mapped_concept_id
                original_candidate_name = mapping_result.mapped_concept_name
                
                # í›„ë³´êµ° ìˆœì°¨ ê²€ì¦
                validated_candidate = self.validator.validate_candidates_sequentially(
                    entity_name=entity_name,
                    candidates=stage3_candidates,
                    max_candidates=10
                )
                
                if validated_candidate:
                    # ê²€ì¦ í†µê³¼í•œ í›„ë³´ë¡œ ë§¤í•‘ ê²°ê³¼ ì¬ìƒì„±
                    validated_concept_name = validated_candidate['concept'].get('concept_name', '')
                    logger.info(f"âœ… ê²€ì¦ í†µê³¼í•œ í›„ë³´ ë°œê²¬: {validated_concept_name}")
                    
                    # ê²€ì¦ í†µê³¼í•œ í›„ë³´ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                    validated_idx = None
                    for idx, candidate in enumerate(stage3_candidates):
                        if candidate['concept'].get('concept_id') == validated_candidate['concept'].get('concept_id'):
                            validated_idx = idx
                            break
                    
                    # ê²€ì¦ í†µê³¼í•œ í›„ë³´ë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë§¤í•‘ ê²°ê³¼ ì¬ìƒì„±
                    if validated_idx is not None and validated_idx > 0:
                        reordered_candidates = [stage3_candidates[validated_idx]] + [
                            c for i, c in enumerate(stage3_candidates) if i != validated_idx
                        ]
                    else:
                        reordered_candidates = stage3_candidates
                    
                    mapping_result = self._create_final_mapping_result(domain_entity_input, reordered_candidates)
                    stage_results['validation_status'] = 'validated_alternative'
                    stage_results['original_candidate'] = {
                        'concept_id': original_candidate_id,
                        'concept_name': original_candidate_name
                    }
                    stage_results['validated_candidate'] = {
                        'concept_id': mapping_result.mapped_concept_id,
                        'concept_name': mapping_result.mapped_concept_name
                    }
                else:
                    logger.error(f"âŒ [{domain_str}] ëª¨ë“  í›„ë³´ ê²€ì¦ ì‹¤íŒ¨ - ë§¤í•‘ ì‹¤íŒ¨")
                    stage_results['validation_status'] = 'failed'
                    return None, stage_results
            else:
                logger.info(f"âœ… [{domain_str}] ìµœì¢… ë§¤í•‘ ê²€ì¦ í†µê³¼: {mapping_result.mapped_concept_name}")
                stage_results['validation_status'] = 'validated'
            
            # ì‹¤ì œ ê²°ê³¼ ë„ë©”ì¸ ì €ì¥
            stage_results['result_domain'] = mapping_result.domain_id
            
            logger.info(f"\nâœ… [{domain_str}] ë§¤í•‘ ì™„ë£Œ!")
            logger.info(f"   ê²€ìƒ‰ ë„ë©”ì¸: {domain_str} â†’ ê²°ê³¼ ë„ë©”ì¸: {mapping_result.domain_id}")
            logger.info(f"   ê°œë…: {mapping_result.mapped_concept_name} (ID: {mapping_result.mapped_concept_id})")
            logger.info(f"   ì ìˆ˜: {mapping_result.mapping_score:.4f} | ì‹ ë¢°ë„: {mapping_result.mapping_confidence}")
            logger.info(f"   Stage ê²½ë¡œ: {stage_results['stage1_count']}ê°œ â†’ {stage_results['stage2_count']}ê°œ â†’ {stage_results['stage3_count']}ê°œ")
            logger.info(f"   ê²€ì¦ ìƒíƒœ: {stage_results.get('validation_status', 'unknown')}")
            
            # ë„ë©”ì¸ë³„ í›„ë³´êµ° ì •ë³´ë¥¼ stage_resultsì— ì €ì¥
            stage_results['candidates'] = {
                'stage1': [
                    {
                        'concept_id': str(hit['_source'].get('concept_id', '')),
                        'concept_name': hit['_source'].get('concept_name', ''),
                        'domain_id': hit['_source'].get('domain_id', ''),
                        'vocabulary_id': hit['_source'].get('vocabulary_id', ''),
                        'standard_concept': hit['_source'].get('standard_concept', ''),
                        'elasticsearch_score': hit['_score'],
                        'search_type': hit.get('_search_type', 'unknown')
                    }
                    for hit in stage1_candidates
                ],
                'stage2': [
                    {
                        'concept_id': str(c['concept'].get('concept_id', '')),
                        'concept_name': c['concept'].get('concept_name', ''),
                        'domain_id': c['concept'].get('domain_id', ''),
                        'vocabulary_id': c['concept'].get('vocabulary_id', ''),
                        'standard_concept': c['concept'].get('standard_concept', ''),
                        'is_original_standard': c['is_original_standard'],
                        'search_type': c.get('search_type', 'unknown'),
                        'original_non_standard': c.get('original_non_standard', None)
                    }
                    for c in stage2_candidates
                ],
                'stage3': [
                    {
                        'concept_id': str(c['concept'].get('concept_id', '')),
                        'concept_name': c['concept'].get('concept_name', ''),
                        'domain_id': c['concept'].get('domain_id', ''),
                        'vocabulary_id': c['concept'].get('vocabulary_id', ''),
                        'standard_concept': c['concept'].get('standard_concept', ''),
                        'llm_score': c.get('llm_score', None),
                        'llm_rank': c.get('llm_rank', None),
                        'llm_reasoning': c.get('llm_reasoning', None),
                        'final_score': c.get('final_score', 0.0),
                        'search_type': c.get('search_type', 'unknown')
                    }
                    for c in stage3_candidates
                ]
            }
            
            return mapping_result, stage_results
            
        except Exception as e:
            logger.error(f"âš ï¸ [{domain_str}] ë§¤í•‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return None, {}
    
    def _create_final_mapping_result(
        self, 
        entity_input: EntityInput, 
        sorted_candidates: List[Dict[str, Any]]
    ) -> MappingResult:
        """
        ìµœì¢… ë§¤í•‘ ê²°ê³¼ ìƒì„±
        
        Args:
            entity_input: ì›ë³¸ ì—”í‹°í‹° ì…ë ¥
            sorted_candidates: ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤
            
        Returns:
            MappingResult: ë§¤í•‘ ê²°ê³¼
        """
        best_candidate = sorted_candidates[0]
        alternative_candidates = sorted_candidates[1:4]  # ìƒìœ„ 3ê°œ ëŒ€ì•ˆ
        
        mapping_result = self._create_mapping_result(entity_input, best_candidate, alternative_candidates)
        
        mapping_type = "direct_standard" if best_candidate['is_original_standard'] else "non_standard_to_standard"
        logger.debug(f"ë§¤í•‘ ìœ í˜•: {mapping_type}")
        
        return mapping_result
    
    def _create_mapping_result(
        self, 
        entity_input: EntityInput, 
        best_candidate: Dict[str, Any], 
        alternative_candidates: List[Dict[str, Any]]
    ) -> MappingResult:
        """
        ë§¤í•‘ ê²°ê³¼ ìƒì„±
        
        Args:
            entity_input: ì›ë³¸ ì—”í‹°í‹° ì…ë ¥
            best_candidate: ìµœì  í›„ë³´
            alternative_candidates: ëŒ€ì•ˆ í›„ë³´ë“¤
            
        Returns:
            MappingResult: ë§¤í•‘ ê²°ê³¼
        """
        concept = best_candidate['concept']
        final_score = best_candidate['final_score']
        
        # ëŒ€ì•ˆ ê°œë…ë“¤ ì¶”ì¶œ
        alternative_concepts = []
        for alt_candidate in alternative_candidates:
            if 'concept' in alt_candidate:
                alt_concept = alt_candidate['concept']
                alternative_concepts.append({
                    'concept_id': str(alt_concept.get('concept_id', '')),
                    'concept_name': alt_concept.get('concept_name', ''),
                    'vocabulary_id': alt_concept.get('vocabulary_id', ''),
                    'score': alt_candidate.get('final_score', 0)
                })
        
        # ë§¤í•‘ ë°©ë²• ê²°ì •
        mapping_method = "direct_standard" if best_candidate['is_original_standard'] else "non_standard_to_standard"
        
        # ë§¤í•‘ ì‹ ë¢°ë„ ê³„ì‚°
        mapping_score = final_score
        mapping_confidence = self._determine_confidence(mapping_score)
        
        return MappingResult(
            source_entity=entity_input,
            mapped_concept_id=str(concept.get('concept_id', '')),
            mapped_concept_name=concept.get('concept_name', ''),
            domain_id=concept.get('domain_id', ''),
            vocabulary_id=concept.get('vocabulary_id', ''),
            concept_class_id=concept.get('concept_class_id', ''),
            standard_concept=concept.get('standard_concept', ''),
            concept_code=concept.get('concept_code', ''),
            valid_start_date=concept.get('valid_start_date'),
            valid_end_date=concept.get('valid_end_date'),
            invalid_reason=concept.get('invalid_reason'),
            concept_embedding=concept.get('concept_embedding'),
            mapping_score=mapping_score,
            mapping_confidence=mapping_confidence,
            mapping_method=mapping_method,
            alternative_concepts=alternative_concepts
        )
    
    def _determine_confidence(self, score: float) -> str:
        """
        ë§¤í•‘ ì‹ ë¢°ë„ ê²°ì • (0.0 ~ 1.0 ì ìˆ˜ ê¸°ì¤€)
        
        ì‹ ë¢°ë„ ê¸°ì¤€:
        - 0.95 ~ 1.00: very_high (ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        - 0.85 ~ 0.94: high (ë†’ì€ ìœ ì‚¬ë„)
        - 0.70 ~ 0.84: medium (ì¤‘ê°„ ìœ ì‚¬ë„)
        - 0.50 ~ 0.69: low (ë‚®ì€ ìœ ì‚¬ë„)
        - 0.00 ~ 0.49: very_low (ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„)
        """
        if score >= 0.95:
            return "very_high"
        elif score >= 0.85:
            return "high"
        elif score >= 0.70:
            return "medium"
        elif score >= 0.50:
            return "low"
        else:
            return "very_low"
    
    def _get_simple_embedding(self, text: str):
        """
        SapBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        """
        try:
            # SapBERT ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not hasattr(self, '_sapbert_model') or self._sapbert_model is None:
                self._initialize_sapbert_model()
            
            if self._sapbert_model is None:
                return None
            
            # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
            text = text.lower().strip()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self._sapbert_tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=25
            )
            inputs = {k: v.to(self._sapbert_device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self._sapbert_model(**inputs)
                # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _initialize_sapbert_model(self):
        """SapBERT ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        try:
            if not HAS_SAPBERT:
                logger.warning("SapBERT ê´€ë ¨ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                self._sapbert_model = None
                self._sapbert_tokenizer = None
                self._sapbert_device = None
                return
            
            model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
            logger.info(f"ğŸ¤– SapBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            
            self._sapbert_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self._sapbert_model = AutoModel.from_pretrained(model_name)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if torch.cuda.is_available():
                self._sapbert_device = torch.device('cuda')
                logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            else:
                self._sapbert_device = torch.device('cpu')
                logger.info("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš©")
            
            self._sapbert_model.to(self._sapbert_device)
            self._sapbert_model.eval()
            
            logger.info(f"âœ… SapBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {self._sapbert_device})")
            
        except Exception as e:
            logger.error(f"âŒ SapBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._sapbert_model = None
            self._sapbert_tokenizer = None
            self._sapbert_device = None
    
    def health_check(self) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸"""
        es_health = self.es_client.health_check()
        
        return {
            "api_status": "healthy",
            "elasticsearch_status": es_health,
            "confidence_threshold": self.confidence_threshold
        }
