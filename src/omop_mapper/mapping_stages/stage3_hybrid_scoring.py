"""
Stage 3: ìµœì¢… Semantic/Lexical ìœ ì‚¬ë„ ê³„ì‚° ë° Hybrid Score ì‚°ì¶œ
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (Lexical Similarity): N-gram ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
- ì˜ë¯¸ì  ìœ ì‚¬ë„ (Semantic Similarity): SapBERT ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ 40% + ì˜ë¯¸ì  ìœ ì‚¬ë„ 60%
"""
from typing import List, Dict, Any, Optional, Tuple
import logging
import numpy as np

try:
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

logger = logging.getLogger(__name__)


class Stage3HybridScoring:
    """Stage 3: Hybrid Score ê³„ì‚° ë° ìµœì¢… ë­í‚¹"""
    
    def __init__(
        self, 
        sapbert_model=None, 
        sapbert_tokenizer=None, 
        sapbert_device=None,
        text_weight: float = 0.4,
        semantic_weight: float = 0.6
    ):
        """
        Args:
            sapbert_model: SapBERT ëª¨ë¸
            sapbert_tokenizer: SapBERT í† í¬ë‚˜ì´ì €
            sapbert_device: SapBERT ë””ë°”ì´ìŠ¤
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.4)
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.6)
        """
        self.sapbert_model = sapbert_model
        self.sapbert_tokenizer = sapbert_tokenizer
        self.sapbert_device = sapbert_device
        self.text_weight = text_weight
        self.semantic_weight = semantic_weight
    
    def calculate_hybrid_scores(
        self, 
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Stage 2 í›„ë³´ë“¤ì— ëŒ€í•´ Hybrid Score ê³„ì‚° ë° ì •ë ¬
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2ì—ì„œ ìˆ˜ì§‘ëœ Standard í›„ë³´ë“¤
            
        Returns:
            List[Dict]: Hybrid Scoreê°€ ê³„ì‚°ë˜ê³  ì •ë ¬ëœ í›„ë³´ë“¤
        """
        logger.info("=" * 80)
        logger.info("Stage 3: Semantic/Lexical ìœ ì‚¬ë„ ê³„ì‚° ë° Hybrid Score ì‚°ì¶œ")
        logger.info("=" * 80)
        
        final_candidates = []
        
        for i, candidate in enumerate(stage2_candidates, 1):
            concept = candidate['concept']
            elasticsearch_score = candidate['elasticsearch_score']
            search_type = candidate.get('search_type', 'unknown')
            is_original_standard = candidate.get('is_original_standard', True)
            
            logger.info(f"\n  [{i}/{len(stage2_candidates)}] {concept.get('concept_name', 'N/A')} "
                      f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
            
            # non-std to std ë³€í™˜ì„ ê±°ì¹œ ê²½ìš° í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¥¼ 0.9ë¡œ ê³ ì •
            if not is_original_standard:
                logger.info(f"     â„¹ï¸  Non-std to Std ë³€í™˜ í›„ë³´ - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ 1.0ìœ¼ë¡œ ê³ ì •")
                text_sim = 1.0
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ë§Œ ê³„ì‚°
                semantic_sim = self._calculate_semantic_similarity(
                    entity_name, 
                    concept
                )
                
                # Hybrid Score ê³„ì‚°
                hybrid_score = (self.text_weight * text_sim) + \
                              (self.semantic_weight * semantic_sim)
                hybrid_score = max(0.0, min(1.0, hybrid_score))
            else:
                # ì›ë˜ Standard í›„ë³´ëŠ” ì •ìƒì ìœ¼ë¡œ Hybrid Score ê³„ì‚°
                hybrid_score, text_sim, semantic_sim = self._calculate_hybrid_score(
                    entity_name,
                    concept.get('concept_name', ''),
                    elasticsearch_score,
                    concept
                )
            
            logger.info(f"     ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_sim:.4f}")
            logger.info(f"     ğŸ“Š ì˜ë¯¸ì  ìœ ì‚¬ë„: {semantic_sim:.4f}")
            logger.info(f"     â­ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.4f}")
            
            final_candidates.append({
                'concept': concept,
                'final_score': hybrid_score,
                'is_original_standard': candidate['is_original_standard'],
                'original_candidate': candidate['original_candidate'],
                'elasticsearch_score': elasticsearch_score,
                'text_similarity': text_sim,
                'semantic_similarity': semantic_sim,
                'search_type': search_type
            })
        
        # Hybrid Score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_candidates = sorted(final_candidates, key=lambda x: x['final_score'], reverse=True)
        
        # ìµœì¢… ìˆœìœ„ ë¡œê¹…
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š Stage 3 ê²°ê³¼ - Hybrid Score ìˆœìœ„:")
        logger.info("=" * 80)
        for i, candidate in enumerate(sorted_candidates[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ë¡œê¹…
            concept = candidate['concept']
            search_type = candidate.get('search_type', 'unknown')
            logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} "
                      f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
            logger.info(f"     ì ìˆ˜: {candidate['final_score']:.4f} "
                      f"(í…ìŠ¤íŠ¸: {candidate['text_similarity']:.4f}, "
                      f"ì˜ë¯¸ì : {candidate['semantic_similarity']:.4f})")
        
        logger.info("=" * 80)
        
        return sorted_candidates
    
    def _calculate_hybrid_score(
        self,
        entity_name: str,
        concept_name: str,
        elasticsearch_score: float,
        concept_source: Dict[str, Any]
    ) -> Tuple[float, float, float]:
        """
        í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ Hybrid Score ê³„ì‚°
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            concept_name: ê°œë… ì´ë¦„
            elasticsearch_score: Elasticsearch ì ìˆ˜ (í˜„ì¬ ë¯¸ì‚¬ìš©)
            concept_source: ê°œë… ì†ŒìŠ¤ ë°ì´í„°
            
        Returns:
            Tuple[float, float, float]: (Hybrid Score, í…ìŠ¤íŠ¸ ìœ ì‚¬ë„, ì˜ë¯¸ì  ìœ ì‚¬ë„)
        """
        try:
            # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)
            text_similarity = self._calculate_text_similarity(entity_name, concept_name)
            
            # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (SapBERT ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            semantic_similarity = self._calculate_semantic_similarity(
                entity_name, 
                concept_source
            )
            
            # 3. Hybrid Score ê³„ì‚°
            hybrid_score = (self.text_weight * text_similarity) + \
                          (self.semantic_weight * semantic_similarity)
            
            # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì œí•œ
            hybrid_score = max(0.0, min(1.0, hybrid_score))
            text_similarity = max(0.0, min(1.0, text_similarity))
            semantic_similarity = max(0.0, min(1.0, semantic_similarity))
            
            return hybrid_score, text_similarity, semantic_similarity
            
        except Exception as e:
            logger.error(f"Hybrid Score ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
            fallback_similarity = self._calculate_text_similarity(entity_name, concept_name)
            return fallback_similarity, fallback_similarity, 0.0
    
    def _calculate_text_similarity(self, entity_name: str, concept_name: str) -> float:
        """
        ë‘ ë¬¸ìì—´ ê°„ì˜ Jaccard ìœ ì‚¬ë„ ê³„ì‚° (N-gram 3 ê¸°ë°˜)
        
        Args:
            entity_name: ì›ë³¸ ì—”í‹°í‹° ì´ë¦„
            concept_name: ë¹„êµí•  ê°œë… ì´ë¦„
            
        Returns:
            float: Jaccard ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not entity_name or not concept_name:
            return 0.0
        
        # ëŒ€ì†Œë¬¸ì ì •ê·œí™”
        entity_name = entity_name.lower()
        concept_name = concept_name.lower()
        
        # N-gram 3ìœ¼ë¡œ ë¶„í• 
        entity_ngrams = self._get_ngrams(entity_name, n=3)
        concept_ngrams = self._get_ngrams(concept_name, n=3)
        
        if not entity_ngrams or not concept_ngrams:
            return 0.0
        
        # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        intersection = entity_ngrams.intersection(concept_ngrams)
        union = entity_ngrams.union(concept_ngrams)
        jaccard_similarity = len(intersection) / len(union) if union else 0.0
        
        return jaccard_similarity
    
    def _get_ngrams(self, text: str, n: int = 3) -> set:
        """
        í…ìŠ¤íŠ¸ë¥¼ N-gramìœ¼ë¡œ ë¶„í• 
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            n: N-gram í¬ê¸° (ê¸°ë³¸ê°’: 3)
            
        Returns:
            set: N-gram ì§‘í•©
        """
        if len(text) < n:
            return {text}
        
        ngrams = set()
        for i in range(len(text) - n + 1):
            ngrams.add(text[i:i + n])
        
        return ngrams
    
    def _calculate_semantic_similarity(
        self, 
        entity_name: str, 
        concept_source: Dict[str, Any]
    ) -> float:
        """
        SapBERT ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            concept_source: ê°œë… ì†ŒìŠ¤ ë°ì´í„°
            
        Returns:
            float: ì˜ë¯¸ì  ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        if not HAS_SKLEARN:
            logger.debug("sklearn ë¯¸ì„¤ì¹˜ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©")
            return 0.0
        
        concept_embedding = concept_source.get('concept_embedding')
        
        if not concept_embedding or len(concept_embedding) != 768:
            logger.debug(f"ê°œë… ì„ë² ë”© ì—†ìŒ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©: "
                        f"{concept_source.get('concept_name', 'N/A')}")
            return 0.0
        
        try:
            # ì—”í‹°í‹° ì„ë² ë”© ìƒì„±
            entity_embedding = self._get_entity_embedding(entity_name)
            
            if entity_embedding is None:
                logger.debug(f"ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©")
                return 0.0
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            concept_emb_array = np.array(concept_embedding).reshape(1, -1)
            entity_emb_array = entity_embedding.reshape(1, -1)
            semantic_similarity = cosine_similarity(entity_emb_array, concept_emb_array)[0][0]
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” -1~1 ë²”ìœ„ì´ë¯€ë¡œ 0~1ë¡œ ì •ê·œí™”
            semantic_similarity = (semantic_similarity + 1.0) / 2.0
            
            logger.debug(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì„±ê³µ: {semantic_similarity:.4f} for "
                        f"{concept_source.get('concept_name', 'N/A')}")
            
            return semantic_similarity
            
        except Exception as e:
            logger.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _get_entity_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        SapBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            Optional[np.ndarray]: ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        try:
            if self.sapbert_model is None or self.sapbert_tokenizer is None:
                return None
            
            import torch
            
            # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
            text = text.lower().strip()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.sapbert_tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=25
            )
            inputs = {k: v.to(self.sapbert_device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self.sapbert_model(**inputs)
                # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

