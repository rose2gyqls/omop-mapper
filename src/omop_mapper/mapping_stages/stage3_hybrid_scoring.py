"""
Stage 3: ìµœì¢… Semantic/Lexical ìœ ì‚¬ë„ ê³„ì‚° ë° Hybrid Score ì‚°ì¶œ
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (Lexical Similarity): N-gram ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
- ì˜ë¯¸ì  ìœ ì‚¬ë„ (Semantic Similarity): SapBERT ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ 40% + ì˜ë¯¸ì  ìœ ì‚¬ë„ 60%
"""
from typing import List, Dict, Any, Optional, Tuple
import logging
import numpy as np

try:
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

logger = logging.getLogger(__name__)


class Stage3HybridScoring:
    """Stage 3: Hybrid Score ê³„ì‚° ë° ìµœì¢… ë­í‚¹"""
    
    def __init__(
        self, 
        sapbert_model=None, 
        sapbert_tokenizer=None, 
        sapbert_device=None,
        text_weight: float = 0.4,
        semantic_weight: float = 0.6,
        es_client=None
    ):
        """
        Args:
            sapbert_model: SapBERT ëª¨ë¸
            sapbert_tokenizer: SapBERT í† í¬ë‚˜ì´ì €
            sapbert_device: SapBERT ë””ë°”ì´ìŠ¤
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.4)
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.6)
        """
        self.sapbert_model = sapbert_model
        self.sapbert_tokenizer = sapbert_tokenizer
        self.sapbert_device = sapbert_device
        self.text_weight = text_weight
        self.semantic_weight = semantic_weight
        self.es_client = es_client
    
    def calculate_hybrid_scores(
        self, 
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Stage 2 í›„ë³´ë“¤ì— ëŒ€í•´ Hybrid Score ê³„ì‚° ë° ì •ë ¬
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2ì—ì„œ ìˆ˜ì§‘ëœ Standard í›„ë³´ë“¤
            
        Returns:
            List[Dict]: Hybrid Scoreê°€ ê³„ì‚°ë˜ê³  ì •ë ¬ëœ í›„ë³´ë“¤
        """
        logger.info("=" * 80)
        logger.info("Stage 3: Semantic/Lexical ìœ ì‚¬ë„ ê³„ì‚° ë° Hybrid Score ì‚°ì¶œ")
        logger.info("=" * 80)
        
        final_candidates = []
        
        # í›„ë³´ë“¤ì˜ concept_idë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë™ì˜ì–´ ì„ë² ë”©ì„ ì¼ê´„ ì¡°íšŒ
        synonyms_with_embeddings_map = {}
        try:
            if self.es_client:
                concept_ids = []
                for c in stage2_candidates:
                    cid = str(c['concept'].get('concept_id', ''))
                    if cid:
                        concept_ids.append(cid)
                concept_ids = list(dict.fromkeys(concept_ids))
                if hasattr(self.es_client, 'search_synonyms_with_embeddings_bulk'):
                    synonyms_with_embeddings_map = self.es_client.search_synonyms_with_embeddings_bulk(concept_ids)
        except Exception as e:
            logger.warning(f"ë™ì˜ì–´ ì„ë² ë”© ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        for i, candidate in enumerate(stage2_candidates, 1):
            concept = candidate['concept']
            elasticsearch_score = candidate['elasticsearch_score']
            search_type = candidate.get('search_type', 'unknown')
            is_original_standard = candidate.get('is_original_standard', True)
            
            logger.info(f"\n  [{i}/{len(stage2_candidates)}] {concept.get('concept_name', 'N/A')} "
                      f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
            
            # non-std to std ë³€í™˜ì„ ê±°ì¹œ ê²½ìš° í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            if not is_original_standard:
                text_sim = self._calculate_text_similarity(
                    entity_name,
                    concept.get('concept_name', '')
                )
                
                # Non-standard â†’ Standard ë³€í™˜ì´ë¯€ë¡œ ê¸°ë³¸ ê°€ì  ì¶”ê°€
                text_sim = max(text_sim, 0.9)  # ìµœì†Œ 0.9 ë³´ì¥
                logger.info(f"     â„¹ï¸  Non-std to Std ë³€í™˜ í›„ë³´ - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_sim:.4f}")
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ë™ì˜ì–´ ê°€ì  í¬í•¨)
                concept_id = str(concept.get('concept_id', ''))
                synonyms_with_embeddings = synonyms_with_embeddings_map.get(concept_id, [])
                semantic_sim, synonym_bonus = self._calculate_semantic_similarity_with_synonyms(
                    entity_name, 
                    concept,
                    synonyms_with_embeddings
                )
                
                if synonym_bonus > 0:
                    logger.info(f"     ğŸ¯ ë™ì˜ì–´ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì : +{synonym_bonus:.4f}")
                
                # Hybrid Score ê³„ì‚°
                hybrid_score = (self.text_weight * text_sim) + \
                              (self.semantic_weight * semantic_sim)
                hybrid_score = max(0.0, min(1.0, hybrid_score))
            else:
                # ì›ë˜ Standard í›„ë³´ëŠ” ì •ìƒì ìœ¼ë¡œ Hybrid Score ê³„ì‚°
                concept_id = str(concept.get('concept_id', ''))
                synonyms_with_embeddings = synonyms_with_embeddings_map.get(concept_id, [])
                hybrid_score, text_sim, semantic_sim, synonym_bonus = self._calculate_hybrid_score(
                    entity_name,
                    concept.get('concept_name', ''),
                    elasticsearch_score,
                    concept,
                    synonyms_with_embeddings
                )
                
                # ë™ì˜ì–´ ê°€ì ì´ ìˆìœ¼ë©´ ë¡œê¹…
                if synonym_bonus > 0:
                    logger.info(f"     ğŸ¯ ë™ì˜ì–´ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì : +{synonym_bonus:.4f}")
            
            logger.info(f"     ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_sim:.4f}")
            logger.info(f"     ğŸ“Š ì˜ë¯¸ì  ìœ ì‚¬ë„: {semantic_sim:.4f}")
            logger.info(f"     â­ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.4f}")
            
            final_candidates.append({
                'concept': concept,
                'final_score': hybrid_score,
                'is_original_standard': candidate['is_original_standard'],
                'original_candidate': candidate['original_candidate'],
                'elasticsearch_score': elasticsearch_score,
                'text_similarity': text_sim,
                'semantic_similarity': semantic_sim,
                'search_type': search_type
            })
        
        # Hybrid Score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_candidates = sorted(final_candidates, key=lambda x: x['final_score'], reverse=True)
        
        # ìµœì¢… ìˆœìœ„ ë¡œê¹…
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š Stage 3 ê²°ê³¼ - Hybrid Score ìˆœìœ„:")
        logger.info("=" * 80)
        for i, candidate in enumerate(sorted_candidates[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ë¡œê¹…
            concept = candidate['concept']
            search_type = candidate.get('search_type', 'unknown')
            logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} "
                      f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
            logger.info(f"     ì ìˆ˜: {candidate['final_score']:.4f} "
                      f"(í…ìŠ¤íŠ¸: {candidate['text_similarity']:.4f}, "
                      f"ì˜ë¯¸ì : {candidate['semantic_similarity']:.4f})")
        
        logger.info("=" * 80)
        
        return sorted_candidates
    
    def _calculate_hybrid_score(
        self,
        entity_name: str,
        concept_name: str,
        elasticsearch_score: float,
        concept_source: Dict[str, Any],
        synonyms_with_embeddings: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[float, float, float, float]:
        """
        í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ì™€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ Hybrid Score ê³„ì‚°
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            concept_name: ê°œë… ì´ë¦„
            elasticsearch_score: Elasticsearch ì ìˆ˜ (í˜„ì¬ ë¯¸ì‚¬ìš©)
            concept_source: ê°œë… ì†ŒìŠ¤ ë°ì´í„°
            synonyms_with_embeddings: ê°œë…ì˜ ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸ (ì„ë² ë”© í¬í•¨, ì„ íƒì‚¬í•­)
            
        Returns:
            Tuple[float, float, float, float]: (Hybrid Score, í…ìŠ¤íŠ¸ ìœ ì‚¬ë„, ì˜ë¯¸ì  ìœ ì‚¬ë„, ë™ì˜ì–´ ê°€ì )
        """
        try:
            # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (concept_nameë§Œ ì‚¬ìš©)
            text_similarity = self._calculate_text_similarity(entity_name, concept_name)
            
            # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (SapBERT ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ë™ì˜ì–´ ê°€ì  í¬í•¨)
            semantic_similarity, synonym_bonus = self._calculate_semantic_similarity_with_synonyms(
                entity_name, 
                concept_source,
                synonyms_with_embeddings
            )
            
            # 3. Hybrid Score ê³„ì‚° (ë™ì˜ì–´ ê°€ì ì€ ì˜ë¯¸ì  ìœ ì‚¬ë„ì— ë°˜ì˜ëœ ìƒíƒœ)
            hybrid_score = (self.text_weight * text_similarity) + \
                          (self.semantic_weight * semantic_similarity)
            
            # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì œí•œ
            hybrid_score = max(0.0, min(1.0, hybrid_score))
            text_similarity = max(0.0, min(1.0, text_similarity))
            semantic_similarity = max(0.0, min(1.0, semantic_similarity))
            
            return hybrid_score, text_similarity, semantic_similarity, synonym_bonus
            
        except Exception as e:
            logger.error(f"Hybrid Score ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
            fallback_similarity = self._calculate_text_similarity(entity_name, concept_name)
            return fallback_similarity, fallback_similarity, 0.0, 0.0
    
    def _normalize_text_for_similarity(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ì •ê·œí™”
        - í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        - ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        - ì•ë’¤ ê³µë°± ì œê±°
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            str: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        import re
        # í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = text.replace('-', ' ').replace('_', ' ')
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
        text = re.sub(r'\s+', ' ', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        return text
    
    def _calculate_text_similarity(self, entity_name: str, concept_name: str) -> float:
        """
        ë‘ ë¬¸ìì—´ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (N-gram Jaccard + ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ)
        íŠ¹ìˆ˜ë¬¸ì(í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ ë“±) ì •ê·œí™” í›„ ê³„ì‚°
        
        Args:
            entity_name: ì›ë³¸ ì—”í‹°í‹° ì´ë¦„
            concept_name: ë¹„êµí•  ê°œë… ì´ë¦„
            
        Returns:
            float: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not entity_name or not concept_name:
            return 0.0
        
        # ëŒ€ì†Œë¬¸ì ì •ê·œí™”
        entity_name_lower = entity_name.lower()
        concept_name_lower = concept_name.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™” (í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ)
        entity_name_normalized = self._normalize_text_for_similarity(entity_name_lower)
        concept_name_normalized = self._normalize_text_for_similarity(concept_name_lower)
        
        # 1. N-gram 3 ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
        entity_ngrams = self._get_ngrams(entity_name_normalized, n=3)
        concept_ngrams = self._get_ngrams(concept_name_normalized, n=3)
        
        if not entity_ngrams or not concept_ngrams:
            return 0.0
        
        intersection = entity_ngrams.intersection(concept_ngrams)
        union = entity_ngrams.union(concept_ngrams)
        ngram_similarity = len(intersection) / len(union) if union else 0.0
        
        # 2. ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
        entity_words = set(entity_name_normalized.split())
        concept_words = set(concept_name_normalized.split())
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´
        exact_match_words = entity_words.intersection(concept_words)
        word_intersection = len(exact_match_words)
        word_union = len(entity_words.union(concept_words))
        word_jaccard = word_intersection / word_union if word_union else 0.0
        
        # 3. ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨ ë³´ë„ˆìŠ¤ (ì˜ˆ: "cardiovascular" vs "vascular")
        partial_match_bonus = 0.0
        for entity_word in entity_words:
            for concept_word in concept_words:
                # í•œ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì— í¬í•¨ë˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ê²½ìš°
                if entity_word in concept_word or concept_word in entity_word:
                    if entity_word != concept_word:  # ì •í™•íˆ ì¼ì¹˜í•˜ë©´ ì´ë¯¸ word_jaccardì— í¬í•¨ë¨
                        partial_match_bonus += 0.1
                # ë§¤ìš° ìœ ì‚¬í•œ ë‹¨ì–´ (í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜, ì˜ˆ: "atherosclerotic" vs "arteriosclerotic")
                elif self._are_words_similar(entity_word, concept_word):
                    partial_match_bonus += 0.15
        
        # ë¶€ë¶„ ì¼ì¹˜ ë³´ë„ˆìŠ¤ëŠ” ìµœëŒ€ 0.3ìœ¼ë¡œ ì œí•œ
        partial_match_bonus = min(0.3, partial_match_bonus)
        
        # 4. ìµœì¢… ìœ ì‚¬ë„ = N-gram ìœ ì‚¬ë„(60%) + ë‹¨ì–´ Jaccard(30%) + ë¶€ë¶„ ì¼ì¹˜ ë³´ë„ˆìŠ¤(ìµœëŒ€ 10%)
        final_similarity = (0.6 * ngram_similarity) + (0.3 * word_jaccard) + min(0.1, partial_match_bonus)
        
        # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
        return min(1.0, final_similarity)
    
    def _are_words_similar(self, word1: str, word2: str, threshold: float = 0.8) -> bool:
        """
        ë‘ ë‹¨ì–´ê°€ ìœ ì‚¬í•œì§€ íŒë‹¨ (í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜)
        
        Args:
            word1: ì²« ë²ˆì§¸ ë‹¨ì–´
            word2: ë‘ ë²ˆì§¸ ë‹¨ì–´
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.8)
            
        Returns:
            bool: ìœ ì‚¬í•˜ë©´ True
        """
        if not word1 or not word2:
            return False
        
        # ê¸¸ì´ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ
        if abs(len(word1) - len(word2)) > max(len(word1), len(word2)) * 0.3:
            return False
        
        # N-gram ìœ ì‚¬ë„ë¡œ íŒë‹¨
        w1_ngrams = self._get_ngrams(word1, n=3)
        w2_ngrams = self._get_ngrams(word2, n=3)
        
        if not w1_ngrams or not w2_ngrams:
            return False
        
        intersection = w1_ngrams.intersection(w2_ngrams)
        union = w1_ngrams.union(w2_ngrams)
        similarity = len(intersection) / len(union) if union else 0.0
        
        return similarity >= threshold
    
    def _get_ngrams(self, text: str, n: int = 3) -> set:
        """
        í…ìŠ¤íŠ¸ë¥¼ N-gramìœ¼ë¡œ ë¶„í• 
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            n: N-gram í¬ê¸° (ê¸°ë³¸ê°’: 3)
            
        Returns:
            set: N-gram ì§‘í•©
        """
        if len(text) < n:
            return {text}
        
        ngrams = set()
        for i in range(len(text) - n + 1):
            ngrams.add(text[i:i + n])
        
        return ngrams
    
    def _calculate_semantic_similarity_with_synonyms(
        self, 
        entity_name: str, 
        concept_source: Dict[str, Any],
        synonyms_with_embeddings: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[float, float]:
        """
        SapBERT ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ë™ì˜ì–´ ì„ë² ë”© í¬í•¨)
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            concept_source: ê°œë… ì†ŒìŠ¤ ë°ì´í„°
            synonyms_with_embeddings: ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸ (ì„ë² ë”© í¬í•¨, ì„ íƒì‚¬í•­)
                ê° í•­ëª©ì€ {'name': str, 'embedding': List[float]} í˜•íƒœ
            
        Returns:
            Tuple[float, float]: (ì˜ë¯¸ì  ìœ ì‚¬ë„, ë™ì˜ì–´ ê°€ì )
        """
        if not HAS_SKLEARN:
            logger.debug("sklearn ë¯¸ì„¤ì¹˜ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©")
            return 0.0, 0.0
        
        concept_embedding = concept_source.get('concept_embedding')
        
        if not concept_embedding or len(concept_embedding) != 768:
            logger.debug(f"ê°œë… ì„ë² ë”© ì—†ìŒ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©: "
                        f"{concept_source.get('concept_name', 'N/A')}")
            return 0.0, 0.0
        
        try:
            # ì—”í‹°í‹° ì„ë² ë”© ìƒì„±
            entity_embedding = self._get_entity_embedding(entity_name)
            
            if entity_embedding is None:
                logger.debug(f"ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ - ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.0 ì‚¬ìš©")
                return 0.0, 0.0
            
            # concept_nameê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            concept_emb_array = np.array(concept_embedding).reshape(1, -1)
            entity_emb_array = entity_embedding.reshape(1, -1)
            base_semantic_similarity_raw = cosine_similarity(entity_emb_array, concept_emb_array)[0][0]
            
            # SapBERT ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë³´í†µ 0.5~1.0 ë²”ìœ„ì´ë¯€ë¡œ ì •ê·œí™” ì—†ì´ ì‚¬ìš©
            # ìŒìˆ˜ ê°’ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 0.0 ì´í•˜ëŠ” 0.0ìœ¼ë¡œ ì œí•œ
            base_semantic_similarity = max(0.0, base_semantic_similarity_raw)
            
            # ë””ë²„ê¹…ìš© ë¡œê¹… (debug ë ˆë²¨ë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
            logger.debug(f"     ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì›ë³¸): {base_semantic_similarity_raw:.4f} â†’ "
                       f"ì‚¬ìš©ê°’: {base_semantic_similarity:.4f} "
                       f"(entity: '{entity_name}' vs concept: '{concept_source.get('concept_name', 'N/A')}')")
            
            # ë™ì˜ì–´ ì„ë² ë”©ê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì )
            synonym_bonus = 0.0
            best_synonym_sim = 0.0
            best_synonym_name = None
            
            if synonyms_with_embeddings:
                for syn_entry in synonyms_with_embeddings[:50]:  # ìµœëŒ€ 50ê°œ ë™ì˜ì–´ë§Œ í™•ì¸
                    syn_name = syn_entry.get('name', '')
                    syn_embedding = syn_entry.get('embedding')
                    
                    if not syn_embedding or len(syn_embedding) != 768:
                        continue
                    
                    # ë™ì˜ì–´ ì„ë² ë”©ê³¼ ì—”í‹°í‹° ì„ë² ë”©ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    syn_emb_array = np.array(syn_embedding).reshape(1, -1)
                    syn_sim_raw = cosine_similarity(entity_emb_array, syn_emb_array)[0][0]
                    syn_sim = max(0.0, syn_sim_raw)  # ì •ê·œí™” ì—†ì´ ì‚¬ìš©, ìŒìˆ˜ëŠ” 0.0ìœ¼ë¡œ ì œí•œ
                    
                    if syn_sim > best_synonym_sim:
                        best_synonym_sim = syn_sim
                        best_synonym_name = syn_name
                
                # ë™ì˜ì–´ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê°€ì  ë¶€ì—¬
                # SapBERT ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì¬ì¡°ì •:
                # - ìœ ì‚¬ë„ 0.9 ì´ìƒ: +0.05 (ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„)
                # - ìœ ì‚¬ë„ 0.8 ì´ìƒ: +0.03 (ë†’ì€ ìœ ì‚¬ë„)
                # - ìœ ì‚¬ë„ 0.7 ì´ìƒ: +0.01 (ì¤‘ê°„ ìœ ì‚¬ë„)
                if best_synonym_sim >= 0.9:
                    synonym_bonus = 0.05
                elif best_synonym_sim >= 0.8:
                    synonym_bonus = 0.03
                elif best_synonym_sim >= 0.7:
                    synonym_bonus = 0.01
                
                # ë™ì˜ì–´ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ë¡œê¹…
                if synonym_bonus > 0:
                    logger.debug(f"     ë™ì˜ì–´ ì˜ë¯¸ì  ë§¤ì¹­: '{best_synonym_name}' "
                               f"(ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {best_synonym_sim:.4f}, ê°€ì : +{synonym_bonus:.4f})")
            
            # ìµœì¢… ì˜ë¯¸ì  ìœ ì‚¬ë„ = ê¸°ë³¸ ìœ ì‚¬ë„ + ë™ì˜ì–´ ê°€ì  (ìµœëŒ€ 1.0)
            final_semantic_similarity = min(1.0, base_semantic_similarity + synonym_bonus)
            
            logger.debug(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°: {final_semantic_similarity:.4f} "
                        f"(ê¸°ë³¸: {base_semantic_similarity:.4f}, ê°€ì : +{synonym_bonus:.4f}) "
                        f"for {concept_source.get('concept_name', 'N/A')}")
            
            return final_semantic_similarity, synonym_bonus
            
        except Exception as e:
            logger.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0, 0.0
    
    def _get_entity_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        SapBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            Optional[np.ndarray]: ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        try:
            if self.sapbert_model is None or self.sapbert_tokenizer is None:
                return None
            
            import torch
            
            # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
            text = text.lower().strip()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.sapbert_tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=25
            )
            inputs = {k: v.to(self.sapbert_device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self.sapbert_model(**inputs)
                # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

