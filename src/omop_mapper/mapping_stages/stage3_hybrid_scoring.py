"""
Stage 3: LLM ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹
- OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´êµ° í‰ê°€
- LLM ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ìˆœìœ„ ê²°ì •
"""
from typing import List, Dict, Any, Optional
import logging
import os
import json
from dotenv import load_dotenv

try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    logger = logging.getLogger(__name__)
    logger.warning("openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if 'logger' not in locals():
    logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class Stage3HybridScoring:
    """Stage 3: LLM ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹"""
    
    def __init__(
        self, 
        sapbert_model=None, 
        sapbert_tokenizer=None, 
        sapbert_device=None,
        text_weight: float = 0.4,
        semantic_weight: float = 0.6,
        es_client=None,
        openai_api_key: Optional[str] = None,
        openai_model: str = "gpt-4o-mini"
    ):
        """
        Args:
            sapbert_model: SapBERT ëª¨ë¸ (ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            sapbert_tokenizer: SapBERT í† í¬ë‚˜ì´ì € (ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            sapbert_device: SapBERT ë””ë°”ì´ìŠ¤ (ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            es_client: Elasticsearch í´ë¼ì´ì–¸íŠ¸
            openai_api_key: OpenAI API í‚¤ (Noneì´ë©´ .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
            openai_model: OpenAI ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gpt-4o-mini)
        """
        self.es_client = es_client
        
        # OpenAI API ì´ˆê¸°í™”
        self.openai_client = None
        self.openai_model = openai_model
        
        if not HAS_OPENAI:
            logger.error("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                logger.info(f"âœ… OpenAI API ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {openai_model})")
            else:
                logger.error("âš ï¸ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âš ï¸ OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def calculate_hybrid_scores(
        self, 
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]],
        stage1_candidates: Optional[List[Dict[str, Any]]] = None
    ) -> List[Dict[str, Any]]:
        """
        Stage 2 í›„ë³´ë“¤ì— ëŒ€í•´ LLM ê¸°ë°˜ í‰ê°€ ë° ìµœì¢… ë­í‚¹
        
        **í‰ê°€ ë°©ì‹**:
        - OpenAI GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° í›„ë³´ì˜ ì˜ë¯¸ì  ì í•©ì„± í‰ê°€
        - ê° í›„ë³´ì— 0.0~1.0 ì ìˆ˜ ë¶€ì—¬
        - í•˜ìœ„ ê°œë…(sub-concept)ìœ¼ë¡œ ë§¤í•‘ë˜ë©´ ë‚®ì€ ì ìˆ˜ ë¶€ì—¬
        - ìµœì¢… ì ìˆ˜(final_score)ëŠ” LLM ì ìˆ˜(llm_score)ë¥¼ ì‚¬ìš©
        
        Args:
            entity_name: í‰ê°€í•  ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2ì—ì„œ ìˆ˜ì§‘ëœ Standard í›„ë³´ë“¤
            stage1_candidates: Stage 1 í›„ë³´ë“¤ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€)
            
        Returns:
            List[Dict]: LLM ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤ (ë‚´ë¦¼ì°¨ìˆœ)
        """
        logger.info("=" * 80)
        logger.info("Stage 3: LLM ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹")
        logger.info("=" * 80)
        
        if not self.openai_client:
            logger.error("âš ï¸ OpenAI API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        if not stage2_candidates:
            logger.warning("âš ï¸ í‰ê°€í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # í›„ë³´êµ° ì •ë³´ ì¤€ë¹„
        final_candidates = []
        for candidate in stage2_candidates:
            concept = candidate['concept']
            final_candidates.append({
                'concept': concept,
                'is_original_standard': candidate.get('is_original_standard', True),
                'original_candidate': candidate.get('original_candidate', {}),
                'elasticsearch_score': candidate.get('elasticsearch_score', 0.0),
                'search_type': candidate.get('search_type', 'unknown')
            })
        
        # LLM ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
        try:
            llm_result = self._calculate_llm_scores(entity_name, final_candidates)
            
            if not llm_result:
                logger.error("âš ï¸ LLM í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # LLM ì ìˆ˜ë¥¼ ê° í›„ë³´ì— ì¶”ê°€
            for candidate in final_candidates:
                concept_id = str(candidate['concept'].get('concept_id', ''))
                if concept_id in llm_result:
                    candidate['llm_score'] = llm_result[concept_id]['score']
                    candidate['llm_rank'] = llm_result[concept_id]['rank']
                    candidate['llm_reasoning'] = llm_result[concept_id].get('reasoning', '')
                    # final_scoreë¥¼ llm_scoreë¡œ ì„¤ì • (ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©)
                    candidate['final_score'] = candidate['llm_score']
                else:
                    # LLM í‰ê°€ì—ì„œ ëˆ„ë½ëœ ê²½ìš° ì ìˆ˜ 0.0
                    candidate['llm_score'] = 0.0
                    candidate['llm_rank'] = 999
                    candidate['llm_reasoning'] = 'LLM í‰ê°€ì—ì„œ ëˆ„ë½ë¨'
                    candidate['final_score'] = 0.0
            
            # LLM ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_candidates = sorted(
                final_candidates, 
                key=lambda x: x.get('llm_score', 0.0), 
                reverse=True
            )
            
            # ìµœì¢… ìˆœìœ„ ë¡œê¹…
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ¤– Stage 3 LLM ê²°ê³¼ - OpenAI ìˆœìœ„:")
            logger.info("=" * 80)
            for i, candidate in enumerate(sorted_candidates[:10], 1):
                concept = candidate['concept']
                search_type = candidate.get('search_type', 'unknown')
                llm_score = candidate.get('llm_score', 0.0)
                llm_rank = candidate.get('llm_rank', 'N/A')
                logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} "
                          f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
                logger.info(f"     LLM ì ìˆ˜: {llm_score:.4f} (ìˆœìœ„: {llm_rank})")
                if candidate.get('llm_reasoning'):
                    reasoning = candidate['llm_reasoning'][:100]
                    logger.info(f"     ì´ìœ : {reasoning}...")
            logger.info("=" * 80)
            
            return sorted_candidates
            
        except Exception as e:
            logger.error(f"âš ï¸ LLM í‰ê°€ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_llm_scores(
        self, 
        entity_name: str, 
        candidates: List[Dict[str, Any]],
        max_candidates: int = 15
    ) -> Optional[Dict[str, Dict[str, Any]]]:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´êµ° í‰ê°€
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            candidates: í‰ê°€í•  í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            max_candidates: í‰ê°€í•  ìµœëŒ€ í›„ë³´êµ° ìˆ˜ (ê¸°ë³¸ê°’: 15)
            
        Returns:
            Dict[str, Dict[str, Any]]: concept_idë¥¼ í‚¤ë¡œ í•˜ëŠ” í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.openai_client or not candidates:
            return None
        
        # ìƒìœ„ í›„ë³´ë§Œ í‰ê°€ (ì„±ëŠ¥ìƒ ì´ìœ )
        top_candidates = candidates[:max_candidates]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_llm_prompt(entity_name, top_candidates)
        
        try:
            # OpenAI API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì˜ë£Œ ìš©ì–´ ë§¤í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì—”í‹°í‹°ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ OMOP CDM ê°œë…ì„ ì„ íƒí•˜ê³  ê° í›„ë³´ì— ëŒ€í•´ ì •í™•í•œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2048,
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            result = self._parse_llm_response(response_text, top_candidates)
            return result
            
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_llm_prompt(self, entity_name: str, candidates: List[Dict[str, Any]]) -> str:
        """
        LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            candidates: í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        candidates_info = []
        for i, candidate in enumerate(candidates, 1):
            concept = candidate['concept']
            candidates_info.append({
                'concept_id': str(concept.get('concept_id', '')),
                'concept_name': concept.get('concept_name', ''),
                'domain_id': concept.get('domain_id', '')
            })
        
        prompt = f"""ë‹¤ìŒ ì—”í‹°í‹°ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ OMOP CDM ê°œë…ì„ ì„ íƒí•˜ê³  ê° í›„ë³´ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

**ì—”í‹°í‹° ì´ë¦„**: {entity_name}

**í›„ë³´ ê°œë…ë“¤**:
{json.dumps(candidates_info, ensure_ascii=False, indent=2)}

**ì§€ì‹œì‚¬í•­**:
1. ê° í›„ë³´ ê°œë…ì´ ì—”í‹°í‹° ì´ë¦„ê³¼ ì–¼ë§ˆë‚˜ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
2. ì˜ë£Œ ìš©ì–´ì˜ ì˜ë¯¸, ì»¨í…ìŠ¤íŠ¸, ë„ë©”ì¸ ì í•©ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”.
3. **ì¤‘ìš”**: ë¬´ì¡°ê±´ ê°™ì€ ë ˆë²¨ì´ê±°ë‚˜ ìƒìœ„ ë ˆë²¨ì˜ ê°œë…ìœ¼ë¡œë§Œ ë§¤í•‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í•˜ìœ„ ê°œë…(sub-concept)ìœ¼ë¡œëŠ” ë§¤í•‘ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
4. ê° í›„ë³´ì— ëŒ€í•´ 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš” (1.0ì´ ê°€ì¥ ì í•©í•¨).
5. ì„ íƒ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš” (í•œêµ­ì–´ë¡œ). íŠ¹íˆ í•˜ìœ„ ê°œë…ì¸ ê²½ìš° ì´ë¥¼ ëª…í™•íˆ ì§€ì í•˜ê³  ì ìˆ˜ë¥¼ ë‚®ê²Œ ë¶€ì—¬í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹** (JSON):
{{
  "results": [
    {{
      "concept_id": "í›„ë³´ ê°œë… ID",
      "score": 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜,
      "rank": 1~{len(candidates)} ì‚¬ì´ì˜ ìˆœìœ„,
      "reasoning": "ì„ íƒ ì´ìœ  (í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ)"
    }},
    ...
  ]
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        return prompt
    
    def _parse_llm_response(self, response_text: str, candidates: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        LLM ì‘ë‹µ íŒŒì‹±
        
        Args:
            response_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            candidates: í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Dict[str, Any]]: concept_idë¥¼ í‚¤ë¡œ í•˜ëŠ” í‰ê°€ ê²°ê³¼
        """
        try:
            # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            text = response_text.strip()
            if '```json' in text:
                text = text.split('```json')[1].split('```')[0].strip()
            elif '```' in text:
                text = text.split('```')[1].split('```')[0].strip()
            
            # JSON íŒŒì‹±
            parsed = json.loads(text)
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result = {}
            if 'results' in parsed:
                for item in parsed['results']:
                    concept_id = str(item.get('concept_id', ''))
                    if concept_id:
                        result[concept_id] = {
                            'score': float(item.get('score', 0.0)),
                            'rank': int(item.get('rank', 999)),
                            'reasoning': item.get('reasoning', '')
                        }
            
            # ëª¨ë“  í›„ë³´ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì—†ìœ¼ë©´ ì ìˆ˜ 0.0ìœ¼ë¡œ ì¶”ê°€)
            for candidate in candidates:
                concept_id = str(candidate['concept'].get('concept_id', ''))
                if concept_id not in result:
                    result[concept_id] = {
                        'score': 0.0,
                        'rank': 999,
                        'reasoning': 'LLM í‰ê°€ì—ì„œ ëˆ„ë½ë¨'
                    }
            
            return result
            
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:500]}")
            return {}
