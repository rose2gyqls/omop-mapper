"""
Stage 3: Hybrid ë˜ëŠ” LLM ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹
- Hybrid: Text ìœ ì‚¬ë„(Jaccard) + Semantic ìœ ì‚¬ë„(SapBERT Cosine) ì¡°í•©
- LLM: OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´êµ° í‰ê°€
"""
from typing import List, Dict, Any, Optional
import logging
import os
import json
from dotenv import load_dotenv
import logging

logger = logging.getLogger(__name__)

try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# Hybrid ëª¨ë“œìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import numpy as np
    import torch
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_HYBRID_LIBS = True
except ImportError:
    HAS_HYBRID_LIBS = False
    np = None

    logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class Stage3HybridScoring:
    """Stage 3: Hybrid ë˜ëŠ” LLM ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹"""
    
    def __init__(
        self, 
        sapbert_model=None, 
        sapbert_tokenizer=None, 
        sapbert_device=None,
        text_weight: float = 0.4,
        semantic_weight: float = 0.6,
        es_client=None,
        openai_api_key: Optional[str] = None,
        openai_model: str = "gpt-4o-mini",
        scoring_mode: str = "llm"
    ):
        """
        Args:
            sapbert_model: SapBERT ëª¨ë¸ (hybrid ëª¨ë“œì—ì„œ ì‚¬ìš©)
            sapbert_tokenizer: SapBERT í† í¬ë‚˜ì´ì € (hybrid ëª¨ë“œì—ì„œ ì‚¬ìš©)
            sapbert_device: SapBERT ë””ë°”ì´ìŠ¤ (hybrid ëª¨ë“œì—ì„œ ì‚¬ìš©)
            text_weight: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (hybrid ëª¨ë“œ, ê¸°ë³¸ê°’: 0.4)
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (hybrid ëª¨ë“œ, ê¸°ë³¸ê°’: 0.6)
            es_client: Elasticsearch í´ë¼ì´ì–¸íŠ¸
            openai_api_key: OpenAI API í‚¤ (llm ëª¨ë“œ, Noneì´ë©´ .env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
            openai_model: OpenAI ëª¨ë¸ëª… (llm ëª¨ë“œ, ê¸°ë³¸ê°’: gpt-4o-mini)
            scoring_mode: ì ìˆ˜ ê³„ì‚° ë°©ì‹ ('llm' ë˜ëŠ” 'hybrid', ê¸°ë³¸ê°’: 'llm')
        """
        self.es_client = es_client
        self.scoring_mode = scoring_mode.lower()
        
        # Hybrid ëª¨ë“œ ì„¤ì •
        self.sapbert_model = sapbert_model
        self.sapbert_tokenizer = sapbert_tokenizer
        self.sapbert_device = sapbert_device
        self.text_weight = text_weight
        self.semantic_weight = semantic_weight
        
        # OpenAI API ì´ˆê¸°í™” (LLM ëª¨ë“œ)
        self.openai_client = None
        self.openai_model = openai_model
        
        if self.scoring_mode == "llm":
            if not HAS_OPENAI:
                logger.error("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            try:
                api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
                if api_key:
                    self.openai_client = OpenAI(api_key=api_key)
                    logger.info(f"âœ… OpenAI API ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {openai_model})")
                else:
                    logger.error("âš ï¸ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"âš ï¸ OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        elif self.scoring_mode == "hybrid":
            if not HAS_HYBRID_LIBS:
                logger.error("âš ï¸ Hybrid ëª¨ë“œì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (numpy, torch, sklearn).")
            elif sapbert_model is None or sapbert_tokenizer is None:
                logger.warning("âš ï¸ SapBERT ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Hybrid ëª¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"âœ… Hybrid ì ìˆ˜ ê³„ì‚° ëª¨ë“œ ì´ˆê¸°í™” (text: {text_weight}, semantic: {semantic_weight})")
        else:
            logger.error(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” scoring_mode: {scoring_mode}. 'llm' ë˜ëŠ” 'hybrid'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    def calculate_hybrid_scores(
        self, 
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]],
        stage1_candidates: Optional[List[Dict[str, Any]]] = None,
        entity_embedding: Optional[Any] = None
    ) -> List[Dict[str, Any]]:
        """
        Stage 2 í›„ë³´ë“¤ì— ëŒ€í•´ Hybrid ë˜ëŠ” LLM ê¸°ë°˜ í‰ê°€ ë° ìµœì¢… ë­í‚¹
        
        **Hybrid í‰ê°€ ë°©ì‹** (scoring_mode='hybrid'):
        - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (0.4): Jaccard ìœ ì‚¬ë„ (n-gram=3)
          - Non-std to std ë³€í™˜ëœ í›„ë³´ëŠ” ê³ ì • 0.9 ì ìˆ˜
        - ì˜ë¯¸ì  ìœ ì‚¬ë„ (0.6): SapBERT ì„ë² ë”© + Cosine ìœ ì‚¬ë„
        - ìµœì¢… ì ìˆ˜ = 0.4 * text_similarity + 0.6 * semantic_similarity
        
        **LLM í‰ê°€ ë°©ì‹** (scoring_mode='llm'):
        - OpenAI GPT-4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° í›„ë³´ì˜ ì˜ë¯¸ì  ì í•©ì„± í‰ê°€
        - ê° í›„ë³´ì— 0.0~1.0 ì ìˆ˜ ë¶€ì—¬
        - í•˜ìœ„ ê°œë…(sub-concept)ìœ¼ë¡œ ë§¤í•‘ë˜ë©´ ë‚®ì€ ì ìˆ˜ ë¶€ì—¬
        - ìµœì¢… ì ìˆ˜(final_score)ëŠ” LLM ì ìˆ˜(llm_score)ë¥¼ ì‚¬ìš©
        
        Args:
            entity_name: í‰ê°€í•  ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2ì—ì„œ ìˆ˜ì§‘ëœ Standard í›„ë³´ë“¤
            stage1_candidates: Stage 1 í›„ë³´ë“¤ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€)
            entity_embedding: ì—”í‹°í‹°ì˜ SapBERT ì„ë² ë”© (hybrid ëª¨ë“œì—ì„œ ì‚¬ìš©)
            
        Returns:
            List[Dict]: ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤ (ë‚´ë¦¼ì°¨ìˆœ)
        """
        logger.info("=" * 80)
        logger.info(f"Stage 3: {'Hybrid' if self.scoring_mode == 'hybrid' else 'LLM'} ê¸°ë°˜ í›„ë³´êµ° í‰ê°€ ë° ìµœì¢… ë­í‚¹")
        logger.info("=" * 80)
        
        if not stage2_candidates:
            logger.warning("âš ï¸ í‰ê°€í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # Scoring modeì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ ì ìš©
        if self.scoring_mode == "hybrid":
            return self._calculate_hybrid_mode(entity_name, stage2_candidates, entity_embedding)
        elif self.scoring_mode == "llm":
            return self._calculate_llm_mode(entity_name, stage2_candidates)
        else:
            logger.error(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” scoring_mode: {self.scoring_mode}")
            return []
    
    def _calculate_hybrid_mode(
        self,
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]],
        entity_embedding: Optional[Any] = None
    ) -> List[Dict[str, Any]]:
        """
        Hybrid ëª¨ë“œ: Text + Semantic ìœ ì‚¬ë„ ì¡°í•©
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2 í›„ë³´ë“¤
            entity_embedding: ì—”í‹°í‹°ì˜ SapBERT ì„ë² ë”©
            
        Returns:
            List[Dict]: ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤
        """
        if entity_embedding is None:
            logger.warning("âš ï¸ ì—”í‹°í‹° ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            entity_embedding = self._get_sapbert_embedding(entity_name)
        
        final_candidates = []
        
        for candidate in stage2_candidates:
            concept = candidate['concept']
            is_original_standard = candidate.get('is_original_standard', True)
            
            # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            if is_original_standard:
                # ì›ë˜ Standardì¸ ê²½ìš°: Jaccard ìœ ì‚¬ë„ ê³„ì‚°
                text_similarity = self._calculate_jaccard_similarity(
                    entity_name, 
                    concept.get('concept_name', ''),
                    ngram=3
                )
            else:
                # Non-std to std ë³€í™˜ëœ ê²½ìš°: ê³ ì • 0.9 ì ìˆ˜
                text_similarity = 0.9
            
            # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            concept_embedding = concept.get('concept_embedding')
            if concept_embedding is not None and entity_embedding is not None and HAS_HYBRID_LIBS:
                # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(concept_embedding, str):
                    # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°: JSON íŒŒì‹±
                    try:
                        concept_embedding = np.array(json.loads(concept_embedding))
                    except:
                        concept_embedding = None
                elif isinstance(concept_embedding, list):
                    # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ëœ ê²½ìš°: numpy ë°°ì—´ë¡œ ë³€í™˜
                    try:
                        concept_embedding = np.array(concept_embedding)
                    except:
                        concept_embedding = None
                elif not isinstance(concept_embedding, np.ndarray):
                    # ê·¸ ì™¸ì˜ ê²½ìš°: numpy ë°°ì—´ë¡œ ì‹œë„
                    try:
                        concept_embedding = np.array(concept_embedding)
                    except:
                        concept_embedding = None
                
                # entity_embeddingë„ numpy ë°°ì—´ë¡œ í™•ì¸/ë³€í™˜
                if isinstance(entity_embedding, list):
                    try:
                        entity_embedding = np.array(entity_embedding)
                    except:
                        entity_embedding = None
                
                if concept_embedding is not None and entity_embedding is not None:
                    semantic_similarity = self._calculate_cosine_similarity(
                        entity_embedding,
                        concept_embedding
                    )
                else:
                    semantic_similarity = 0.0
            else:
                semantic_similarity = 0.0
                if concept_embedding is None:
                    logger.warning(f"âš ï¸ í›„ë³´ {concept.get('concept_id')}ì˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. ìµœì¢… ì ìˆ˜ ê³„ì‚°: 0.4 * text + 0.6 * semantic
            final_score = (self.text_weight * text_similarity + 
                          self.semantic_weight * semantic_similarity)
            
            final_candidates.append({
                'concept': concept,
                'is_original_standard': is_original_standard,
                'original_candidate': candidate.get('original_candidate', {}),
                'elasticsearch_score': candidate.get('elasticsearch_score', 0.0),
                'search_type': candidate.get('search_type', 'unknown'),
                'text_similarity': text_similarity,
                'semantic_similarity': semantic_similarity,
                'final_score': final_score
            })
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        sorted_candidates = sorted(
            final_candidates,
            key=lambda x: x['final_score'],
            reverse=True
        )
        
        # ê²°ê³¼ ë¡œê¹…
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ”¢ Stage 3 Hybrid ê²°ê³¼:")
        logger.info("=" * 80)
        for i, candidate in enumerate(sorted_candidates[:10], 1):
            concept = candidate['concept']
            search_type = candidate.get('search_type', 'unknown')
            is_std_marker = "âœ“" if candidate['is_original_standard'] else "â†’"
            logger.info(f"  {i}. [{search_type}] {is_std_marker} {concept.get('concept_name', 'N/A')} "
                       f"(ID: {concept.get('concept_id', 'N/A')})")
            logger.info(f"     í…ìŠ¤íŠ¸: {candidate['text_similarity']:.4f}, "
                       f"ì˜ë¯¸ì : {candidate['semantic_similarity']:.4f}, "
                       f"ìµœì¢…: {candidate['final_score']:.4f}")
        logger.info("=" * 80)
        
        return sorted_candidates
    
    def _calculate_llm_mode(
        self,
        entity_name: str,
        stage2_candidates: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        LLM ëª¨ë“œ: OpenAI APIë¥¼ ì‚¬ìš©í•œ í‰ê°€
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            stage2_candidates: Stage 2 í›„ë³´ë“¤
            
        Returns:
            List[Dict]: LLM ì ìˆ˜ë¡œ ì •ë ¬ëœ í›„ë³´ë“¤
        """
        if not self.openai_client:
            logger.error("âš ï¸ OpenAI API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # í›„ë³´êµ° ì •ë³´ ì¤€ë¹„
        final_candidates = []
        for candidate in stage2_candidates:
            concept = candidate['concept']
            final_candidates.append({
                'concept': concept,
                'is_original_standard': candidate.get('is_original_standard', True),
                'original_candidate': candidate.get('original_candidate', {}),
                'elasticsearch_score': candidate.get('elasticsearch_score', 0.0),
                'search_type': candidate.get('search_type', 'unknown')
            })
        
        # LLM ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
        try:
            llm_result = self._calculate_llm_scores_api(entity_name, final_candidates)
            
            if not llm_result:
                logger.error("âš ï¸ LLM í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # LLM ì ìˆ˜ë¥¼ ê° í›„ë³´ì— ì¶”ê°€
            for candidate in final_candidates:
                concept_id = str(candidate['concept'].get('concept_id', ''))
                if concept_id in llm_result:
                    candidate['llm_score'] = llm_result[concept_id]['score']
                    candidate['llm_rank'] = llm_result[concept_id]['rank']
                    candidate['llm_reasoning'] = llm_result[concept_id].get('reasoning', '')
                    # final_scoreë¥¼ llm_scoreë¡œ ì„¤ì • (ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©)
                    candidate['final_score'] = candidate['llm_score']
                else:
                    # LLM í‰ê°€ì—ì„œ ëˆ„ë½ëœ ê²½ìš° ì ìˆ˜ 0.0
                    candidate['llm_score'] = 0.0
                    candidate['llm_rank'] = 999
                    candidate['llm_reasoning'] = 'LLM í‰ê°€ì—ì„œ ëˆ„ë½ë¨'
                    candidate['final_score'] = 0.0
            
            # LLM ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_candidates = sorted(
                final_candidates, 
                key=lambda x: x.get('llm_score', 0.0), 
                reverse=True
            )
            
            # ìµœì¢… ìˆœìœ„ ë¡œê¹…
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ¤– Stage 3 LLM ê²°ê³¼ - OpenAI ìˆœìœ„:")
            logger.info("=" * 80)
            for i, candidate in enumerate(sorted_candidates[:10], 1):
                concept = candidate['concept']
                search_type = candidate.get('search_type', 'unknown')
                llm_score = candidate.get('llm_score', 0.0)
                llm_rank = candidate.get('llm_rank', 'N/A')
                logger.info(f"  {i}. {concept.get('concept_name', 'N/A')} "
                          f"(ID: {concept.get('concept_id', 'N/A')}) [{search_type}]")
                logger.info(f"     LLM ì ìˆ˜: {llm_score:.4f} (ìˆœìœ„: {llm_rank})")
                if candidate.get('llm_reasoning'):
                    reasoning = candidate['llm_reasoning'][:100]
                    logger.info(f"     ì´ìœ : {reasoning}...")
            logger.info("=" * 80)
            
            return sorted_candidates
            
        except Exception as e:
            logger.error(f"âš ï¸ LLM í‰ê°€ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_jaccard_similarity(self, text1: str, text2: str, ngram: int = 3) -> float:
        """
        Jaccard ìœ ì‚¬ë„ ê³„ì‚° (n-gram ê¸°ë°˜)
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            ngram: n-gram í¬ê¸° (ê¸°ë³¸ê°’: 3)
            
        Returns:
            float: Jaccard ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        def get_ngrams(text: str, n: int) -> set:
            """í…ìŠ¤íŠ¸ì—ì„œ n-gram ì¶”ì¶œ"""
            text = text.lower().strip()
            if len(text) < n:
                return {text}
            return {text[i:i+n] for i in range(len(text) - n + 1)}
        
        ngrams1 = get_ngrams(text1, ngram)
        ngrams2 = get_ngrams(text2, ngram)
        
        if not ngrams1 or not ngrams2:
            return 0.0
        
        intersection = len(ngrams1 & ngrams2)
        union = len(ngrams1 | ngrams2)
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def _get_sapbert_embedding(self, text: str) -> Optional[Any]:
        """
        í…ìŠ¤íŠ¸ì˜ SapBERT ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            np.ndarray: SapBERT ì„ë² ë”© ë²¡í„° (ë˜ëŠ” None)
        """
        if not HAS_HYBRID_LIBS:
            logger.error("âš ï¸ Hybrid ëª¨ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        if self.sapbert_model is None or self.sapbert_tokenizer is None:
            logger.warning("âš ï¸ SapBERT ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.sapbert_tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.sapbert_device:
                inputs = {k: v.to(self.sapbert_device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self.sapbert_model(**inputs)
                # CLS í† í° ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
            
            return embedding
            
        except Exception as e:
            logger.error(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_cosine_similarity(
        self, 
        embedding1: Any, 
        embedding2: Any
    ) -> float:
        """
        ë‘ ì„ë² ë”© ê°„ì˜ Cosine ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            embedding1: ì²« ë²ˆì§¸ ì„ë² ë”©
            embedding2: ë‘ ë²ˆì§¸ ì„ë² ë”©
            
        Returns:
            float: Cosine ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        if not HAS_HYBRID_LIBS:
            logger.error("âš ï¸ Hybrid ëª¨ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0.0
            
        try:
            # 2D ë°°ì—´ë¡œ ë³€í™˜ (cosine_similarity ìš”êµ¬ì‚¬í•­)
            emb1 = embedding1.reshape(1, -1)
            emb2 = embedding2.reshape(1, -1)
            
            # Cosine ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity(emb1, emb2)[0][0]
            
            # -1 ~ 1 ë²”ìœ„ë¥¼ 0 ~ 1ë¡œ ì •ê·œí™”
            normalized_similarity = (similarity + 1) / 2
            
            return float(normalized_similarity)
            
        except Exception as e:
            logger.error(f"Cosine ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_llm_scores_api(
        self, 
        entity_name: str, 
        candidates: List[Dict[str, Any]],
        max_candidates: int = 15
    ) -> Optional[Dict[str, Dict[str, Any]]]:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´êµ° í‰ê°€
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            candidates: í‰ê°€í•  í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            max_candidates: í‰ê°€í•  ìµœëŒ€ í›„ë³´êµ° ìˆ˜ (ê¸°ë³¸ê°’: 15)
            
        Returns:
            Dict[str, Dict[str, Any]]: concept_idë¥¼ í‚¤ë¡œ í•˜ëŠ” í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.openai_client or not candidates:
            return None
        
        # ìƒìœ„ í›„ë³´ë§Œ í‰ê°€ (ì„±ëŠ¥ìƒ ì´ìœ )
        top_candidates = candidates[:max_candidates]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_llm_prompt(entity_name, top_candidates)
        
        try:
            # OpenAI API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì˜ë£Œ ìš©ì–´ ë§¤í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì—”í‹°í‹°ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ OMOP CDM ê°œë…ì„ ì„ íƒí•˜ê³  ê° í›„ë³´ì— ëŒ€í•´ ì •í™•í•œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2048,
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content
            result = self._parse_llm_response(response_text, top_candidates)
            return result
            
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_llm_prompt(self, entity_name: str, candidates: List[Dict[str, Any]]) -> str:
        """
        LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            entity_name: ì—”í‹°í‹° ì´ë¦„
            candidates: í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        candidates_info = []
        for i, candidate in enumerate(candidates, 1):
            concept = candidate['concept']
            candidates_info.append({
                'concept_id': str(concept.get('concept_id', '')),
                'concept_name': concept.get('concept_name', ''),
                'domain_id': concept.get('domain_id', '')
            })
        
        prompt = f"""ë‹¤ìŒ ì—”í‹°í‹°ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ OMOP CDM ê°œë…ì„ ì„ íƒí•˜ê³  ê° í›„ë³´ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

**ì—”í‹°í‹° ì´ë¦„**: {entity_name}

**í›„ë³´ ê°œë…ë“¤**:
{json.dumps(candidates_info, ensure_ascii=False, indent=2)}

**ì§€ì‹œì‚¬í•­**:
1. ê° í›„ë³´ ê°œë…ì´ ì—”í‹°í‹° ì´ë¦„ê³¼ ì–¼ë§ˆë‚˜ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
2. ì˜ë£Œ ìš©ì–´ì˜ ì˜ë¯¸, ì»¨í…ìŠ¤íŠ¸, ë„ë©”ì¸ ì í•©ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”.
3. **ì¤‘ìš”**: ë¬´ì¡°ê±´ ê°™ì€ ë ˆë²¨ì´ê±°ë‚˜ ìƒìœ„ ë ˆë²¨ì˜ ê°œë…ìœ¼ë¡œë§Œ ë§¤í•‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í•˜ìœ„ ê°œë…(sub-concept)ìœ¼ë¡œëŠ” ë§¤í•‘ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
4. ê° í›„ë³´ì— ëŒ€í•´ 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš” (1.0ì´ ê°€ì¥ ì í•©í•¨).
5. ì„ íƒ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš” (í•œêµ­ì–´ë¡œ). íŠ¹íˆ í•˜ìœ„ ê°œë…ì¸ ê²½ìš° ì´ë¥¼ ëª…í™•íˆ ì§€ì í•˜ê³  ì ìˆ˜ë¥¼ ë‚®ê²Œ ë¶€ì—¬í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹** (JSON):
{{
  "results": [
    {{
      "concept_id": "í›„ë³´ ê°œë… ID",
      "score": 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜,
      "rank": 1~{len(candidates)} ì‚¬ì´ì˜ ìˆœìœ„,
      "reasoning": "ì„ íƒ ì´ìœ  (í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ)"
    }},
    ...
  ]
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        return prompt
    
    def _parse_llm_response(self, response_text: str, candidates: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        LLM ì‘ë‹µ íŒŒì‹±
        
        Args:
            response_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            candidates: í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Dict[str, Any]]: concept_idë¥¼ í‚¤ë¡œ í•˜ëŠ” í‰ê°€ ê²°ê³¼
        """
        try:
            # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            text = response_text.strip()
            if '```json' in text:
                text = text.split('```json')[1].split('```')[0].strip()
            elif '```' in text:
                text = text.split('```')[1].split('```')[0].strip()
            
            # JSON íŒŒì‹±
            parsed = json.loads(text)
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result = {}
            if 'results' in parsed:
                for item in parsed['results']:
                    concept_id = str(item.get('concept_id', ''))
                    if concept_id:
                        result[concept_id] = {
                            'score': float(item.get('score', 0.0)),
                            'rank': int(item.get('rank', 999)),
                            'reasoning': item.get('reasoning', '')
                        }
            
            # ëª¨ë“  í›„ë³´ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì—†ìœ¼ë©´ ì ìˆ˜ 0.0ìœ¼ë¡œ ì¶”ê°€)
            for candidate in candidates:
                concept_id = str(candidate['concept'].get('concept_id', ''))
                if concept_id not in result:
                    result[concept_id] = {
                        'score': 0.0,
                        'rank': 999,
                        'reasoning': 'LLM í‰ê°€ì—ì„œ ëˆ„ë½ë¨'
                    }
            
            return result
            
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:500]}")
            return {}
