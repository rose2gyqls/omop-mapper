"""
CONCEPT ë°ì´í„° SapBERT ì„ë² ë”© ì¸ë±ì‹± ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸

CONCEPT.csv ë°ì´í„°ë¥¼ ì½ì–´ì„œ SapBERT ëª¨ë¸ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ 
Elasticsearchì— ì¸ë±ì‹±í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import logging
import os
import time
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from concept_data_processor import ConceptDataProcessor
from sapbert_embedder import SapBERTEmbedder
from elasticsearch_indexer import ConceptElasticsearchIndexer


class ConceptIndexerWithSapBERT:
    """SapBERT ì„ë² ë”©ì„ í¬í•¨í•œ CONCEPT ë°ì´í„° ì¸ë±ì„œ"""
    
    def __init__(
        self,
        csv_file_path: str,
        es_host: str = "3.35.110.161",
        es_port: int = 9200,
        es_username: str = "elastic",
        es_password: str = "snomed",
        index_name: str = "concept",
        model_name: str = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext",
        gpu_device: int = 0,
        batch_size: int = 128,
        chunk_size: int = 1000,
        lowercase_concept_name: bool = False,
        include_embeddings: bool = True
    ):
        """
        ì¸ë±ì„œ ì´ˆê¸°í™”
        
        Args:
            csv_file_path: CONCEPT.csv íŒŒì¼ ê²½ë¡œ
            es_host: Elasticsearch í˜¸ìŠ¤íŠ¸
            es_port: Elasticsearch í¬íŠ¸
            es_username: Elasticsearch ì‚¬ìš©ìëª…
            es_password: Elasticsearch ë¹„ë°€ë²ˆí˜¸
            index_name: ì¸ë±ìŠ¤ëª… (ì˜ˆ: "concepts" ë˜ëŠ” "concepts-small")
            model_name: SapBERT ëª¨ë¸ëª…
            gpu_device: ì‚¬ìš©í•  GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸
            batch_size: ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
            chunk_size: ë°ì´í„° ì²˜ë¦¬ ì²­í¬ í¬ê¸°
            lowercase_concept_name: concept_nameì„ ì†Œë¬¸ìë¡œ ë³€í™˜í• ì§€ ì—¬ë¶€
        """
        self.csv_file_path = csv_file_path
        self.index_name = index_name
        self.batch_size = batch_size
        self.chunk_size = chunk_size
        self.lowercase_concept_name = lowercase_concept_name
        self.include_embeddings = include_embeddings
        
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = f"cuda:{gpu_device}" if gpu_device >= 0 else "cpu"
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logging.info("=== CONCEPT ì¸ë±ì„œ ì´ˆê¸°í™” ===")
        
        # 1. ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        logging.info("1. ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”...")
        self.data_processor = ConceptDataProcessor(csv_file_path)
        
        # 2. SapBERT ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™” (ì˜µì…˜)
        if self.include_embeddings:
            logging.info("2. SapBERT ëª¨ë¸ ë¡œë”©...")
            self.embedder = SapBERTEmbedder(
                model_name=model_name,
                device=device,
                batch_size=batch_size
            )
        else:
            logging.info("2. ì„ë² ë”© ë¹„í™œì„±í™” ëª¨ë“œ: SapBERT ëª¨ë¸ ë¡œë”© ê±´ë„ˆëœ€")
        
        # 3. Elasticsearch ì¸ë±ì„œ ì´ˆê¸°í™”
        logging.info("3. Elasticsearch ì¸ë±ì„œ ì´ˆê¸°í™”...")
        self.es_indexer = ConceptElasticsearchIndexer(
            es_host=es_host,
            es_port=es_port,
            username=es_username,
            password=es_password,
            index_name=index_name,
            include_embeddings=self.include_embeddings
        )
        
        logging.info("=== ì´ˆê¸°í™” ì™„ë£Œ ===")
    
    def run_full_indexing(
        self,
        delete_existing_index: bool = True,
        max_concepts: int = None,
        skip_concepts: int = 0
    ) -> bool:
        """
        ì „ì²´ ì¸ë±ì‹± í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            delete_existing_index: ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì—¬ë¶€
            max_concepts: ìµœëŒ€ ì²˜ë¦¬í•  concept ìˆ˜ (Noneì´ë©´ ì „ì²´)
            skip_concepts: ê±´ë„ˆë›¸ concept ìˆ˜
            
        Returns:
            ì¸ë±ì‹± ì„±ê³µ ì—¬ë¶€
        """
        start_time = time.time()
        
        try:
            # 1. ì¸ë±ìŠ¤ ìƒì„±
            logging.info("=== 1ë‹¨ê³„: Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ===")
            if not self.es_indexer.create_index(delete_if_exists=delete_existing_index):
                logging.error("ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # 2. ì´ ë°ì´í„° í–‰ ìˆ˜ í™•ì¸
            logging.info("=== 2ë‹¨ê³„: ë°ì´í„° í¬ê¸° í™•ì¸ ===")
            total_rows = self.data_processor.get_total_rows()
            logging.info(f"ì´ CONCEPT ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}")
            
            # ì²˜ë¦¬í•  í–‰ ìˆ˜ ê³„ì‚°
            actual_skip = skip_concepts
            actual_max = min(max_concepts, total_rows - actual_skip) if max_concepts else (total_rows - actual_skip)
            
            logging.info(f"ê±´ë„ˆë›¸ í–‰ ìˆ˜: {actual_skip:,}")
            logging.info(f"ì²˜ë¦¬í•  í–‰ ìˆ˜: {actual_max:,}")
            
            # 3. ë°ì´í„° ì²˜ë¦¬ ë° ì¸ë±ì‹±
            logging.info("=== 3ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ì¸ë±ì‹± ===")
            
            total_processed = 0
            total_indexed = 0
            
            # ì „ì²´ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
            with tqdm(total=actual_max, desc="ì „ì²´ ì§„í–‰ë¥ ", unit="ê°œ") as pbar:
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬
                for chunk_df in self.data_processor.read_concepts_in_chunks(
                    chunk_size=self.chunk_size,
                    skip_rows=actual_skip,
                    max_rows=actual_max
                ):
                    
                    if len(chunk_df) == 0:
                        continue
                    
                    # concept_nameì„ ì†Œë¬¸ìë¡œ ë³€í™˜ (ì˜µì…˜)
                    if self.lowercase_concept_name:
                        chunk_df = chunk_df.copy()
                        chunk_df['concept_name'] = chunk_df['concept_name'].str.lower()
                    
                    # concept_name ì¶”ì¶œ (ì„ë² ë”©ìš©)
                    concept_names = chunk_df['concept_name'].fillna('').tolist()
                    
                    # SapBERT ì„ë² ë”© ìƒì„± (ì˜µì…˜)
                    embeddings = None
                    if self.include_embeddings:
                        logging.info(f"ì²­í¬ {len(chunk_df)}ê°œ concept ì„ë² ë”© ìƒì„± ì¤‘...")
                        embeddings = self.embedder.encode_texts(concept_names, show_progress=False)
                    
                    # Elasticsearch ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    documents = self.data_processor.convert_to_elasticsearch_format(
                        chunk_df, 
                        embeddings=embeddings,
                        include_embeddings=self.include_embeddings
                    )
                    
                    # Elasticsearchì— ì¸ë±ì‹± (ì†Œë¬¸ì ë³€í™˜ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ False)
                    logging.info(f"ì²­í¬ {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì¤‘...")
                    if self.es_indexer.index_concepts(
                        documents, 
                        show_progress=False,
                        lowercase_concept_name=False  # ì´ë¯¸ ë³€í™˜ ì™„ë£Œ
                    ):
                        total_indexed += len(documents)
                    
                    total_processed += len(chunk_df)
                    pbar.update(len(chunk_df))
                    
                    # ê°„ë‹¨í•œ ì§„í–‰ ìƒí™© ë¡œê¹…
                    if total_processed % (self.chunk_size * 10) == 0:  # 10ì²­í¬ë§ˆë‹¤ ë¡œê¹…
                        elapsed_time = time.time() - start_time
                        rate = total_processed / elapsed_time if elapsed_time > 0 else 0
                        logging.info(
                            f"ì§„í–‰: {total_processed:,}/{actual_max:,} "
                            f"({total_processed/actual_max*100:.1f}%) | "
                            f"ì²˜ë¦¬ì†ë„: {rate:.1f} concepts/sec"
                        )
            
            # 4. ê²°ê³¼ í™•ì¸
            logging.info("=== 4ë‹¨ê³„: ì¸ë±ì‹± ê²°ê³¼ í™•ì¸ ===")
            
            # ì¸ë±ìŠ¤ í†µê³„ í™•ì¸
            stats = self.es_indexer.get_index_stats()
            logging.info(f"ì¸ë±ìŠ¤ í†µê³„: {stats}")
            
            # ì´ ì†Œìš” ì‹œê°„
            total_time = time.time() - start_time
            logging.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            logging.info(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {total_processed/total_time:.1f} concepts/sec")
            
            logging.info("=== ì¸ë±ì‹± ì™„ë£Œ ===")
            return True
            
        except Exception as e:
            logging.error(f"ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def test_search(self, test_queries: List[str] = None) -> None:
        """
        ì¸ë±ì‹±ëœ ë°ì´í„°ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        
        Args:
            test_queries: í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if test_queries is None:
            test_queries = [
                "covid-19",
                "hypertension",
                "diabetes",
                "heart failure",
                "pneumonia"
            ]
        
        logging.info("=== ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
        
        for query in test_queries:
            logging.info(f"\nê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
            
            # ì†Œë¬¸ì ë³€í™˜ ì¸ë±ìŠ¤ì¸ ê²½ìš° ì¿¼ë¦¬ë„ ì†Œë¬¸ìë¡œ ë³€í™˜
            search_query = query.lower() if self.lowercase_concept_name else query
            
            # ì„ë² ë”© ë¹„í™œì„±í™” ì‹œ ê²€ìƒ‰ ìŠ¤í‚µ
            if not self.include_embeddings:
                logging.info("ì„ë² ë”© ë¹„í™œì„±í™” ìƒíƒœì´ë¯€ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ë° ê²€ìƒ‰
            query_embedding = self.embedder.encode_texts([search_query], show_progress=False)[0]
            results = self.es_indexer.search_by_embedding(
                query_embedding.tolist(),
                size=5,
                min_score=0.5
            )
            
            # ê²°ê³¼ ì¶œë ¥
            for i, result in enumerate(results, 1):
                logging.info(
                    f"  {i}. {result['concept_name']} "
                    f"(ID: {result['concept_id']}, "
                    f"ìœ ì‚¬ë„: {result['similarity_score']:.3f})"
                )
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'embedder'):
            del self.embedder
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


def main(create_small_index: bool = False, gpu_device: int = 0, resume: bool = False, include_embeddings: bool = True):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì¸ë±ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ì„¤ì •
    if create_small_index:
        index_name = "concept-small"
        lowercase_concept_name = True
        log_prefix = "concept_small"
        print(f"=== concepts-small ì¸ë±ìŠ¤ ìƒì„± (ì†Œë¬¸ì ë³€í™˜, GPU {gpu_device}) ===")
    else:
        index_name = "concept"
        lowercase_concept_name = False
        log_prefix = "concept"
        print(f"=== concepts ì¸ë±ìŠ¤ ìƒì„± (ì›ë³¸ ìœ ì§€, GPU {gpu_device}) ===")
    
    # ë¡œê¹… ì„¤ì •
    log_filename = f'{log_prefix}_indexing_gpu{gpu_device}_{time.strftime("%Y%m%d_%H%M%S")}.log'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    
    # ì„¤ì •
    CSV_FILE_PATH = "/home/work/skku/hyo/omop-mapper/data/CONCEPT.csv"
    ES_HOST = "3.35.110.161"
    ES_PORT = 9200
    ES_USERNAME = "elastic"
    ES_PASSWORD = "snomed"
    MODEL_NAME = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
    BATCH_SIZE = 128  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
    CHUNK_SIZE = 1000   # ë°ì´í„° ì²˜ë¦¬ ì²­í¬ í¬ê¸° (ì„ë² ë”© í¬í•¨ ì‹œ ì¶•ì†Œ)
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • (ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” Noneìœ¼ë¡œ ì„¤ì •)
    MAX_CONCEPTS = None  # Noneì´ë©´ ì „ì²´ ë°ì´í„° ì²˜ë¦¬
    SKIP_CONCEPTS = 0    # ê±´ë„ˆë›¸ concept ìˆ˜
    
    # Resume ê¸°ëŠ¥: ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ í˜„ì¬ ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
    if resume:
        try:
            # Elasticsearch ì¸ë±ì„œ ì„ì‹œ ìƒì„±í•˜ì—¬ í˜„ì¬ ë¬¸ì„œ ìˆ˜ í™•ì¸
            from elasticsearch_indexer import ConceptElasticsearchIndexer
            temp_indexer = ConceptElasticsearchIndexer(
                es_host=ES_HOST,
                es_port=ES_PORT,
                username=ES_USERNAME,
                password=ES_PASSWORD,
                index_name=index_name
            )
            
            # í˜„ì¬ ì¸ë±ìŠ¤ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            stats = temp_indexer.get_index_stats()
            if stats and 'document_count' in stats:
                current_count = stats['document_count']
                SKIP_CONCEPTS = current_count
                logging.info(f"ğŸ”„ Resume ëª¨ë“œ: í˜„ì¬ {current_count:,}ê°œ ë¬¸ì„œê°€ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                logging.info(f"ğŸ”„ {current_count:,}ê°œ ë¬¸ì„œë¥¼ ê±´ë„ˆë›°ê³  ì´í›„ë¶€í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logging.warning("âš ï¸ Resume ëª¨ë“œì´ì§€ë§Œ ê¸°ì¡´ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                logging.warning(f"âš ï¸ ë°›ì€ í†µê³„ ì •ë³´: {stats}")
                SKIP_CONCEPTS = 0
                
        except Exception as e:
            logging.error(f"âŒ Resume ëª¨ë“œ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logging.warning("âš ï¸ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            SKIP_CONCEPTS = 0
    
    try:
        # ì¸ë±ì„œ ì´ˆê¸°í™”
        indexer = ConceptIndexerWithSapBERT(
            csv_file_path=CSV_FILE_PATH,
            es_host=ES_HOST,
            es_port=ES_PORT,
            es_username=ES_USERNAME,
            es_password=ES_PASSWORD,
            index_name=index_name,  # ë™ì ìœ¼ë¡œ ì„¤ì •
            model_name=MODEL_NAME,
            gpu_device=gpu_device,
            batch_size=BATCH_SIZE,
            chunk_size=CHUNK_SIZE,
            lowercase_concept_name=lowercase_concept_name,  # ë™ì ìœ¼ë¡œ ì„¤ì •
            include_embeddings=include_embeddings
        )
        
        # ì „ì²´ ì¸ë±ì‹± ì‹¤í–‰
        success = indexer.run_full_indexing(
            delete_existing_index=not resume,  # resume ëª¨ë“œì¼ ë•ŒëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            max_concepts=MAX_CONCEPTS,
            skip_concepts=SKIP_CONCEPTS
        )
        
        if success:
            logging.info(f"{index_name} ì¸ë±ì‹±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            indexer.test_search()
        else:
            logging.error(f"{index_name} ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if 'indexer' in locals():
            indexer.cleanup()


if __name__ == "__main__":
    import sys
    import argparse
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='CONCEPT ë°ì´í„° ì¸ë±ì‹±')
    parser.add_argument('--small', action='store_true', help='concepts-small ì¸ë±ìŠ¤ ìƒì„± (ì†Œë¬¸ì ë³€í™˜)')
    parser.add_argument('--gpu', type=int, default=0, help='ì‚¬ìš©í•  GPU ë²ˆí˜¸ (ê¸°ë³¸ê°’: 0)')
    parser.add_argument('--no-embedding', action='store_true', help='ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹± ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    main(create_small_index=args.small, gpu_device=args.gpu, include_embeddings=True)
