#!/usr/bin/env python3
"""
SapBERTì™€ eland í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” SapBERT ëª¨ë¸ì„ elandë¥¼ í†µí•´ Elasticsearchì— ë°°í¬í•˜ê³ 
text_embedding íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import logging
import time
import subprocess
import sys
from pathlib import Path
from elasticsearch import Elasticsearch
import json


def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )


def check_elasticsearch_connection(es_url="http://3.35.110.161:9200"):
    """Elasticsearch ì—°ê²° í™•ì¸"""
    try:
        es = Elasticsearch(
            [es_url], 
            basic_auth=('elastic', 'snomed'),
            request_timeout=30
        )
        if es.ping():
            logging.info("âœ… Elasticsearch ì—°ê²° ì„±ê³µ")
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ í™•ì¸
            info = es.info()
            logging.info(f"Elasticsearch ë²„ì „: {info['version']['number']}")
            
            # ML ê¸°ëŠ¥ í™•ì¸
            try:
                ml_info = es.ml.info()
                logging.info("âœ… ML ê¸°ëŠ¥ í™œì„±í™”ë¨")
                logging.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œì„¸ì„œ: {ml_info.get('defaults', {}).get('anomaly_detectors', {}).get('max_model_memory_limit', 'N/A')}")
                return es
            except Exception as e:
                logging.warning(f"âš ï¸ ML ê¸°ëŠ¥ í™•ì¸ ì‹¤íŒ¨: {e}")
                return es
                
        else:
            logging.error("âŒ Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
            
    except Exception as e:
        logging.error(f"âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {e}")
        return None


def test_sapbert_model_info():
    """SapBERT ëª¨ë¸ ì •ë³´ í™•ì¸"""
    logging.info("=== SapBERT ëª¨ë¸ ì •ë³´ í™•ì¸ ===")
    
    try:
        from transformers import AutoTokenizer, AutoModel, AutoConfig
        
        model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
        
        # ëª¨ë¸ ì„¤ì • í™•ì¸
        config = AutoConfig.from_pretrained(model_name)
        logging.info(f"ëª¨ë¸ëª…: {model_name}")
        logging.info(f"ëª¨ë¸ íƒ€ì…: {config.model_type}")
        logging.info(f"íˆë“  ì‚¬ì´ì¦ˆ: {config.hidden_size}")
        logging.info(f"ìµœëŒ€ ê¸¸ì´: {config.max_position_embeddings}")
        
        # í† í¬ë‚˜ì´ì € í™•ì¸
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        logging.info(f"í† í¬ë‚˜ì´ì € íƒ€ì…: {type(tokenizer).__name__}")
        
        # í…ŒìŠ¤íŠ¸ í† í°í™”
        test_text = "covid-19"
        tokens = tokenizer(test_text, return_tensors="pt")
        logging.info(f"í…ŒìŠ¤íŠ¸ í† í°í™” ì„±ê³µ: {test_text} -> {tokens['input_ids'].shape}")
        
        return True
        
    except Exception as e:
        logging.error(f"âŒ SapBERT ëª¨ë¸ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def deploy_sapbert_to_elasticsearch(es_url="http://3.35.110.161:9200"):
    """SapBERT ëª¨ë¸ì„ Elasticsearchì— ë°°í¬"""
    logging.info("=== SapBERT ëª¨ë¸ Elasticsearch ë°°í¬ ===")
    
    model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
    es_model_id = "sapbert-from-pubmedbert"
    
    cmd = [
        "poetry", "run", "eland_import_hub_model",
        "--url", es_url,
        "-u", "elastic",
        "-p", "snomed",
        "--hub-model-id", model_name,
        "--es-model-id", es_model_id,
        "--task-type", "text_embedding",
        "--start",
        "--clear-previous"
    ]
    
    logging.info(f"ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    try:
        # ëª¨ë¸ ë°°í¬ ì‹¤í–‰
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode == 0:
            logging.info("âœ… SapBERT ëª¨ë¸ ë°°í¬ ì„±ê³µ")
            logging.info(result.stdout)
            return es_model_id
        else:
            logging.error("âŒ SapBERT ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨")
            logging.error(result.stderr)
            return None
            
    except subprocess.TimeoutExpired:
        logging.error("âŒ ëª¨ë¸ ë°°í¬ íƒ€ì„ì•„ì›ƒ (10ë¶„)")
        return None
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë°°í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def test_model_inference(es, model_id):
    """ë°°í¬ëœ ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    logging.info("=== ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        models = es.ml.get_trained_models(model_id=model_id)
        model_info = models['trained_models'][0]
        logging.info(f"ëª¨ë¸ ìƒíƒœ: {model_info.get('metadata', {}).get('model_id', 'N/A')}")
        
        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        test_texts = [
            "covid-19",
            "high fever", 
            "diabetes mellitus",
            "heart attack"
        ]
        
        for text in test_texts:
            try:
                # ML ì¶”ë¡  API í˜¸ì¶œ
                response = es.ml.infer_trained_model(
                    model_id=model_id,
                    docs=[{"text_field": text}]
                )
                
                if 'inference_results' in response:
                    result = response['inference_results'][0]
                    if 'predicted_value' in result:
                        embedding = result['predicted_value']
                        logging.info(f"âœ… '{text}' -> ì„ë² ë”© ì°¨ì›: {len(embedding)}")
                    else:
                        logging.warning(f"âš ï¸ '{text}' -> ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°: {result}")
                else:
                    logging.warning(f"âš ï¸ '{text}' -> ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ: {response}")
                    
            except Exception as e:
                logging.error(f"âŒ '{text}' ì¶”ë¡  ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def create_test_ingest_pipeline(es, model_id):
    """í…ŒìŠ¤íŠ¸ìš© Ingest Pipeline ìƒì„±"""
    logging.info("=== Ingest Pipeline ìƒì„± ===")
    
    pipeline_name = "sapbert-embedding-pipeline"
    
    pipeline_config = {
        "processors": [
            {
                "inference": {
                    "model_id": model_id,
                    "target_field": "ml",
                    "field_map": {
                        "concept_name": "text_field"
                    }
                }
            },
            {
                "set": {
                    "field": "concept_embedding",
                    "value": "{{ml.predicted_value}}"
                }
            },
            {
                "remove": {
                    "field": "ml",
                    "ignore_missing": True
                }
            }
        ]
    }
    
    try:
        # Pipeline ìƒì„±
        es.ingest.put_pipeline(id=pipeline_name, body=pipeline_config)
        logging.info(f"âœ… Ingest Pipeline ìƒì„± ì„±ê³µ: {pipeline_name}")
        
        # Pipeline í…ŒìŠ¤íŠ¸
        test_doc = {
            "concept_id": "test_001",
            "concept_name": "covid-19",
            "domain_id": "Condition"
        }
        
        simulate_response = es.ingest.simulate(
            id=pipeline_name,
            body={"docs": [{"_source": test_doc}]}
        )
        
        if simulate_response.get('docs'):
            processed_doc = simulate_response['docs'][0]['doc']['_source']
            if 'concept_embedding' in processed_doc:
                embedding_len = len(processed_doc['concept_embedding'])
                logging.info(f"âœ… Pipeline í…ŒìŠ¤íŠ¸ ì„±ê³µ: ì„ë² ë”© ì°¨ì› {embedding_len}")
                return pipeline_name
            else:
                logging.error("âŒ Pipeline í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: concept_embedding í•„ë“œê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                logging.error(f"ì²˜ë¦¬ëœ ë¬¸ì„œ: {processed_doc}")
        else:
            logging.error("âŒ Pipeline ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨")
            logging.error(f"ì‘ë‹µ: {simulate_response}")
        
        return None
        
    except Exception as e:
        logging.error(f"âŒ Ingest Pipeline ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def test_index_with_pipeline(es, pipeline_name):
    """Pipelineì„ ì‚¬ìš©í•œ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸"""
    logging.info("=== Pipeline ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ ===")
    
    test_index = "test_concepts"
    
    try:
        # í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì‚­ì œ (ì¡´ì¬í•  ê²½ìš°)
        if es.indices.exists(index=test_index):
            es.indices.delete(index=test_index)
        
        # ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
        index_mapping = {
            "mappings": {
                "properties": {
                    "concept_id": {"type": "keyword"},
                    "concept_name": {"type": "text"},
                    "domain_id": {"type": "keyword"},
                    "concept_embedding": {
                        "type": "dense_vector",
                        "dims": 768,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
        
        es.indices.create(index=test_index, body=index_mapping)
        logging.info(f"âœ… í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ìƒì„±: {test_index}")
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹±
        test_docs = [
            {"concept_id": "001", "concept_name": "covid-19", "domain_id": "Condition"},
            {"concept_id": "002", "concept_name": "high fever", "domain_id": "Observation"},
            {"concept_id": "003", "concept_name": "diabetes", "domain_id": "Condition"}
        ]
        
        for doc in test_docs:
            response = es.index(
                index=test_index,
                id=doc["concept_id"],
                body=doc,
                pipeline=pipeline_name
            )
            logging.info(f"âœ… ë¬¸ì„œ ì¸ë±ì‹±: {doc['concept_name']} -> {response['result']}")
        
        # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
        es.indices.refresh(index=test_index)
        
        # ê²°ê³¼ í™•ì¸
        search_response = es.search(index=test_index, body={"query": {"match_all": {}}})
        
        for hit in search_response['hits']['hits']:
            doc = hit['_source']
            if 'concept_embedding' in doc:
                embedding_len = len(doc['concept_embedding'])
                logging.info(f"âœ… í™•ì¸: {doc['concept_name']} -> ì„ë² ë”© ì°¨ì› {embedding_len}")
            else:
                logging.error(f"âŒ ì„ë² ë”© ëˆ„ë½: {doc['concept_name']}")
        
        # í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì‚­ì œ
        es.indices.delete(index=test_index)
        logging.info("âœ… í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        logging.error(f"âŒ Pipeline ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def cleanup_test_resources(es, model_id, pipeline_name):
    """í…ŒìŠ¤íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    logging.info("=== í…ŒìŠ¤íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ===")
    
    try:
        # Pipeline ì‚­ì œ
        if pipeline_name:
            es.ingest.delete_pipeline(id=pipeline_name)
            logging.info(f"âœ… Pipeline ì‚­ì œ: {pipeline_name}")
        
        # ëª¨ë¸ ì¤‘ì§€ ë° ì‚­ì œ (ì„ íƒì‚¬í•­)
        # es.ml.stop_trained_model_deployment(model_id=model_id)
        # es.ml.delete_trained_model(model_id=model_id)
        # logging.info(f"âœ… ëª¨ë¸ ì‚­ì œ: {model_id}")
        
    except Exception as e:
        logging.warning(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    setup_logging()
    
    logging.info("ğŸš€ SapBERT + eland í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # 1. SapBERT ëª¨ë¸ ì •ë³´ í™•ì¸
    if not test_sapbert_model_info():
        logging.error("âŒ SapBERT ëª¨ë¸ ì •ë³´ í™•ì¸ ì‹¤íŒ¨")
        return False
    
    # 2. Elasticsearch ì—°ê²° í™•ì¸
    es = check_elasticsearch_connection()
    if not es:
        logging.error("âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨")
        logging.info("ğŸ’¡ Elasticsearchë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        logging.info("   docker run -d --name elasticsearch -p 9200:9200 -e 'discovery.type=single-node' -e 'xpack.security.enabled=false' -e 'xpack.ml.enabled=true' docker.elastic.co/elasticsearch/elasticsearch:9.1.0")
        return False
    
    # 3. SapBERT ëª¨ë¸ ë°°í¬
    model_id = deploy_sapbert_to_elasticsearch()
    if not model_id:
        logging.error("âŒ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨")
        return False
    
    # 4. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    if not test_model_inference(es, model_id):
        logging.error("âŒ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False
    
    # 5. Ingest Pipeline ìƒì„± ë° í…ŒìŠ¤íŠ¸
    pipeline_name = create_test_ingest_pipeline(es, model_id)
    if not pipeline_name:
        logging.error("âŒ Ingest Pipeline ìƒì„± ì‹¤íŒ¨")
        return False
    
    # 6. Pipelineì„ ì‚¬ìš©í•œ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸
    if not test_index_with_pipeline(es, pipeline_name):
        logging.error("âŒ Pipeline ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False
    
    # 7. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanup_test_resources(es, model_id, pipeline_name)
    
    logging.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! SapBERT + eland í˜¸í™˜ì„± í™•ì¸ë¨")
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
