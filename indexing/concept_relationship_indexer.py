"""
CONCEPT_RELATIONSHIP ë°ì´í„° ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸

CONCEPT_RELATIONSHIP.csv ë°ì´í„°ë¥¼ ì½ì–´ì„œ Elasticsearchì— ì¸ë±ì‹±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import logging
import os
import time
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from tqdm.auto import tqdm

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from concept_relationship_data_processor import ConceptRelationshipDataProcessor
from elasticsearch_indexer import ConceptElasticsearchIndexer


class ConceptRelationshipIndexer:
    """CONCEPT_RELATIONSHIP ë°ì´í„° ì¸ë±ì„œ"""
    
    def __init__(
        self,
        csv_file_path: str,
        es_host: str = "3.35.110.161",
        es_port: int = 9200,
        es_username: str = "elastic",
        es_password: str = "snomed",
        index_name: str = "concept-relationship",
        chunk_size: int = 10000
    ):
        """
        ì¸ë±ì„œ ì´ˆê¸°í™”
        
        Args:
            csv_file_path: CONCEPT_RELATIONSHIP.csv íŒŒì¼ ê²½ë¡œ
            es_host: Elasticsearch í˜¸ìŠ¤íŠ¸
            es_port: Elasticsearch í¬íŠ¸
            es_username: Elasticsearch ì‚¬ìš©ìëª…
            es_password: Elasticsearch ë¹„ë°€ë²ˆí˜¸
            index_name: ì¸ë±ìŠ¤ëª…
            chunk_size: ë°ì´í„° ì²˜ë¦¬ ì²­í¬ í¬ê¸°
        """
        self.csv_file_path = csv_file_path
        self.index_name = index_name
        self.chunk_size = chunk_size
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logging.info("=== CONCEPT_RELATIONSHIP ì¸ë±ì„œ ì´ˆê¸°í™” ===")
        
        # 1. ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        logging.info("1. ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”...")
        self.data_processor = ConceptRelationshipDataProcessor(csv_file_path)
        
        # 2. Elasticsearch ì¸ë±ì„œ ì´ˆê¸°í™”
        logging.info("2. Elasticsearch ì¸ë±ì„œ ì´ˆê¸°í™”...")
        self.es_indexer = ConceptElasticsearchIndexer(
            es_host=es_host,
            es_port=es_port,
            username=es_username,
            password=es_password,
            index_name=index_name,
            include_embeddings=False  # relationship ë°ì´í„°ëŠ” ì„ë² ë”© ë¶ˆí•„ìš”
        )
        
        logging.info("=== ì´ˆê¸°í™” ì™„ë£Œ ===")
    
    def run_full_indexing(
        self,
        delete_existing_index: bool = True,
        max_rows: int = None,
        skip_rows: int = 0
    ) -> bool:
        """
        ì „ì²´ ì¸ë±ì‹± í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            delete_existing_index: ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì—¬ë¶€
            max_rows: ìµœëŒ€ ì²˜ë¦¬í•  í–‰ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            skip_rows: ê±´ë„ˆë›¸ í–‰ ìˆ˜
            
        Returns:
            ì¸ë±ì‹± ì„±ê³µ ì—¬ë¶€
        """
        start_time = time.time()
        
        try:
            # 1. ì¸ë±ìŠ¤ ìƒì„±
            logging.info("=== 1ë‹¨ê³„: Elasticsearch ì¸ë±ìŠ¤ ìƒì„± ===")
            if not self.es_indexer.create_index(delete_if_exists=delete_existing_index):
                logging.error("ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # 2. ì´ ë°ì´í„° í–‰ ìˆ˜ í™•ì¸
            logging.info("=== 2ë‹¨ê³„: ë°ì´í„° í¬ê¸° í™•ì¸ ===")
            total_rows = self.data_processor.get_total_rows()
            logging.info(f"ì´ CONCEPT_RELATIONSHIP ë°ì´í„° í–‰ ìˆ˜: {total_rows:,}")
            
            # ì²˜ë¦¬í•  í–‰ ìˆ˜ ê³„ì‚°
            actual_skip = skip_rows
            actual_max = min(max_rows, total_rows - actual_skip) if max_rows else (total_rows - actual_skip)
            
            logging.info(f"ê±´ë„ˆë›¸ í–‰ ìˆ˜: {actual_skip:,}")
            logging.info(f"ì²˜ë¦¬í•  í–‰ ìˆ˜: {actual_max:,}")
            
            # 3. ë°ì´í„° ì²˜ë¦¬ ë° ì¸ë±ì‹±
            logging.info("=== 3ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ì¸ë±ì‹± ===")
            
            total_processed = 0
            total_indexed = 0
            
            # ì „ì²´ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
            with tqdm(total=actual_max, desc="ì „ì²´ ì§„í–‰ë¥ ", unit="ê°œ") as pbar:
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬
                for chunk_df in self.data_processor.read_relationships_in_chunks(
                    chunk_size=self.chunk_size,
                    skip_rows=actual_skip,
                    max_rows=actual_max
                ):
                    
                    if len(chunk_df) == 0:
                        continue
                    
                    # Elasticsearch ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    documents = self.data_processor.convert_to_elasticsearch_format(chunk_df)
                    
                    # Elasticsearchì— ì¸ë±ì‹±
                    logging.info(f"ì²­í¬ {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì¤‘...")
                    if self.es_indexer.index_concepts(
                        documents, 
                        show_progress=False
                    ):
                        total_indexed += len(documents)
                    
                    total_processed += len(chunk_df)
                    pbar.update(len(chunk_df))
                    
                    # ê°„ë‹¨í•œ ì§„í–‰ ìƒí™© ë¡œê¹…
                    if total_processed % (self.chunk_size * 10) == 0:  # 10ì²­í¬ë§ˆë‹¤ ë¡œê¹…
                        elapsed_time = time.time() - start_time
                        rate = total_processed / elapsed_time if elapsed_time > 0 else 0
                        logging.info(
                            f"ì§„í–‰: {total_processed:,}/{actual_max:,} "
                            f"({total_processed/actual_max*100:.1f}%) | "
                            f"ì²˜ë¦¬ì†ë„: {rate:.1f} rows/sec"
                        )
            
            # 4. ê²°ê³¼ í™•ì¸
            logging.info("=== 4ë‹¨ê³„: ì¸ë±ì‹± ê²°ê³¼ í™•ì¸ ===")
            
            # ì¸ë±ìŠ¤ í†µê³„ í™•ì¸
            stats = self.es_indexer.get_index_stats()
            logging.info(f"ì¸ë±ìŠ¤ í†µê³„: {stats}")
            
            # ì´ ì†Œìš” ì‹œê°„
            total_time = time.time() - start_time
            logging.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            logging.info(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {total_processed/total_time:.1f} rows/sec")
            
            logging.info("=== ì¸ë±ì‹± ì™„ë£Œ ===")
            return True
            
        except Exception as e:
            logging.error(f"ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return False


def main(resume: bool = False):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    index_name = "concept-relationship"
    print(f"=== {index_name} ì¸ë±ìŠ¤ ìƒì„± ===")
    
    # ë¡œê¹… ì„¤ì •
    log_filename = f'concept_relationship_indexing_{time.strftime("%Y%m%d_%H%M%S")}.log'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    
    # ì„¤ì •
    CSV_FILE_PATH = "/home/work/skku/hyo/omop-mapper/data/CONCEPT_RELATIONSHIP.csv"
    ES_HOST = "3.35.110.161"
    ES_PORT = 9200
    ES_USERNAME = "elastic"
    ES_PASSWORD = "snomed"
    CHUNK_SIZE = 10000  # relationship ë°ì´í„°ëŠ” ì„ë² ë”©ì´ ì—†ìœ¼ë¯€ë¡œ í° ì²­í¬ í¬ê¸° ì‚¬ìš©
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • (ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” Noneìœ¼ë¡œ ì„¤ì •)
    MAX_ROWS = None  # Noneì´ë©´ ì „ì²´ ë°ì´í„° ì²˜ë¦¬
    SKIP_ROWS = 0    # ê±´ë„ˆë›¸ í–‰ ìˆ˜
    
    # Resume ê¸°ëŠ¥: ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ í˜„ì¬ ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
    if resume:
        try:
            # Elasticsearch ì¸ë±ì„œ ì„ì‹œ ìƒì„±í•˜ì—¬ í˜„ì¬ ë¬¸ì„œ ìˆ˜ í™•ì¸
            from elasticsearch_indexer import ConceptElasticsearchIndexer
            temp_indexer = ConceptElasticsearchIndexer(
                es_host=ES_HOST,
                es_port=ES_PORT,
                username=ES_USERNAME,
                password=ES_PASSWORD,
                index_name=index_name,
                include_embeddings=False
            )
            
            # í˜„ì¬ ì¸ë±ìŠ¤ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            stats = temp_indexer.get_index_stats()
            if stats and 'document_count' in stats:
                current_count = stats['document_count']
                SKIP_ROWS = current_count
                logging.info(f"ğŸ”„ Resume ëª¨ë“œ: í˜„ì¬ {current_count:,}ê°œ ë¬¸ì„œê°€ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                logging.info(f"ğŸ”„ {current_count:,}ê°œ ë¬¸ì„œë¥¼ ê±´ë„ˆë›°ê³  ì´í›„ë¶€í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logging.warning("âš ï¸ Resume ëª¨ë“œì´ì§€ë§Œ ê¸°ì¡´ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                logging.warning(f"âš ï¸ ë°›ì€ í†µê³„ ì •ë³´: {stats}")
                SKIP_ROWS = 0
                
        except Exception as e:
            logging.error(f"âŒ Resume ëª¨ë“œ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logging.warning("âš ï¸ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            SKIP_ROWS = 0
    
    try:
        # ì¸ë±ì„œ ì´ˆê¸°í™”
        indexer = ConceptRelationshipIndexer(
            csv_file_path=CSV_FILE_PATH,
            es_host=ES_HOST,
            es_port=ES_PORT,
            es_username=ES_USERNAME,
            es_password=ES_PASSWORD,
            index_name=index_name,
            chunk_size=CHUNK_SIZE
        )
        
        # ì „ì²´ ì¸ë±ì‹± ì‹¤í–‰
        success = indexer.run_full_indexing(
            delete_existing_index=not resume,  # resume ëª¨ë“œì¼ ë•ŒëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            max_rows=MAX_ROWS,
            skip_rows=SKIP_ROWS
        )
        
        if success:
            logging.info(f"{index_name} ì¸ë±ì‹±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logging.error(f"{index_name} ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logging.error(traceback.format_exc())


if __name__ == "__main__":
    import sys
    import argparse
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='CONCEPT_RELATIONSHIP ë°ì´í„° ì¸ë±ì‹±')
    parser.add_argument('--resume', action='store_true', help='ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì¸ë±ì‹± ì¬ì‹œì‘')
    
    args = parser.parse_args()
    
    # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    main(resume=args.resume)

