"""
6ê°€ì§€ Ablation í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

í…ŒìŠ¤íŠ¸ ì¡°ê±´:
1. Stage1 (Semantic + Combined) + Stage3 LLM (ì ìˆ˜ í¬í•¨)
2. Stage1 (Semantic + Combined) + Stage3 LLM (ì ìˆ˜ ë¯¸í¬í•¨)
3. Stage1 (Semantic + Combined) + Stage3 Semantic Only
4. Stage1 (Lexical + Semantic + Combined) + Stage3 LLM (ì ìˆ˜ í¬í•¨)
5. Stage1 (Lexical + Semantic + Combined) + Stage3 LLM (ì ìˆ˜ ë¯¸í¬í•¨)
6. Stage1 (Lexical + Semantic + Combined) + Stage3 Semantic Only
"""
import pandas as pd
import logging
import os
from datetime import datetime
from pathlib import Path
import sys
import openpyxl
from openpyxl.styles import Font, Alignment, PatternFill
from tqdm import tqdm
import time
import json

# ìƒëŒ€ ê²½ë¡œë¡œ src ë””ë ‰í† ë¦¬ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / 'src'))

from MapOMOP.entity_mapping_api import EntityMappingAPI, EntityInput, DomainID
from MapOMOP.elasticsearch_client import ElasticsearchClient


# 6ê°€ì§€ í…ŒìŠ¤íŠ¸ ì¡°ê±´ ì •ì˜
TEST_CONDITIONS = [
    {
        'name': 'semantic_combined_llm_with_scores',
        'description': 'Stage1 (Semantic + Combined) + Stage3 LLM (ì ìˆ˜ í¬í•¨)',
        'use_lexical': False,
        'scoring_mode': 'llm',
        'include_stage1_scores': True
    },
    {
        'name': 'semantic_combined_llm_no_scores',
        'description': 'Stage1 (Semantic + Combined) + Stage3 LLM (ì ìˆ˜ ë¯¸í¬í•¨)',
        'use_lexical': False,
        'scoring_mode': 'llm',
        'include_stage1_scores': False
    },
    {
        'name': 'semantic_combined_semantic_only',
        'description': 'Stage1 (Semantic + Combined) + Stage3 Semantic Only',
        'use_lexical': False,
        'scoring_mode': 'semantic_only',
        'include_stage1_scores': False
    },
    {
        'name': 'full_search_llm_with_scores',
        'description': 'Stage1 (Lexical + Semantic + Combined) + Stage3 LLM (ì ìˆ˜ í¬í•¨)',
        'use_lexical': True,
        'scoring_mode': 'llm',
        'include_stage1_scores': True
    },
    {
        'name': 'full_search_llm_no_scores',
        'description': 'Stage1 (Lexical + Semantic + Combined) + Stage3 LLM (ì ìˆ˜ ë¯¸í¬í•¨)',
        'use_lexical': True,
        'scoring_mode': 'llm',
        'include_stage1_scores': False
    },
    {
        'name': 'full_search_semantic_only',
        'description': 'Stage1 (Lexical + Semantic + Combined) + Stage3 Semantic Only',
        'use_lexical': True,
        'scoring_mode': 'semantic_only',
        'include_stage1_scores': False
    }
]


class AblationTester:
    """6ê°€ì§€ Ablation í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, log_dir: str = "test_logs_ablation"):
        """í…ŒìŠ¤í„° ì´ˆê¸°í™”"""
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        self.setup_logging()
        
        self.es_client = ElasticsearchClient()
        self.es_client.concept_index = "concept-small"
        self.es_client.concept_synonym_index = "concept-synonym"
        
        self.domain_mapping = {
            'Condition': DomainID.CONDITION,
            'Procedure': DomainID.PROCEDURE,
            'Drug': DomainID.DRUG,
            'Observation': DomainID.OBSERVATION,
            'Measurement': DomainID.MEASUREMENT,
            'Device': DomainID.DEVICE,
        }
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        log_file = self.log_dir / f"ablation_test_{timestamp}.log"
        
        self.logger = logging.getLogger('ablation_test')
        self.logger.setLevel(logging.INFO)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        self.logger.handlers.clear()
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")
    
    def load_and_sample_data(
        self, 
        csv_path: str, 
        sample_size: int = 1000, 
        random_state: int = 42
    ) -> pd.DataFrame:
        """
        CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë”© ë° ëœë¤ ìƒ˜í”Œë§
        
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            sample_size: ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ê°’: 1000)
            random_state: ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42) - ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©
        """
        self.logger.info(f"ë°ì´í„° ë¡œë”© ì‹œì‘: {csv_path}")
        self.logger.info(f"ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ")
        self.logger.info(f"ëœë¤ ì‹œë“œ: {random_state} (ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©)")
        
        # ì „ì²´ ë°ì´í„° ë¡œë“œ
        chunk_size = 100000
        chunks = []
        
        self.logger.info("ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì½ëŠ” ì¤‘...")
        for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunk_size), desc="ë°ì´í„° ë¡œë”©"):
            chunks.append(chunk)
        
        # ì „ì²´ ë°ì´í„° ë³‘í•©
        df = pd.concat(chunks, ignore_index=True)
        self.logger.info(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {len(df):,}ê°œ")
        
        # ëœë¤ ìƒ˜í”Œë§ (ê³ ì •ëœ ì‹œë“œ ì‚¬ìš©)
        df_sample = df.sample(n=min(sample_size, len(df)), random_state=random_state)
        df_sample = df_sample.reset_index(drop=True)
        self.logger.info(f"ëœë¤ ìƒ˜í”Œë§ ì™„ë£Œ: {len(df_sample):,}ê°œ (seed={random_state})")
        
        # ë„ë©”ì¸ ë¶„í¬ ì¶œë ¥
        if 'domain_id' in df_sample.columns:
            domain_dist = df_sample['domain_id'].value_counts()
            self.logger.info("\në„ë©”ì¸ ë¶„í¬:")
            for domain, count in domain_dist.items():
                self.logger.info(f"  {domain}: {count}ê°œ ({count/len(df_sample)*100:.1f}%)")
        
        return df_sample
    
    def create_entity_input(self, row) -> EntityInput:
        """DataFrame í–‰ì—ì„œ EntityInput ìƒì„±"""
        entity_name = str(row['source_value']).strip()
        
        # ë„ë©”ì¸ ì •ë³´ ì‚¬ìš© (CSV ì»¬ëŸ¼ëª…: domain_id)
        domain_id = None
        if 'domain_id' in row and pd.notna(row['domain_id']):
            domain_str = str(row['domain_id']).strip()
            if domain_str and domain_str in self.domain_mapping:
                domain_id = self.domain_mapping[domain_str]
        
        return EntityInput(
            entity_name=entity_name,
            domain_id=domain_id,
            vocabulary_id=None
        )
    
    def test_single_entity(
        self, 
        api: EntityMappingAPI,
        entity_input: EntityInput, 
        ground_truth_concept_id: int
    ) -> dict:
        """ë‹¨ì¼ ì—”í‹°í‹° í…ŒìŠ¤íŠ¸"""
        try:
            # ë§¤í•‘ ìˆ˜í–‰
            results = api.map_entity(entity_input)
            
            # ë‹¨ê³„ë³„ í›„ë³´êµ° ìˆ˜ì§‘
            stage1_candidates = []
            stage2_candidates = []
            stage3_candidates = []
            
            if hasattr(api, '_last_stage1_candidates') and api._last_stage1_candidates:
                stage1_candidates = api._last_stage1_candidates
            
            if hasattr(api, '_last_stage2_candidates') and api._last_stage2_candidates:
                stage2_candidates = api._last_stage2_candidates
            
            if hasattr(api, '_last_rerank_candidates') and api._last_rerank_candidates:
                stage3_candidates = api._last_rerank_candidates
            
            # ìµœê³  ì ìˆ˜ ê²°ê³¼ ì„ íƒ
            best_result = max(results, key=lambda x: x.mapping_score) if results else None
            
            # ë§¤í•‘ ì„±ê³µ ì—¬ë¶€ íŒë‹¨ (concept_id ì¼ì¹˜)
            mapping_correct = False
            if best_result and ground_truth_concept_id:
                try:
                    best_concept_id_int = int(best_result.mapped_concept_id)
                    ground_truth_int = int(ground_truth_concept_id)
                    mapping_correct = (best_concept_id_int == ground_truth_int)
                except (ValueError, TypeError):
                    mapping_correct = False
            
            return {
                'entity_name': entity_input.entity_name,
                'input_domain': entity_input.domain_id.value if entity_input.domain_id else 'All',
                'ground_truth_concept_id': ground_truth_concept_id,
                'success': results is not None and len(results) > 0,
                'mapping_correct': mapping_correct,
                'best_concept_id': best_result.mapped_concept_id if best_result else None,
                'best_concept_name': best_result.mapped_concept_name if best_result else None,
                'best_score': best_result.mapping_score if best_result else 0.0,
                'best_confidence': best_result.mapping_confidence if best_result else None,
                'stage1_candidates': stage1_candidates,
                'stage2_candidates': stage2_candidates,
                'stage3_candidates': stage3_candidates
            }
            
        except Exception as e:
            return {
                'entity_name': entity_input.entity_name,
                'input_domain': entity_input.domain_id.value if entity_input.domain_id else 'All',
                'ground_truth_concept_id': ground_truth_concept_id,
                'success': False,
                'mapping_correct': False,
                'best_concept_id': None,
                'best_concept_name': None,
                'best_score': 0.0,
                'best_confidence': None,
                'error': str(e),
                'stage1_candidates': [],
                'stage2_candidates': [],
                'stage3_candidates': []
            }
    
    def run_single_condition_test(
        self, 
        condition: dict, 
        test_data: pd.DataFrame
    ) -> dict:
        """ë‹¨ì¼ ì¡°ê±´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        condition_name = condition['name']
        
        self.logger.info("\n" + "=" * 100)
        self.logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì¡°ê±´: {condition['description']}")
        self.logger.info(f"   - use_lexical: {condition['use_lexical']}")
        self.logger.info(f"   - scoring_mode: {condition['scoring_mode']}")
        self.logger.info(f"   - include_stage1_scores: {condition['include_stage1_scores']}")
        self.logger.info("=" * 100)
        
        start_time = time.time()
        
        # API ì´ˆê¸°í™” (ì¡°ê±´ì— ë§ê²Œ)
        api = EntityMappingAPI(
            es_client=self.es_client,
            scoring_mode=condition['scoring_mode'],
            include_stage1_scores=condition['include_stage1_scores'],
            use_lexical=condition['use_lexical']
        )
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = []
        successful_mappings = 0
        correct_mappings = 0
        
        for idx, row in tqdm(test_data.iterrows(), total=len(test_data), 
                           desc=f"[{condition_name}]"):
            try:
                entity_input = self.create_entity_input(row)
                ground_truth = int(row['concept_id']) if pd.notna(row['concept_id']) else None
                
                result = self.test_single_entity(api, entity_input, ground_truth)
                test_results.append(result)
                
                if result['success']:
                    successful_mappings += 1
                    if result['mapping_correct']:
                        correct_mappings += 1
                        
            except Exception as e:
                self.logger.error(f"í…ŒìŠ¤íŠ¸ #{idx + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # ê²°ê³¼ ìš”ì•½
        total_tests = len(test_results)
        success_rate = (successful_mappings / total_tests * 100) if total_tests > 0 else 0
        accuracy = (correct_mappings / total_tests * 100) if total_tests > 0 else 0
        
        summary = {
            'condition_name': condition_name,
            'description': condition['description'],
            'use_lexical': condition['use_lexical'],
            'scoring_mode': condition['scoring_mode'],
            'include_stage1_scores': condition['include_stage1_scores'],
            'total_tests': total_tests,
            'successful_mappings': successful_mappings,
            'correct_mappings': correct_mappings,
            'success_rate': success_rate,
            'accuracy': accuracy,
            'elapsed_time': elapsed_time,
            'avg_time_per_entity': elapsed_time / total_tests if total_tests > 0 else 0,
            'results': test_results
        }
        
        self.logger.info(f"\nğŸ“Š [{condition_name}] ê²°ê³¼ ìš”ì•½:")
        self.logger.info(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests:,}ê°œ")
        self.logger.info(f"   ë§¤í•‘ ì„±ê³µ: {successful_mappings:,}ê°œ ({success_rate:.2f}%)")
        self.logger.info(f"   ì •ë‹µ ë§¤ì¹­: {correct_mappings:,}ê°œ ({accuracy:.2f}%)")
        self.logger.info(f"   ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.2f}ë¶„)")
        
        return summary
    
    def run_all_conditions(
        self, 
        csv_path: str, 
        sample_size: int = 1000, 
        random_state: int = 42,
        conditions: list = None
    ):
        """ëª¨ë“  ì¡°ê±´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("=" * 100)
        self.logger.info("ğŸš€ Ablation Study í…ŒìŠ¤íŠ¸ ì‹œì‘")
        self.logger.info("=" * 100)
        
        # ë°ì´í„° ë¡œë”© (í•œ ë²ˆë§Œ ë¡œë”©í•˜ì—¬ ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©)
        test_data = self.load_and_sample_data(csv_path, sample_size, random_state)
        
        # í…ŒìŠ¤íŠ¸í•  ì¡°ê±´ ì„ íƒ
        if conditions is None:
            conditions = TEST_CONDITIONS
        
        self.logger.info(f"\nì´ {len(conditions)}ê°œ ì¡°ê±´ í…ŒìŠ¤íŠ¸ ì˜ˆì •:")
        for i, cond in enumerate(conditions, 1):
            self.logger.info(f"  {i}. {cond['description']}")
        
        # ê° ì¡°ê±´ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        all_summaries = []
        total_start_time = time.time()
        
        for i, condition in enumerate(conditions, 1):
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"[{i}/{len(conditions)}] í…ŒìŠ¤íŠ¸ ì‹œì‘: {condition['name']}")
            self.logger.info(f"{'='*50}")
            
            summary = self.run_single_condition_test(condition, test_data)
            all_summaries.append(summary)
        
        total_elapsed_time = time.time() - total_start_time
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.logger.info("\n" + "=" * 100)
        self.logger.info("ğŸ“Š ì „ì²´ Ablation Study ê²°ê³¼ ìš”ì•½")
        self.logger.info("=" * 100)
        self.logger.info(f"{'ì¡°ê±´ëª…':<45} {'Accuracy':>10} {'Success Rate':>12} {'ì‹œê°„(ì´ˆ)':>10}")
        self.logger.info("-" * 80)
        
        for summary in all_summaries:
            self.logger.info(
                f"{summary['description']:<45} "
                f"{summary['accuracy']:>9.2f}% "
                f"{summary['success_rate']:>11.2f}% "
                f"{summary['elapsed_time']:>9.1f}s"
            )
        
        self.logger.info("-" * 80)
        self.logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ ({total_elapsed_time/60:.2f}ë¶„)")
        self.logger.info("=" * 100)
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(all_summaries, random_state)
        
        return all_summaries
    
    def save_results(self, all_summaries: list, random_state: int):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ìš”ì•½ CSV ì €ì¥
        summary_data = []
        for summary in all_summaries:
            summary_data.append({
                'condition_name': summary['condition_name'],
                'description': summary['description'],
                'use_lexical': summary['use_lexical'],
                'scoring_mode': summary['scoring_mode'],
                'include_stage1_scores': summary['include_stage1_scores'],
                'total_tests': summary['total_tests'],
                'successful_mappings': summary['successful_mappings'],
                'correct_mappings': summary['correct_mappings'],
                'success_rate': summary['success_rate'],
                'accuracy': summary['accuracy'],
                'elapsed_time': summary['elapsed_time'],
                'avg_time_per_entity': summary['avg_time_per_entity']
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_csv = self.log_dir / f"ablation_summary_{timestamp}.csv"
        summary_df.to_csv(summary_csv, index=False, encoding='utf-8')
        self.logger.info(f"ğŸ“„ ìš”ì•½ CSV ì €ì¥: {summary_csv}")
        
        # 2. ìƒì„¸ ê²°ê³¼ XLSX ì €ì¥
        xlsx_file = self.log_dir / f"ablation_detailed_{timestamp}.xlsx"
        
        wb = openpyxl.Workbook()
        
        # ìš”ì•½ ì‹œíŠ¸
        ws_summary = wb.active
        ws_summary.title = "Summary"
        self._create_summary_sheet(ws_summary, all_summaries)
        
        # ê° ì¡°ê±´ë³„ ìƒì„¸ ì‹œíŠ¸
        for summary in all_summaries:
            ws = wb.create_sheet(title=summary['condition_name'][:31])  # ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
            self._create_detail_sheet(ws, summary)
        
        wb.save(xlsx_file)
        self.logger.info(f"ğŸ“Š ìƒì„¸ XLSX ì €ì¥: {xlsx_file}")
        
        # 3. JSONìœ¼ë¡œ ì „ì²´ ê²°ê³¼ ì €ì¥
        json_file = self.log_dir / f"ablation_results_{timestamp}.json"
        json_data = {
            'timestamp': timestamp,
            'random_state': random_state,
            'summaries': [
                {k: v for k, v in s.items() if k != 'results'}
                for s in all_summaries
            ]
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ğŸ“ JSON ì €ì¥: {json_file}")
    
    def _create_summary_sheet(self, ws, all_summaries):
        """ìš”ì•½ ì‹œíŠ¸ ìƒì„±"""
        headers = [
            "ì¡°ê±´ëª…", "ì„¤ëª…", "use_lexical", "scoring_mode", "include_scores",
            "ì´ í…ŒìŠ¤íŠ¸", "ë§¤í•‘ ì„±ê³µ", "ì •ë‹µ ë§¤ì¹­", "Success Rate (%)", "Accuracy (%)",
            "ì†Œìš”ì‹œê°„(ì´ˆ)", "í‰ê· ì‹œê°„(ì´ˆ/ì—”í‹°í‹°)"
        ]
        
        # í—¤ë” ìŠ¤íƒ€ì¼
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center")
        
        # ë°ì´í„° ì‘ì„±
        for row, summary in enumerate(all_summaries, 2):
            ws.cell(row=row, column=1, value=summary['condition_name'])
            ws.cell(row=row, column=2, value=summary['description'])
            ws.cell(row=row, column=3, value=str(summary['use_lexical']))
            ws.cell(row=row, column=4, value=summary['scoring_mode'])
            ws.cell(row=row, column=5, value=str(summary['include_stage1_scores']))
            ws.cell(row=row, column=6, value=summary['total_tests'])
            ws.cell(row=row, column=7, value=summary['successful_mappings'])
            ws.cell(row=row, column=8, value=summary['correct_mappings'])
            ws.cell(row=row, column=9, value=round(summary['success_rate'], 2))
            ws.cell(row=row, column=10, value=round(summary['accuracy'], 2))
            ws.cell(row=row, column=11, value=round(summary['elapsed_time'], 2))
            ws.cell(row=row, column=12, value=round(summary['avg_time_per_entity'], 4))
            
            # Accuracy ì»¬ëŸ¬ë§
            accuracy_cell = ws.cell(row=row, column=10)
            if summary['accuracy'] >= 80:
                accuracy_cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
            elif summary['accuracy'] >= 60:
                accuracy_cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
            else:
                accuracy_cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        
        # ì—´ ë„ˆë¹„ ì„¤ì •
        column_widths = [35, 55, 12, 15, 15, 10, 12, 12, 15, 12, 12, 18]
        for i, width in enumerate(column_widths, 1):
            ws.column_dimensions[openpyxl.utils.get_column_letter(i)].width = width
    
    def _create_detail_sheet(self, ws, summary):
        """ìƒì„¸ ê²°ê³¼ ì‹œíŠ¸ ìƒì„±"""
        headers = [
            "Entity Name", "Domain", "Ground Truth ID", "Success", "Correct",
            "Best Concept ID", "Best Concept Name", "Score", "Confidence",
            "Stage1 Candidates", "Stage2 Candidates", "Stage3 Candidates"
        ]
        
        # í—¤ë” ìŠ¤íƒ€ì¼
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # ë°ì´í„° ì‘ì„±
        for row, result in enumerate(summary['results'], 2):
            ws.cell(row=row, column=1, value=result.get('entity_name', 'N/A'))
            ws.cell(row=row, column=2, value=result.get('input_domain', 'N/A'))
            ws.cell(row=row, column=3, value=result.get('ground_truth_concept_id', 'N/A'))
            ws.cell(row=row, column=4, value="ì„±ê³µ" if result.get('success') else "ì‹¤íŒ¨")
            
            # ì •ë‹µ ì—¬ë¶€ ì»¬ëŸ¬ë§
            correct_cell = ws.cell(row=row, column=5, value="ì •ë‹µ" if result.get('mapping_correct') else "ì˜¤ë‹µ")
            if result.get('mapping_correct'):
                correct_cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
            else:
                correct_cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            
            ws.cell(row=row, column=6, value=result.get('best_concept_id', 'N/A'))
            ws.cell(row=row, column=7, value=result.get('best_concept_name', 'N/A'))
            ws.cell(row=row, column=8, value=result.get('best_score', 0.0))
            ws.cell(row=row, column=9, value=result.get('best_confidence', 'N/A'))
            
            # Stageë³„ í›„ë³´êµ° ì¶”ê°€
            stage1_text = self._format_candidates_for_cell(result.get('stage1_candidates', []), 'stage1')
            ws.cell(row=row, column=10, value=stage1_text)
            
            stage2_text = self._format_candidates_for_cell(result.get('stage2_candidates', []), 'stage2')
            ws.cell(row=row, column=11, value=stage2_text)
            
            stage3_text = self._format_candidates_for_cell(result.get('stage3_candidates', []), 'stage3', summary.get('scoring_mode', 'llm'))
            ws.cell(row=row, column=12, value=stage3_text)
            
            # Stage ì»¬ëŸ¼ ìŠ¤íƒ€ì¼ ì„¤ì •
            for col in range(10, 13):
                cell = ws.cell(row=row, column=col)
                cell.alignment = Alignment(wrap_text=True, vertical='top')
        
        # ì—´ ë„ˆë¹„ ì„¤ì •
        column_widths = [45, 15, 18, 10, 10, 18, 50, 10, 12, 70, 70, 85]
        for i, width in enumerate(column_widths, 1):
            ws.column_dimensions[openpyxl.utils.get_column_letter(i)].width = width
        
        # í–‰ ë†’ì´ ì„¤ì • (Stage í›„ë³´êµ° í‘œì‹œë¥¼ ìœ„í•´)
        for row_num in range(2, len(summary['results']) + 2):
            ws.row_dimensions[row_num].height = 150
    
    def _format_candidates_for_cell(self, candidates, stage_type, scoring_mode='llm'):
        """í›„ë³´êµ° í¬ë§·íŒ… (ì—‘ì…€ ì…€ìš©)"""
        if not candidates:
            return "í›„ë³´ ì—†ìŒ"
        
        lines = []
        max_candidates = 15 if stage_type in ['stage1', 'stage2'] else 10
        
        for i, candidate in enumerate(candidates[:max_candidates], 1):
            if stage_type == 'stage1':
                # Stage 1: Elasticsearch ê²°ê³¼
                search_type = candidate.get('search_type', 'unknown')
                line = f"{i}. [{search_type}] {candidate.get('concept_name', 'N/A')} (ID: {candidate.get('concept_id', 'N/A')})\n"
                line += f"   ESì ìˆ˜: {candidate.get('elasticsearch_score', 0):.4f}, "
                line += f"Standard: {candidate.get('standard_concept', 'N/A')}, "
                line += f"Domain: {candidate.get('domain_id', 'N/A')}"
            
            elif stage_type == 'stage2':
                # Stage 2: Standard ë³€í™˜ ê²°ê³¼
                search_type = candidate.get('search_type', 'unknown')
                is_std = "âœ“" if candidate.get('is_original_standard', True) else "â†’"
                line = f"{i}. [{search_type}] {is_std} {candidate.get('concept_name', 'N/A')} (ID: {candidate.get('concept_id', 'N/A')})\n"
                line += f"   Standard: {candidate.get('standard_concept', 'N/A')}, "
                line += f"Domain: {candidate.get('domain_id', 'N/A')}"
                if not candidate.get('is_original_standard', True):
                    original_non_std = candidate.get('original_non_standard', {})
                    if original_non_std:
                        line += f"\n   ì›ë³¸ Non-std: {original_non_std.get('concept_name', 'N/A')} (ID: {original_non_std.get('concept_id', 'N/A')})"
            
            else:  # stage3
                # Stage 3: LLM ë˜ëŠ” Semantic Only ê²°ê³¼
                search_type = candidate.get('search_type', 'unknown')
                line = f"{i}. [{search_type}] {candidate.get('concept_name', 'N/A')} (ID: {candidate.get('concept_id', 'N/A')})\n"
                
                # LLM ëª¨ë“œì¸ ê²½ìš°
                llm_score = candidate.get('llm_score')
                llm_rank = candidate.get('llm_rank')
                llm_reasoning = candidate.get('llm_reasoning')
                semantic_similarity = candidate.get('semantic_similarity')
                
                if llm_score is not None:
                    line += f"   LLMì ìˆ˜: {llm_score}, ìˆœìœ„: {llm_rank}"
                    if semantic_similarity is not None:
                        line += f" | ì˜ë¯¸ìœ ì‚¬ë„: {semantic_similarity:.4f}"
                    line += "\n"
                    if llm_reasoning:
                        reasoning_short = llm_reasoning[:60] + '...' if len(llm_reasoning) > 60 else llm_reasoning
                        line += f"   ì´ìœ : {reasoning_short}\n"
                else:
                    # Semantic Only ë˜ëŠ” Hybrid ëª¨ë“œì¸ ê²½ìš°
                    text_sim = candidate.get('text_similarity', 0)
                    sem_sim = candidate.get('semantic_similarity', 0)
                    final_score = candidate.get('final_score', 0)
                    line += f"   í…ìŠ¤íŠ¸: {text_sim:.4f}, ì˜ë¯¸ì : {sem_sim:.4f}, ìµœì¢…: {final_score:.4f}\n"
                
                line += f"   Standard: {candidate.get('standard_concept', 'N/A')}, "
                line += f"Domain: {candidate.get('domain_id', 'N/A')}"
            
            lines.append(line)
        
        return "\n\n".join(lines)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ============================================================
    # ì„¤ì •
    # ============================================================
    CSV_PATH = "/home/work/skku/hyo/omop-mapper/data/mapomop_test_data.csv"
    SAMPLE_SIZE = 4129  # ìƒ˜í”Œ í¬ê¸°
    RANDOM_STATE = 42   # ëœë¤ ì‹œë“œ (ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©)
    
    # í…ŒìŠ¤íŠ¸í•  ì¡°ê±´ ì„ íƒ (Noneì´ë©´ ëª¨ë“  ì¡°ê±´ í…ŒìŠ¤íŠ¸)
    # íŠ¹ì • ì¡°ê±´ë§Œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ì¸ë±ìŠ¤ ì§€ì •
    # ì˜ˆ: [TEST_CONDITIONS[0], TEST_CONDITIONS[3]]  # 1ë²ˆ, 4ë²ˆ ì¡°ê±´ë§Œ
    CONDITIONS_TO_TEST = [TEST_CONDITIONS[4]]  # ëª¨ë“  6ê°€ì§€ ì¡°ê±´ í…ŒìŠ¤íŠ¸
    
    # ============================================================
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # ============================================================
    tester = AblationTester()
    
    results = tester.run_all_conditions(
        csv_path=CSV_PATH,
        sample_size=SAMPLE_SIZE,
        random_state=RANDOM_STATE,
        conditions=CONDITIONS_TO_TEST
    )
    
    print(f"\nâœ… Ablation Study ì™„ë£Œ!")
    print(f"   ê²°ê³¼ëŠ” {tester.log_dir} ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()