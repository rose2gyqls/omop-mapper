"""
SapBERTë¥¼ ì‚¬ìš©í•œ ì˜ë£Œ ì—”í‹°í‹° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ë° ì‹œê°í™”
"""

import os
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import umap
from transformers import AutoTokenizer, AutoModel
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

class SapBERTEmbeddingTester:
    """SapBERT ì„ë² ë”© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name="cambridgeltl/SapBERT-from-PubMedBERT-fulltext"):
        """
        SapBERT ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  SapBERT ëª¨ë¸ëª…
        """
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        print("SapBERT ëª¨ë¸ì„ ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
    def load_concept_data(self, csv_path, sample_size=1000, random_state=42):
        """
        CONCEPT.csv íŒŒì¼ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
        
        Args:
            csv_path: CONCEPT.csv íŒŒì¼ ê²½ë¡œ
            sample_size: ìƒ˜í”Œë§í•  ë°ì´í„° ê°œìˆ˜
            random_state: ëœë¤ ì‹œë“œ
        
        Returns:
            pandas.DataFrame: ìƒ˜í”Œë§ëœ concept ë°ì´í„°
        """
        print(f"CONCEPT.csv íŒŒì¼ì„ ë¡œë”© ì¤‘... (ìƒ˜í”Œ í¬ê¸°: {sample_size})")
        
        # ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        total_lines = sum(1 for _ in open(csv_path)) - 1  # í—¤ë” ì œì™¸
        print(f"ì „ì²´ concept ê°œìˆ˜: {total_lines:,}")
        
        # ëœë¤ ìƒ˜í”Œë§ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        np.random.seed(random_state)
        skip_idx = sorted(np.random.choice(range(1, total_lines + 1), 
                                         size=total_lines - sample_size, 
                                         replace=False))
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_path, sep='\t', skiprows=skip_idx)
        
        # ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬
        df = df.dropna(subset=['concept_name'])
        df['concept_name'] = df['concept_name'].astype(str).str.strip()
        df = df[df['concept_name'] != '']
        
        print(f"ë¡œë”©ëœ ìƒ˜í”Œ ë°ì´í„°: {len(df):,}ê°œ")
        print(f"ë„ë©”ì¸ ë¶„í¬:\n{df['domain_id'].value_counts()}")
        
        return df
    
    def get_embeddings(self, texts, batch_size=32, max_length=128):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ SapBERT ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
        
        Returns:
            numpy.ndarray: ì„ë² ë”© ë²¡í„°ë“¤
        """
        all_embeddings = []
        
        print(f"{len(texts)}ê°œ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„± ì¤‘...")
        
        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size)):
                batch_texts = texts[i:i+batch_size]
                
                # í† í¬ë‚˜ì´ì§•
                encoded = self.tokenizer.batch_encode_plus(
                    batch_texts,
                    padding="max_length",
                    max_length=max_length,
                    truncation=True,
                    return_tensors="pt"
                )
                
                # GPUë¡œ ì´ë™
                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)
                
                # ì„ë² ë”© ìƒì„± (CLS í† í° ì‚¬ìš©)
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°
                
                all_embeddings.append(cls_embeddings.cpu().numpy())
        
        embeddings = np.concatenate(all_embeddings, axis=0)
        print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ! Shape: {embeddings.shape}")
        
        return embeddings
    
    def analyze_similarity(self, df, embeddings, top_k=5):
        """
        ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ì„± ë¶„ì„
        
        Args:
            df: concept ë°ì´í„°í”„ë ˆì„
            embeddings: ì„ë² ë”© ë²¡í„°ë“¤
            top_k: ìƒìœ„ kê°œ ìœ ì‚¬ ì—”í‹°í‹° í‘œì‹œ
        
        Returns:
            dict: ìœ ì‚¬ì„± ë¶„ì„ ê²°ê³¼
        """
        print("ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ê³„ì‚° ì¤‘...")
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)
        
        # ê° ë„ë©”ì¸ë³„ í‰ê·  ìœ ì‚¬ì„± ê³„ì‚°
        domain_similarities = {}
        for domain in df['domain_id'].unique():
            domain_mask = df['domain_id'] == domain
            domain_indices = np.where(domain_mask)[0]
            
            if len(domain_indices) > 1:
                domain_sim_matrix = similarity_matrix[np.ix_(domain_indices, domain_indices)]
                # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ì„± ì œì™¸)
                mask = ~np.eye(domain_sim_matrix.shape[0], dtype=bool)
                avg_similarity = domain_sim_matrix[mask].mean()
                domain_similarities[domain] = avg_similarity
        
        # ì „ì²´ í‰ê·  ìœ ì‚¬ì„± (ëŒ€ê°ì„  ì œì™¸)
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        overall_avg_similarity = similarity_matrix[mask].mean()
        
        # ëª‡ ê°€ì§€ ì˜ˆì‹œ conceptì— ëŒ€í•œ ìœ ì‚¬í•œ ì—”í‹°í‹° ì°¾ê¸°
        sample_examples = []
        sample_indices = np.random.choice(len(df), size=min(5, len(df)), replace=False)
        
        for idx in sample_indices:
            concept_name = df.iloc[idx]['concept_name']
            domain = df.iloc[idx]['domain_id']
            
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ê°€ì¥ ìœ ì‚¬í•œ ì—”í‹°í‹°ë“¤ ì°¾ê¸°
            similarities = similarity_matrix[idx]
            similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # ìê¸° ìì‹  ì œì™¸
            
            similar_entities = []
            for sim_idx in similar_indices:
                similar_entities.append({
                    'name': df.iloc[sim_idx]['concept_name'],
                    'domain': df.iloc[sim_idx]['domain_id'],
                    'similarity': similarities[sim_idx]
                })
            
            sample_examples.append({
                'original': {'name': concept_name, 'domain': domain},
                'similar': similar_entities
            })
        
        results = {
            'overall_avg_similarity': overall_avg_similarity,
            'domain_similarities': domain_similarities,
            'sample_examples': sample_examples,
            'similarity_matrix': similarity_matrix
        }
        
        print(f"ì „ì²´ í‰ê·  ìœ ì‚¬ì„±: {overall_avg_similarity:.4f}")
        print("ë„ë©”ì¸ë³„ í‰ê·  ìœ ì‚¬ì„±:")
        for domain, sim in sorted(domain_similarities.items(), key=lambda x: x[1], reverse=True):
            print(f"  {domain}: {sim:.4f}")
        
        return results
    
    def visualize_embeddings_2d(self, df, embeddings, method='umap', save_path=None):
        """
        2D ì„ë² ë”© ì‹œê°í™” (UMAP ë˜ëŠ” t-SNE)
        
        Args:
            df: concept ë°ì´í„°í”„ë ˆì„
            embeddings: ì„ë² ë”© ë²¡í„°ë“¤
            method: ì°¨ì› ì¶•ì†Œ ë°©ë²• ('umap' ë˜ëŠ” 'tsne')
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        print(f"{method.upper()}ë¥¼ ì‚¬ìš©í•œ 2D ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì°¨ì› ì¶•ì†Œ
        if method.lower() == 'umap':
            reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)
        elif method.lower() == 'tsne':
            reducer = TSNE(n_components=2, random_state=42, perplexity=30)
        else:
            raise ValueError("methodëŠ” 'umap' ë˜ëŠ” 'tsne'ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        embeddings_2d = reducer.fit_transform(embeddings)
        
        # ì‹œê°í™” ë°ì´í„° ì¤€ë¹„
        viz_df = df.copy()
        viz_df['x'] = embeddings_2d[:, 0]
        viz_df['y'] = embeddings_2d[:, 1]
        
        # Plotly ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
        fig = px.scatter(
            viz_df, 
            x='x', 
            y='y', 
            color='domain_id',
            hover_data=['concept_name', 'vocabulary_id'],
            title=f'SapBERT ì˜ë£Œ ì—”í‹°í‹° ì„ë² ë”© ì‹œê°í™” ({method.upper()})',
            width=1000,
            height=700
        )
        
        fig.update_traces(marker=dict(size=5, opacity=0.7))
        fig.update_layout(
            xaxis_title=f'{method.upper()} 1',
            yaxis_title=f'{method.upper()} 2',
            legend_title='ë„ë©”ì¸',
            font=dict(size=12)
        )
        
        if save_path:
            fig.write_html(save_path)
            print(f"ì‹œê°í™” ì €ì¥ë¨: {save_path}")
        
        fig.show()
        
        return embeddings_2d, fig
    
    def visualize_similarity_heatmap(self, df, similarity_matrix, sample_size=50, save_path=None):
        """
        ìœ ì‚¬ì„± íˆíŠ¸ë§µ ì‹œê°í™”
        
        Args:
            df: concept ë°ì´í„°í”„ë ˆì„
            similarity_matrix: ìœ ì‚¬ì„± ë§¤íŠ¸ë¦­ìŠ¤
            sample_size: íˆíŠ¸ë§µì— í‘œì‹œí•  ìƒ˜í”Œ í¬ê¸°
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        print(f"ìœ ì‚¬ì„± íˆíŠ¸ë§µ ìƒì„± ì¤‘... (ìƒ˜í”Œ í¬ê¸°: {sample_size})")
        
        # ëœë¤ ìƒ˜í”Œë§
        sample_indices = np.random.choice(len(df), size=min(sample_size, len(df)), replace=False)
        sample_df = df.iloc[sample_indices].copy()
        sample_similarity = similarity_matrix[np.ix_(sample_indices, sample_indices)]
        
        # ë¼ë²¨ ìƒì„± (concept_nameì„ ì§§ê²Œ)
        labels = [name[:30] + '...' if len(name) > 30 else name 
                 for name in sample_df['concept_name'].values]
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            sample_similarity,
            xticklabels=labels,
            yticklabels=labels,
            cmap='viridis',
            center=0.5,
            square=True,
            cbar_kws={'label': 'Cosine Similarity'}
        )
        
        plt.title('SapBERT ì˜ë£Œ ì—”í‹°í‹° ìœ ì‚¬ì„± íˆíŠ¸ë§µ', fontsize=16, pad=20)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"íˆíŠ¸ë§µ ì €ì¥ë¨: {save_path}")
        
        plt.show()
    
    def cluster_analysis(self, df, embeddings, n_clusters=8, save_path=None):
        """
        í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        
        Args:
            df: concept ë°ì´í„°í”„ë ˆì„
            embeddings: ì„ë² ë”© ë²¡í„°ë“¤
            n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
            save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        print(f"K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘... (í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {n_clusters})")
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë„ë©”ì¸ ë¶„í¬ ë¶„ì„
        cluster_df = df.copy()
        cluster_df['cluster'] = cluster_labels
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
        cluster_stats = []
        for cluster_id in range(n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_data = df[cluster_mask]
            
            domain_counts = cluster_data['domain_id'].value_counts()
            vocab_counts = cluster_data['vocabulary_id'].value_counts()
            
            cluster_stats.append({
                'cluster_id': cluster_id,
                'size': cluster_mask.sum(),
                'top_domain': domain_counts.index[0] if len(domain_counts) > 0 else 'N/A',
                'top_domain_ratio': domain_counts.iloc[0] / len(cluster_data) if len(domain_counts) > 0 else 0,
                'top_vocab': vocab_counts.index[0] if len(vocab_counts) > 0 else 'N/A',
                'sample_concepts': cluster_data['concept_name'].head(3).tolist()
            })
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„ ì¶œë ¥
        print("\ní´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼:")
        for stat in cluster_stats:
            print(f"í´ëŸ¬ìŠ¤í„° {stat['cluster_id']} (í¬ê¸°: {stat['size']}):")
            print(f"  ì£¼ìš” ë„ë©”ì¸: {stat['top_domain']} ({stat['top_domain_ratio']:.2%})")
            print(f"  ì£¼ìš” ì–´íœ˜: {stat['top_vocab']}")
            print(f"  ìƒ˜í”Œ ì»¨ì…‰: {', '.join(stat['sample_concepts'][:2])}")
            print()
        
        # UMAPìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
        reducer = umap.UMAP(n_components=2, random_state=42)
        embeddings_2d = reducer.fit_transform(embeddings)
        
        viz_df = df.copy()
        viz_df['x'] = embeddings_2d[:, 0]
        viz_df['y'] = embeddings_2d[:, 1]
        viz_df['cluster'] = cluster_labels.astype(str)
        
        fig = px.scatter(
            viz_df,
            x='x',
            y='y',
            color='cluster',
            hover_data=['concept_name', 'domain_id'],
            title=f'SapBERT ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (K={n_clusters})',
            width=1000,
            height=700
        )
        
        fig.update_traces(marker=dict(size=5, opacity=0.7))
        fig.update_layout(
            xaxis_title='UMAP 1',
            yaxis_title='UMAP 2',
            legend_title='í´ëŸ¬ìŠ¤í„°'
        )
        
        if save_path:
            fig.write_html(save_path)
            print(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” ì €ì¥ë¨: {save_path}")
        
        fig.show()
        
        return cluster_labels, cluster_stats, fig
    
    def run_comprehensive_test(self, csv_path, sample_size=1000, output_dir="./sapbert_results"):
        """
        ì¢…í•©ì ì¸ SapBERT ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Args:
            csv_path: CONCEPT.csv íŒŒì¼ ê²½ë¡œ
            sample_size: í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ í¬ê¸°
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        print("=" * 60)
        print("SapBERT ì˜ë£Œ ì—”í‹°í‹° ì„ë² ë”© ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_concept_data(csv_path, sample_size=sample_size)
        
        # 2. ì„ë² ë”© ìƒì„±
        concept_names = df['concept_name'].tolist()
        embeddings = self.get_embeddings(concept_names)
        
        # 3. ìœ ì‚¬ì„± ë¶„ì„
        print("\n" + "="*50)
        print("ìœ ì‚¬ì„± ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        print("="*50)
        similarity_results = self.analyze_similarity(df, embeddings)
        
        # ìœ ì‚¬ì„± ì˜ˆì‹œ ì¶œë ¥
        print("\nìœ ì‚¬ì„± ë¶„ì„ ì˜ˆì‹œ:")
        for i, example in enumerate(similarity_results['sample_examples'][:3]):
            print(f"\n{i+1}. '{example['original']['name']}' ({example['original']['domain']})")
            print("   ìœ ì‚¬í•œ ì—”í‹°í‹°ë“¤:")
            for j, similar in enumerate(example['similar'][:3]):
                print(f"   {j+1}. {similar['name']} ({similar['domain']}) - ìœ ì‚¬ë„: {similar['similarity']:.4f}")
        
        # 4. 2D ì‹œê°í™” (UMAP)
        print("\n" + "="*50)
        print("UMAP 2D ì‹œê°í™” ìƒì„± ì¤‘...")
        print("="*50)
        embeddings_2d_umap, fig_umap = self.visualize_embeddings_2d(
            df, embeddings, method='umap', 
            save_path=os.path.join(output_dir, 'sapbert_embedding_umap.html')
        )
        
        # 5. t-SNE ì‹œê°í™”
        print("\n" + "="*50)
        print("t-SNE 2D ì‹œê°í™” ìƒì„± ì¤‘...")
        print("="*50)
        embeddings_2d_tsne, fig_tsne = self.visualize_embeddings_2d(
            df, embeddings, method='tsne',
            save_path=os.path.join(output_dir, 'sapbert_embedding_tsne.html')
        )
        
        # 6. ìœ ì‚¬ì„± íˆíŠ¸ë§µ
        print("\n" + "="*50)
        print("ìœ ì‚¬ì„± íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        print("="*50)
        self.visualize_similarity_heatmap(
            df, similarity_results['similarity_matrix'], 
            sample_size=50,
            save_path=os.path.join(output_dir, 'similarity_heatmap.png')
        )
        
        # 7. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        print("\n" + "="*50)
        print("í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        print("="*50)
        cluster_labels, cluster_stats, fig_cluster = self.cluster_analysis(
            df, embeddings, n_clusters=8,
            save_path=os.path.join(output_dir, 'clustering_analysis.html')
        )
        
        # 8. ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary = {
            'model_name': self.model_name,
            'sample_size': len(df),
            'embedding_dim': embeddings.shape[1],
            'overall_avg_similarity': similarity_results['overall_avg_similarity'],
            'domain_similarities': similarity_results['domain_similarities'],
            'cluster_stats': cluster_stats
        }
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        report_path = os.path.join(output_dir, 'sapbert_test_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("SapBERT ì˜ë£Œ ì—”í‹°í‹° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ëª¨ë¸: {summary['model_name']}\n")
            f.write(f"ìƒ˜í”Œ í¬ê¸°: {summary['sample_size']:,}\n")
            f.write(f"ì„ë² ë”© ì°¨ì›: {summary['embedding_dim']}\n")
            f.write(f"ì „ì²´ í‰ê·  ìœ ì‚¬ì„±: {summary['overall_avg_similarity']:.4f}\n\n")
            
            f.write("ë„ë©”ì¸ë³„ í‰ê·  ìœ ì‚¬ì„±:\n")
            for domain, sim in sorted(summary['domain_similarities'].items(), 
                                    key=lambda x: x[1], reverse=True):
                f.write(f"  {domain}: {sim:.4f}\n")
            
            f.write("\ní´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼:\n")
            for stat in summary['cluster_stats']:
                f.write(f"í´ëŸ¬ìŠ¤í„° {stat['cluster_id']} (í¬ê¸°: {stat['size']}):\n")
                f.write(f"  ì£¼ìš” ë„ë©”ì¸: {stat['top_domain']} ({stat['top_domain_ratio']:.2%})\n")
                f.write(f"  ì£¼ìš” ì–´íœ˜: {stat['top_vocab']}\n\n")
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë¦¬í¬íŠ¸ íŒŒì¼: {report_path}")
        
        return {
            'df': df,
            'embeddings': embeddings,
            'similarity_results': similarity_results,
            'cluster_results': (cluster_labels, cluster_stats),
            'summary': summary
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # CONCEPT.csv íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_path = "../data/CONCEPT.csv"
    
    # SapBERT í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = SapBERTEmbeddingTester()
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.run_comprehensive_test(
        csv_path=csv_path,
        sample_size=1000,  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1000ê°œ ìƒ˜í”Œ ì‚¬ìš©
        output_dir="./sapbert_results"
    )
    
    print("\nğŸ‰ SapBERT ì„ë² ë”© í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
