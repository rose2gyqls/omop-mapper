#!/usr/bin/env python3
"""
κ°„λ‹¨ν• SapBERT λ°λ¨ μ¤ν¬λ¦½νΈ
μλ£ μ©μ–΄λ“¤μ μ„λ² λ”©κ³Ό μ μ‚¬μ„±μ„ λΉ λ¥΄κ² ν™•μΈν•  μ μμµλ‹λ‹¤.
"""

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')


def load_sapbert_model():
    """SapBERT λ¨λΈ λ΅λ“"""
    model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"π§  SapBERT λ¨λΈ λ΅λ”© μ¤‘... (λ””λ°”μ΄μ¤: {device})")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)
    model.eval()
    print("β… λ¨λΈ λ΅λ”© μ™„λ£!")
    
    return tokenizer, model, device


def get_embeddings(texts, tokenizer, model, device):
    """ν…μ¤νΈλ“¤μ„ μ„λ² λ”©μΌλ΅ λ³€ν™"""
    embeddings = []
    
    with torch.no_grad():
        for text in texts:
            # ν† ν¬λ‚μ΄μ§•
            encoded = tokenizer(
                text, 
                padding=True, 
                truncation=True, 
                max_length=128,
                return_tensors="pt"
            )
            
            # GPUλ΅ μ΄λ™
            input_ids = encoded['input_ids'].to(device)
            attention_mask = encoded['attention_mask'].to(device)
            
            # μ„λ² λ”© μƒμ„± ([CLS] ν† ν° μ‚¬μ©)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(cls_embedding[0])
    
    return np.array(embeddings)


def analyze_similarity(terms, embeddings):
    """μ μ‚¬μ„± λ¶„μ„ λ° κ²°κ³Ό μ¶λ ¥"""
    similarity_matrix = cosine_similarity(embeddings)
    
    print(f"\nπ“ {len(terms)}κ° μλ£ μ©μ–΄μ μ μ‚¬μ„± λ¶„μ„ κ²°κ³Ό:")
    print("=" * 60)
    
    # κ° μ©μ–΄λ³„ κ°€μ¥ μ μ‚¬ν• μ©μ–΄λ“¤ μ°ΎκΈ°
    for i, term in enumerate(terms):
        similarities = similarity_matrix[i]
        # μκΈ° μμ‹  μ μ™Έν•κ³  μ •λ ¬
        similar_indices = np.argsort(similarities)[::-1][1:4]  # μƒμ„ 3κ°
        
        print(f"\nπ” '{term}'κ³Ό κ°€μ¥ μ μ‚¬ν• μ©μ–΄λ“¤:")
        for j, idx in enumerate(similar_indices):
            print(f"   {j+1}. {terms[idx]} (μ μ‚¬λ„: {similarities[idx]:.4f})")
    
    # μ „μ²΄ ν‰κ·  μ μ‚¬μ„±
    mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
    avg_similarity = similarity_matrix[mask].mean()
    print(f"\nπ“ μ „μ²΄ ν‰κ·  μ μ‚¬μ„±: {avg_similarity:.4f}")
    
    # κ°€μ¥ μ μ‚¬ν•/λ‹¤λ¥Έ μ μ°ΎκΈ°
    pairs = []
    for i in range(len(terms)):
        for j in range(i+1, len(terms)):
            pairs.append((terms[i], terms[j], similarity_matrix[i, j]))
    
    pairs.sort(key=lambda x: x[2], reverse=True)
    
    print(f"\nπ”¥ κ°€μ¥ μ μ‚¬ν• μλ“¤:")
    for i, (term1, term2, sim) in enumerate(pairs[:3]):
        print(f"   {i+1}. {term1} β†” {term2}: {sim:.4f}")
    
    print(f"\nβ„οΈ  κ°€μ¥ λ‹¤λ¥Έ μλ“¤:")
    for i, (term1, term2, sim) in enumerate(pairs[-3:]):
        print(f"   {i+1}. {term1} β†” {term2}: {sim:.4f}")


def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    print("π§¬ SapBERT μλ£ μ—”ν‹°ν‹° μ„λ² λ”© κ°„λ‹¨ λ°λ¨")
    print("=" * 50)
    
    # ν…μ¤νΈν•  μλ£ μ©μ–΄λ“¤
    medical_terms = [
        "diabetes mellitus",           # λ‹Ήλ‡¨λ³‘
        "type 2 diabetes",            # 2ν• λ‹Ήλ‡¨λ³‘  
        "hypertension",               # κ³ νμ••
        "high blood pressure",        # κ³ νμ•• (λ‹¤λ¥Έ ν‘ν„)
        "myocardial infarction",      # μ‹¬κ·Όκ²½μƒ‰
        "heart attack",               # μ‹¬μ¥λ§λΉ„
        "pneumonia",                  # νλ ΄
        "lung infection",             # νκ°μ—Ό
        "influenza",                  # μΈν”λ£¨μ—”μ
        "flu",                        # λ…κ°
        "covid-19",                   # μ½”λ΅λ‚19
        "coronavirus infection",       # μ½”λ΅λ‚λ°”μ΄λ¬μ¤ κ°μ—Ό
        "headache",                   # λ‘ν†µ
        "migraine",                   # νΈλ‘ν†µ
        "fever",                      # λ°μ—΄
        "high temperature",           # κ³ μ²΄μ¨
        "cough",                      # κΈ°μΉ¨
        "chest pain",                 # ν‰ν†µ
        "shortness of breath",        # νΈν΅κ³¤λ€
        "difficulty breathing"        # νΈν΅ μ–΄λ ¤μ›€
    ]
    
    print(f"π“‹ ν…μ¤νΈ μ©μ–΄ μ: {len(medical_terms)}")
    print(f"π“ μ©μ–΄ λ©λ΅: {', '.join(medical_terms[:5])}...")
    
    try:
        # 1. λ¨λΈ λ΅λ“
        tokenizer, model, device = load_sapbert_model()
        
        # 2. μ„λ² λ”© μƒμ„±
        print(f"\nβ΅ μ„λ² λ”© μƒμ„± μ¤‘...")
        embeddings = get_embeddings(medical_terms, tokenizer, model, device)
        print(f"β… μ„λ² λ”© μƒμ„± μ™„λ£! ν•νƒ: {embeddings.shape}")
        
        # 3. μ μ‚¬μ„± λ¶„μ„
        analyze_similarity(medical_terms, embeddings)
        
        # 4. μ„±λ¥ ν‰κ°€
        similarity_matrix = cosine_similarity(embeddings)
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        avg_similarity = similarity_matrix[mask].mean()
        
        print(f"\nπ’΅ SapBERT μ„±λ¥ ν‰κ°€:")
        if avg_similarity > 0.7:
            print("   πΆ μ°μ: μλ£ μ©μ–΄ μ„λ² λ”© ν’μ§μ΄ λ§¤μ° μΆ‹μµλ‹λ‹¤!")
        elif avg_similarity > 0.5:
            print("   π΅ μ–‘νΈ: μλ£ μ©μ–΄ μ„λ² λ”© ν’μ§μ΄ κ΄μ°®μµλ‹λ‹¤.")
        else:
            print("   π”΄ κ°μ„  ν•„μ”: μ„λ² λ”© ν’μ§ ν–¥μƒμ΄ ν•„μ”ν•©λ‹λ‹¤.")
        
        print(f"\nπ‰ λ°λ¨ μ™„λ£!")
        print(f"π’¬ λ” μμ„Έν• λ¶„μ„μ„ μ›ν•μ‹λ©΄ 'python run_sapbert_test.py --mode comprehensive' λ¥Ό μ‹¤ν–‰ν•΄λ³΄μ„Έμ”!")
        
    except Exception as e:
        print(f"β μ¤λ¥ λ°μƒ: {str(e)}")
        print("π’΅ ν•΄κ²° λ°©λ²•:")
        print("   1. μΈν„°λ„· μ—°κ²° ν™•μΈ (λ¨λΈ λ‹¤μ΄λ΅λ“ ν•„μ”)")
        print("   2. μ¶©λ¶„ν• λ©”λ¨λ¦¬ ν™•μΈ (μµμ† 4GB RAM κ¶μ¥)")
        print("   3. PyTorchμ™€ transformers λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ ν™•μΈ")


if __name__ == "__main__":
    main()
