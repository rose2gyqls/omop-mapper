"""
ì—”í‹°í‹° ë§¤í•‘ API í…ŒìŠ¤íŠ¸ ì½”ë“œ
ë‘ ê°œì˜ ì—”í‹°í‹°ì— ëŒ€í•´ 6ë‹¨ê³„ë³„ë¡œ í›„ë³´êµ°ê³¼ ì ìˆ˜ë¥¼ í™•ì¸
"""

import sys
import os
import logging
import numpy as np
from typing import Dict, Any, List
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ src ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(__file__))
src_path = os.path.join(project_root, 'src')
sys.path.insert(0, src_path)

from omop_mapper.entity_mapping_api import (
    EntityMappingAPI, 
    EntityInput, 
    EntityTypeAPI
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EntityMappingTester:
    """ì—”í‹°í‹° ë§¤í•‘ API í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        self.api = EntityMappingAPI()
        
        # SapBERT ëª¨ë¸ ì´ˆê¸°í™”
        self.model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
        logger.info(f"ğŸ¤– SapBERT ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"âœ… SapBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {self.device})")
        except Exception as e:
            logger.error(f"âŒ SapBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
            
        logger.info("âœ… EntityMappingTester ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_sapbert_embedding(self, text: str) -> np.ndarray:
        """
        SapBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„±
        
        Args:
            text: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° (768ì°¨ì›)
        """
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                outputs = self.model(**inputs)
                # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
            return embedding.flatten()
            
        except Exception as e:
            logger.error(f"SapBERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(768)  # ê¸°ë³¸ê°’ ë°˜í™˜
    
    def _search_concepts_by_name(self, entity_name: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        concept_nameì´ ì¼ì¹˜í•˜ëŠ” í›„ë³´êµ°ì„ ì°¾ëŠ” ê°„ë‹¨í•œ ê²€ìƒ‰
        
        Args:
            entity_name: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
            
        Returns:
            List[ë§¤ì¹­ëœ ì»¨ì…‰ í›„ë³´ë“¤]
        """
        # concepts ì¸ë±ìŠ¤ì—ì„œ concept_name ê¸°ë°˜ ê²€ìƒ‰
        query = {
            "query": {
                "bool": {
                    "should": [
                        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ì •í™• ì¼ì¹˜
                        {
                            "term": {
                                "concept_name.keyword": {
                                    "value": entity_name.lower(),
                                    "boost": 9.0
                                }
                            }
                        },
                        # ë¶€ë¶„ ì¼ì¹˜
                        {
                            "match": {
                                "concept_name": {
                                    "query": entity_name,
                                    "boost": 5.0
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "size": top_k,
            "sort": [
                {"_score": {"order": "desc"}},
                {"concept_name.keyword": {"order": "asc"}}
            ]
        }
        
        try:
            # concepts ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
            response = self.api.es_client.es_client.search(
                index="concepts",
                body=query
            )
            
            return response['hits']['hits'] if response['hits']['total']['value'] > 0 else []
            
        except Exception as e:
            logger.error(f"concept_name ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _search_concepts_hybrid(self, entity_name: str, top_k: int = 10, 
                               text_weight: float = 0.4, semantic_weight: float = 0.6) -> List[Dict[str, Any]]:
        """
        ë¬¸ìì—´ ì¼ì¹˜ë„ì™€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        Args:
            entity_name: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
            text_weight: ë¬¸ìì—´ ì¼ì¹˜ë„ ê°€ì¤‘ì¹˜
            semantic_weight: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
            
        Returns:
            List[í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬ëœ ì»¨ì…‰ í›„ë³´ë“¤]
        """
        logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: '{entity_name}' (í…ìŠ¤íŠ¸:{text_weight}, ì˜ë¯¸:{semantic_weight})")
        
        # 1ë‹¨ê³„: ì—”í‹°í‹° ì„ë² ë”© ìƒì„±
        entity_embedding = self._get_sapbert_embedding(entity_name)
        logger.info(f"ğŸ“Š ì—”í‹°í‹° ì„ë² ë”© ìƒì„± ì™„ë£Œ (Shape: {entity_embedding.shape})")
        
        # 2ë‹¨ê³„: ë¬¸ìì—´ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì´ˆê¸° í›„ë³´ í™•ë³´
        text_candidates = self._search_concepts_by_name(entity_name, top_k=50)  # ë” ë§ì€ í›„ë³´ í™•ë³´
        logger.info(f"ğŸ“ ë¬¸ìì—´ ê²€ìƒ‰ ê²°ê³¼: {len(text_candidates)}ê°œ í›„ë³´")
        
        if not text_candidates:
            logger.warning("ë¬¸ìì—´ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []
        
        # 3ë‹¨ê³„: ê° í›„ë³´ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_candidates = []
        
        for candidate in text_candidates:
            try:
                source = candidate['_source']
                concept_name = source.get('concept_name', '')
                elasticsearch_score = candidate['_score']
                
                # ë¬¸ìì—´ ìœ ì‚¬ë„ (ì •ê·œí™”ëœ Elasticsearch ì ìˆ˜)
                max_es_score = text_candidates[0]['_score'] if text_candidates else 1.0
                text_similarity = elasticsearch_score / max_es_score
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                concept_embedding = source.get('concept_embedding')
                if concept_embedding and len(concept_embedding) == 768:
                    concept_emb_array = np.array(concept_embedding).reshape(1, -1)
                    entity_emb_array = entity_embedding.reshape(1, -1)
                    semantic_similarity = cosine_similarity(entity_emb_array, concept_emb_array)[0][0]
                else:
                    # ì„ë² ë”©ì´ ì—†ëŠ” ê²½ìš° ë¬¸ìì—´ ìœ ì‚¬ë„ë¡œ ëŒ€ì²´
                    semantic_similarity = self.api._calculate_similarity(entity_name, concept_name)
                    logger.debug(f"ì„ë² ë”© ì—†ìŒ - ë¬¸ìì—´ ìœ ì‚¬ë„ ì‚¬ìš©: {concept_name}")
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
                hybrid_score = (text_weight * text_similarity) + (semantic_weight * semantic_similarity)
                
                # í›„ë³´ ì •ë³´ ì €ì¥
                hybrid_candidate = {
                    '_source': source,
                    '_score': elasticsearch_score,
                    'text_similarity': text_similarity,
                    'semantic_similarity': semantic_similarity,
                    'hybrid_score': hybrid_score,
                    'original_candidate': candidate
                }
                
                hybrid_candidates.append(hybrid_candidate)
                
            except Exception as e:
                logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        # 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_candidates.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # ìƒìœ„ Kê°œ ë°˜í™˜
        top_candidates = hybrid_candidates[:top_k]
        
        logger.info(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ë°˜í™˜")
        
        # ì ìˆ˜ ì •ë³´ ë¡œê¹…
        for i, candidate in enumerate(top_candidates[:3], 1):
            source = candidate['_source']
            logger.info(f"  {i}. {source.get('concept_name', 'N/A')}")
            logger.info(f"     í…ìŠ¤íŠ¸: {candidate['text_similarity']:.3f}, "
                       f"ì˜ë¯¸: {candidate['semantic_similarity']:.3f}, "
                       f"í•˜ì´ë¸Œë¦¬ë“œ: {candidate['hybrid_score']:.3f}")
        
        return top_candidates
    
    def test_entity_mapping_6_steps(self, entity_name: str, entity_type: str, 
                                   golden_concept_id: str, golden_concept_name: str) -> None:
        """
        ì—”í‹°í‹° ë§¤í•‘ì„ 6ë‹¨ê³„ë³„ë¡œ ìƒì„¸ í…ŒìŠ¤íŠ¸
        
        Args:
            entity_name: í…ŒìŠ¤íŠ¸í•  ì—”í‹°í‹° ì´ë¦„
            entity_type: ì—”í‹°í‹° íƒ€ì…
            golden_concept_id: ê³¨ë“ ì…‹ ì»¨ì…‰ ID
            golden_concept_name: ê³¨ë“ ì…‹ ì»¨ì…‰ ì´ë¦„
        """
        print(f"\n{'='*80}")
        print(f"ğŸ” ì—”í‹°í‹°: {entity_name}")
        print(f"ğŸ¯ ê³¨ë“ ì…‹: {golden_concept_id} - {golden_concept_name}")
        print(f"{'='*80}")
        
        # ì—”í‹°í‹° ì´ë¦„ ì „ì²˜ë¦¬
        preprocessed_name = self.api._preprocess_entity_name(entity_name)
        print(f"ğŸ“ ì „ì²˜ë¦¬: '{entity_name}' â†’ '{preprocessed_name}'")
        
        # ì—”í‹°í‹° ì…ë ¥ ìƒì„±
        entity_input = EntityInput(
            entity_name=preprocessed_name,
            entity_type=EntityTypeAPI(entity_type),
            confidence=1.0
        )
        
        # ì‚¬ì „ ë§¤í•‘ ì •ë³´ ì¤€ë¹„
        entities_to_map = self.api._prepare_entity_for_mapping(entity_input)
        if not entities_to_map:
            print("âŒ ì—”í‹°í‹° ë§¤í•‘ ì¤€ë¹„ ì‹¤íŒ¨")
            return
        
        entity_info = entities_to_map[0]
        
        # ===== 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ ë¬¸ìì—´ ì¼ì¹˜ë„ + ì˜ë¯¸ì  ìœ ì‚¬ë„ =====
        print(f"\nğŸ” 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ ë¬¸ìì—´ ì¼ì¹˜ë„ + ì˜ë¯¸ì  ìœ ì‚¬ë„")
        print(f"{'='*60}")
        
        candidates = self._search_concepts_hybrid(preprocessed_name, top_k=5, text_weight=0.4, semantic_weight=0.6)
        
        if not candidates:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return
        
        print(f"ì´ {len(candidates)}ê°œ í›„ë³´ ë°œê²¬:")
        for i, candidate in enumerate(candidates, 1):
            source = candidate['_source']
            hybrid_score = candidate.get('hybrid_score', 0.0)
            text_sim = candidate.get('text_similarity', 0.0)
            semantic_sim = candidate.get('semantic_similarity', 0.0)
            
            print(f"  {i}. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.3f}")
            print(f"     â”” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_sim:.3f} | ì˜ë¯¸ì  ìœ ì‚¬ë„: {semantic_sim:.3f}")
            print(f"     ì»¨ì…‰ ID: {source.get('concept_id', 'N/A')}")
            print(f"     ì»¨ì…‰ëª…: {source.get('concept_name', 'N/A')}")
            print(f"     ë„ë©”ì¸: {source.get('domain_id', 'N/A')}")
            print(f"     ì–´íœ˜ì²´ê³„: {source.get('vocabulary_id', 'N/A')}")
            print(f"     í‘œì¤€ì—¬ë¶€: {source.get('standard_concept', 'N/A')}")
            print()
        
        # ===== 2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜ =====
        print(f"ğŸ”„ 2ë‹¨ê³„: Standard/Non-standard ë¶„ë¥˜")
        print(f"{'='*60}")
        
        standard_candidates = []
        non_standard_candidates = []
        
        for i, candidate in enumerate(candidates, 1):
            source = candidate['_source']
            # í•˜ì´ë¸Œë¦¬ë“œ í›„ë³´ì—ì„œëŠ” original_candidateê°€ ìˆì„ ìˆ˜ ìˆìŒ
            original_candidate = candidate.get('original_candidate', candidate)
            
            if source.get('standard_concept') == 'S':
                standard_candidates.append(original_candidate)
                print(f"  {i}. âœ… Standard: {source.get('concept_name', 'N/A')}")
            else:
                non_standard_candidates.append(original_candidate)
                print(f"  {i}. âš ï¸ Non-standard: {source.get('concept_name', 'N/A')}")
        
        print(f"\n  ğŸ“Š ë¶„ë¥˜ ê²°ê³¼: Standard {len(standard_candidates)}ê°œ, Non-standard {len(non_standard_candidates)}ê°œ")
        
        # ===== 3ë‹¨ê³„: Non-standardì¸ ê²½ìš° â†’ Maps to ê´€ê³„ë¡œ Standard í›„ë³´ ì¡°íšŒ =====
        print(f"\nğŸ”— 3ë‹¨ê³„: Non-standard â†’ Maps to ê´€ê³„ë¡œ Standard í›„ë³´ ì¡°íšŒ")
        print(f"{'='*60}")
        
        all_standard_candidates = []
        
        # Standard í›„ë³´ë“¤ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        for candidate in standard_candidates:
            source = candidate['_source']
            all_standard_candidates.append({
                'concept': source,
                'final_score': candidate['_score'],
                'is_original_standard': True,
                'original_candidate': candidate
            })
        
        # Non-standard í›„ë³´ë“¤ì€ Standard í›„ë³´ ì¡°íšŒ
        for i, candidate in enumerate(non_standard_candidates, 1):
            source = candidate['_source']
            concept_id = str(source.get('concept_id', ''))
            print(f"  Non-standard {i}: {source.get('concept_name', 'N/A')} (ID: {concept_id})")
            
            standard_candidates_from_non = self.api._get_standard_candidates(concept_id, entity_info["domain_id"])
            
            if standard_candidates_from_non:
                print(f"    â†’ Maps to ê´€ê³„ë¡œ {len(standard_candidates_from_non)}ê°œ Standard í›„ë³´ ë°œê²¬")
                for j, std_candidate in enumerate(standard_candidates_from_non[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                    print(f"      {j}. {std_candidate.get('concept_name', 'N/A')} (ID: {std_candidate.get('concept_id', 'N/A')})")
            else:
                print(f"    â†’ Maps to ê´€ê³„ ì—†ìŒ")
            
            print()
        
        # ===== 4ë‹¨ê³„: Non-standard â†’ Standard í›„ë³´ ì¡°íšŒ ë° ì„ì‹œ ì €ì¥ =====
        print(f"ğŸ”— 4ë‹¨ê³„: Non-standard â†’ Standard í›„ë³´ ì¡°íšŒ ë° ì„ì‹œ ì €ì¥")
        print(f"{'='*60}")
        
        # Non-standard í›„ë³´ë“¤ì˜ Standard í›„ë³´ë“¤ì„ ì„ì‹œë¡œ ì €ì¥
        non_standard_to_standard_mappings = []
        
        for i, candidate in enumerate(non_standard_candidates, 1):
            source = candidate['_source']
            concept_id = str(source.get('concept_id', ''))
            print(f"  Non-standard {i}: {source.get('concept_name', 'N/A')} (ID: {concept_id})")
            
            standard_candidates_from_non = self.api._get_standard_candidates(concept_id, entity_info["domain_id"])
            
            if standard_candidates_from_non:
                print(f"    â†’ Maps to ê´€ê³„ë¡œ {len(standard_candidates_from_non)}ê°œ Standard í›„ë³´ ë°œê²¬")
                for j, std_candidate in enumerate(standard_candidates_from_non[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                    print(f"      {j}. {std_candidate.get('concept_name', 'N/A')} (ID: {std_candidate.get('concept_id', 'N/A')}")
                
                # ëª¨ë“  Standard í›„ë³´ë“¤ì„ ì„ì‹œ ì €ì¥ (ë‚˜ì¤‘ì— ìœ ì‚¬ë„ ì¬ê³„ì‚°)
                non_standard_to_standard_mappings.append({
                    'non_standard_source': source,
                    'non_standard_candidate': candidate,
                    'standard_candidates': standard_candidates_from_non
                })
            else:
                print(f"    â†’ Maps to ê´€ê³„ ì—†ìŒ")
            
            print()
        
        # ===== 5ë‹¨ê³„: ëª¨ë“  í›„ë³´êµ°ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ë°˜ Re-ranking =====
        print(f"ğŸ¯ 5ë‹¨ê³„: ëª¨ë“  í›„ë³´êµ°ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ë°˜ Re-ranking")
        print(f"{'='*60}")
        
        all_standard_candidates = []
        
        # 1. Standard í›„ë³´ë“¤ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ì‚¬ìš©
        print("  ğŸ“Š Standard í›„ë³´ë“¤ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜:")
        for i, candidate in enumerate(standard_candidates, 1):
            source = candidate['_source']
            original_score = candidate['_score']
            
            # í•˜ì´ë¸Œë¦¬ë“œ í›„ë³´ì—ì„œ í•´ë‹¹ í›„ë³´ ì°¾ê¸°
            hybrid_score = 0.0
            text_sim = 0.0
            semantic_sim = 0.0
            
            # candidatesëŠ” í•˜ì´ë¸Œë¦¬ë“œ í›„ë³´ë“¤ì´ë¯€ë¡œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì°¾ê¸°
            for hybrid_candidate in candidates:
                if hybrid_candidate['_source'].get('concept_id') == source.get('concept_id'):
                    hybrid_score = hybrid_candidate.get('hybrid_score', 0.0)
                    text_sim = hybrid_candidate.get('text_similarity', 0.0)
                    semantic_sim = hybrid_candidate.get('semantic_similarity', 0.0)
                    break
            
            print(f"    {i}. {source.get('concept_name', 'N/A')}")
            print(f"       í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score:.3f} (í…ìŠ¤íŠ¸: {text_sim:.3f}, ì˜ë¯¸: {semantic_sim:.3f})")
            
            all_standard_candidates.append({
                'concept': source,
                'final_score': hybrid_score,  # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ì‚¬ìš©
                'is_original_standard': True,
                'original_candidate': candidate,
                'elasticsearch_score': original_score,
                'hybrid_score': hybrid_score,
                'text_similarity': text_sim,
                'semantic_similarity': semantic_sim
            })
            print()
        
        # 2. Non-standard â†’ Standard í›„ë³´ë“¤ì— ëŒ€í•´ Python ìœ ì‚¬ë„ ì¬ê³„ì‚°
        print("  ğŸ“Š Non-standard â†’ Standard í›„ë³´ë“¤ Python ìœ ì‚¬ë„ ì¬ê³„ì‚°:")
        for i, mapping in enumerate(non_standard_to_standard_mappings, 1):
            non_standard_source = mapping['non_standard_source']
            non_standard_candidate = mapping['non_standard_candidate']
            standard_candidates_list = mapping['standard_candidates']
            
            print(f"    Non-standard {i}: {non_standard_source.get('concept_name', 'N/A')}")
            
            for j, std_candidate in enumerate(standard_candidates_list, 1):
                # Python ìœ ì‚¬ë„ ì¬ê³„ì‚°
                python_similarity = self.api._calculate_similarity(preprocessed_name, std_candidate.get('concept_name', ''))
                
                print(f"      Standard {j}: {std_candidate.get('concept_name', 'N/A')}")
                print(f"        Python ìœ ì‚¬ë„: {python_similarity:.3f}")
                
                all_standard_candidates.append({
                    'concept': std_candidate,
                    'final_score': python_similarity,  # Python ìœ ì‚¬ë„ ì‚¬ìš©
                    'is_original_standard': False,
                    'original_non_standard': non_standard_source,
                    'original_candidate': non_standard_candidate,
                    'python_similarity': python_similarity
                })
            
            print()
        
        # ===== 6ë‹¨ê³„: ì ìˆ˜ ì •ê·œí™” (0.0~1.0) â†’ ìµœì¢… ë§¤í•‘ ê²°ê³¼ =====
        print(f"ğŸ“Š 6ë‹¨ê³„: ì ìˆ˜ ì •ê·œí™” (0.0~1.0) â†’ ìµœì¢… ë§¤í•‘ ê²°ê³¼")
        print(f"{'='*60}")
        
        if not all_standard_candidates:
            print("âŒ ì²˜ë¦¬ëœ í›„ë³´ ì—†ìŒ")
            return
        
        # ì ìˆ˜ë³„ ì •ë ¬ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€)
        sorted_candidates = sorted(all_standard_candidates, key=lambda x: x['final_score'], reverse=True)
        
        print("ìµœì¢… í›„ë³´ ìˆœìœ„ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€):")
        for i, candidate in enumerate(sorted_candidates, 1):
            concept = candidate['concept']
            final_score = candidate['final_score']  # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            is_standard = candidate['is_original_standard']
            mapping_type = "ì§ì ‘ Standard" if is_standard else "Non-standard â†’ Standard"
            
            # ì ìˆ˜ ì •ê·œí™” (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ëŠ” ì´ë¯¸ 0~1 ì‚¬ì´)
            normalized_score = self.api._normalize_score(final_score)
            confidence = self.api._determine_confidence(normalized_score)
            
            print(f"  {i}. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {final_score:.3f} â†’ ì •ê·œí™”: {normalized_score:.3f} ({confidence})")
            
            # ì„¸ë¶€ ì ìˆ˜ ì •ë³´
            if 'hybrid_score' in candidate:
                text_sim = candidate.get('text_similarity', 0.0)
                semantic_sim = candidate.get('semantic_similarity', 0.0)
                print(f"     â”” í…ìŠ¤íŠ¸: {text_sim:.3f} | ì˜ë¯¸: {semantic_sim:.3f}")
            
            print(f"     ì»¨ì…‰ ID: {concept.get('concept_id', 'N/A')}")
            print(f"     ì»¨ì…‰ëª…: {concept.get('concept_name', 'N/A')}")
            print(f"     ë„ë©”ì¸: {concept.get('domain_id', 'N/A')}")
            print(f"     ì–´íœ˜ì²´ê³„: {concept.get('vocabulary_id', 'N/A')}")
            print(f"     ë§¤í•‘ ë°©ë²•: {mapping_type}")
            
            print()
        
        # ê³¨ë“ ì…‹ê³¼ ë¹„êµ
        best_candidate = sorted_candidates[0]
        best_concept = best_candidate['concept']
        best_concept_id = str(best_concept.get('concept_id', ''))
        
        print(f"ğŸ¯ ê³¨ë“ ì…‹ ë¹„êµ:")
        if best_concept_id == golden_concept_id:
            print(f"  âœ… ì„±ê³µ! ê³¨ë“ ì…‹ê³¼ ì •í™•íˆ ì¼ì¹˜")
        else:
            print(f"  âŒ ë¶ˆì¼ì¹˜")
        
        print(f"  ì˜ˆìƒ: {golden_concept_id} - {golden_concept_name}")
        print(f"  ì‹¤ì œ: {best_concept_id} - {best_concept.get('concept_name', 'N/A')}")
        
        print(f"\n{'='*80}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ ì—”í‹°í‹° ë§¤í•‘ API í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # API ìƒíƒœ í™•ì¸
    tester = EntityMappingTester()
    health_check = tester.api.health_check()
    print(f"ğŸ“Š API ìƒíƒœ: {health_check}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: Adrenal Cushing's syndrome
    print("\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: Adrenal Cushing's syndrome")
    print("="*80)
    tester.test_entity_mapping_6_steps(
        entity_name="Adrenal Cushing's syndrome",
        entity_type="condition",
        golden_concept_id="4030206",
        golden_concept_name="Adrenal Cushing's syndrome"
    )
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Acute Coronary Syndromes (ACS)
    print("\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Acute Coronary Syndromes (ACS)")
    print("="*80)
    tester.test_entity_mapping_6_steps(
        entity_name="Acute Coronary Syndromes (ACS)",
        entity_type="diagnostic",
        golden_concept_id="4215140",
        golden_concept_name="Acute coronary syndrome"
    )

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ST-segment elevation myocardial infarction (STEMI)
    print("\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ST-segment elevation myocardial infarction (STEMI)")
    print("="*80)
    tester.test_entity_mapping_6_steps(
        entity_name="ST-segment elevation myocardial infarction (STEMI)",
        entity_type="diagnostic",
        golden_concept_id="4296653",
        golden_concept_name="Acute ST segment elevation myocardial infarction"
    )
    
    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()